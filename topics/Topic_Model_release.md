# Model release news

| Title | Summary | Topics | Week |
| --- | --- | --- | --- |
| [Apple releases OpenELM: small, open source AI models designed to run on-device](https://venturebeat.com/ai/apple-releases-openelm-small-open-source-ai-models-designed-to-run-on-device/) 🟢 | Apple has introduced OpenELM, a suite of open-source AI text generation models with 270M to 3B parameters, optimized for on-device deployment. Available on Hugging Face, these models are released under a sample code license to enable AI functionalities independently of the cloud. | [Hugging Face 🤗](topics/Topic_Hugging_Face_🤗.md), [Apple 🍏](topics/Topic_Apple_🍏.md), [Model release 🎉](topics/Topic_Model_release_🎉.md) | 2024-04-29 |
| [Snowflake releases Arctic, an open LLM for Enterprise AI](https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/) 🟢 | Snowflake AI Research has released Arctic, a cost-effective enterprise AI LLM featuring a Dense-MoE Hybrid transformer architecture with 480 billion parameters. Trained for less than $2 million, Arctic excels in tasks like SQL generation and coding. It’s fully open-source under Apache 2.0, providing free access to model weights and code. | [Mixture-of-Experts 🤝](topics/Topic_Mixture-of-Experts_🤝.md), [AI for coding 👨‍💻](topics/Topic_AI_for_coding_👨‍💻.md), [Model release 🎉](topics/Topic_Model_release_🎉.md), [Funding 💰](topics/Topic_Funding_💰.md) | 2024-04-29 |
| [Introducing Meta Llama 3: The most capable openly available LLM to date](https://ai.meta.com/blog/meta-llama-3/) 🟢 | Meta has introduced Meta Llama 3, a state-of-the-art open-source large language model (LLM) with versions up to 70 billion parameters, providing enhanced reasoning and multilingual capabilities. The current best models are pretrained and instruction-fine-tuned at both 8B and 70B scales. Additionally, even larger models exceeding 400 billion parameters are in development, promising to push the boundaries further upon their release in the coming months. | [Model release 🎉](topics/Topic_Model_release_🎉.md), [Meta ♾](topics/Topic_Meta_♾.md) | 2024-04-23 |
| [Mistral released Mixtral 8x22B](https://mistral.ai/news/mixtral-8x22b/) 🟢 | Mistral unveiled Mixtral 8x22B, an efficient sparse Mixture-of-Experts model with 39B active out of 141B total parameters, specializing in multilingual communication, coding, mathematics, and excelling in reasoning and knowledge tasks. The model boasts a 64K token context window, is compatible with multiple platforms, and is available under the open-source Apache 2.0 license. | [Mixture-of-Experts 🤝](topics/Topic_Mixture-of-Experts_🤝.md), [AI for coding 👨‍💻](topics/Topic_AI_for_coding_👨‍💻.md), [Model release 🎉](topics/Topic_Model_release_🎉.md), [Mistral 🌬️](topics/Topic_Mistral_🌬️.md) | 2024-04-23 |
| [Introducing DBRX: A New State-of-the-Art Open LLM](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm) 🟢 | Databricks has unveiled DBRX, a new open-source large language model (LLM) that surpasses GPT-3.5 in programming and general tasks, and is competitive with Gemini 1.0 Pro. DBRX features a mixture-of-experts architecture with a massive 132 billion parameters, though only 36 billion are active for any given input. Both DBRX Base and DBRX Instruct models are openly accessible on Hugging Face. | [Hugging Face 🤗](topics/Topic_Hugging_Face_🤗.md), [Mixture-of-Experts 🤝](topics/Topic_Mixture-of-Experts_🤝.md), [AI for coding 👨‍💻](topics/Topic_AI_for_coding_👨‍💻.md), [Model release 🎉](topics/Topic_Model_release_🎉.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md) | 2024-04-02 |
| [Grok open release](https://github.com/xai-org/grok-1) 🟢 | xAI has released Grok-1, a language Mixture-of-Experts model with 314 billion parameters, following its pre-training in October 2023. This base model checkpoint is intended for further research and the development of conversational applications, and it is accessible under the Apache 2.0 license. | [Mixture-of-Experts 🤝](topics/Topic_Mixture-of-Experts_🤝.md), [Grok 🐦](topics/Topic_Grok_🐦.md), [Model release 🎉](topics/Topic_Model_release_🎉.md) | 2024-03-18 |
| [Introducing the next generation of Claude](https://www.anthropic.com/news/claude-3-family) 🟢 | Anthropic has launched Claude 3, a new AI that surpasses GPT-4, with three models: Opus, Sonnet, and Haiku. Each supports a 200k context window, vision abilities, and multiple languages. Opus is touted as the top performer. Sonnet is integrated with Amazon Bedrock and Google Cloud’s Vertex AI, while Opus and Haiku are slated for future release along with new features like function calling and REPL. | [Claude 🖋️](topics/Topic_Claude_🖋️.md), [Anthropic 🌐](topics/Topic_Anthropic_🌐.md), [Model release 🎉](topics/Topic_Model_release_🎉.md), [Multimodal AI (image, video, audio) 📸](topics/Topic_Multimodal_AI_(image_video_audio)_📸.md), [Google 🔍](topics/Topic_Google_🔍.md), [Amazon 🌳](topics/Topic_Amazon_🌳.md), [GPT-4 and GPT-4 turbo 🚀](topics/Topic_GPT-4_and_GPT-4_turbo_🚀.md) | 2024-03-11 |
| [Inflection-2.5: meet the world’s best personal AI](https://inflection.ai/inflection-2-5) 🟢 | Inflection has launched its latest AI version, Inflection-2.5, enhancing its AI model, Pi, with advanced cognitive capabilities that challenge leading language models like GPT-4. Notably, Inflection-2.5 achieves competitive performance in AI tasks, particularly in coding and math, with 40% less computational power required during its training phase. In addition to its improved processing efficiency, Pi now features the ability to conduct real-time web searches to provide updated news and information. | [AI for coding 👨‍💻](topics/Topic_AI_for_coding_👨‍💻.md), [Model release 🎉](topics/Topic_Model_release_🎉.md), [GPT-4 and GPT-4 turbo 🚀](topics/Topic_GPT-4_and_GPT-4_turbo_🚀.md) | 2024-03-11 |
| [Mistral AI releases new model to rival GPT-4 and its own chat assistant](https://techcrunch.com/2024/02/26/mistral-ai-releases-new-model-to-rival-gpt-4-and-its-own-chat-assistant/) 🟢 | Mistral has launched Mistral Large, ranking just below GPT4. It boasts a 32K token context window and multilingual support for English, French, Spanish, German, and Italian. The model excels in following precise instructions, allowing for tailored moderation policies. | [Model release 🎉](topics/Topic_Model_release_🎉.md), [Mistral 🌬️](topics/Topic_Mistral_🌬️.md), [GPT-4 and GPT-4 turbo 🚀](topics/Topic_GPT-4_and_GPT-4_turbo_🚀.md) | 2024-03-04 |
| [Gemma — a family of lightweight, state-of-the art open models from Google.](https://ai.google.dev/gemma/) 🟢 | Google has released Gemma, an open-source large language model based on Gemini, in two versions with 2 billion (2B) and 7 billion (7B) parameters. Both versions come with a basic pretrained model and an instruction-tuned variant to enhance performance. | [Model release 🎉](topics/Topic_Model_release_🎉.md), [Google Gemini 🌌](topics/Topic_Google_Gemini_🌌.md), [Google 🔍](topics/Topic_Google_🔍.md) | 2024-02-26 |
| [OpenAI releases new embedding models and API updates](https://openai.com/blog/new-embedding-models-and-api-updates) 🟢 | OpenAI has announced updates to their AI model suite, including the launch of more efficient embedding models and cost-reduced versions of GPT-3.5 Turbo and a new GPT-4 Turbo model. The “text-embedding-3-large” leads with a 64.6% MTEB score at $0.00013 per 1k tokens, while the “text-embedding-3-small” offers improved performance over its predecessor at a fivefold cost reduction. Additionally, the “gpt-3.5-turbo-0125” is now 50% cheaper, priced at $0.0005 per 1k tokens, and a new “gpt-4–0125-preview” model has been introduced. | [Model release 🎉](topics/Topic_Model_release_🎉.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md), [GPT-4 and GPT-4 turbo 🚀](topics/Topic_GPT-4_and_GPT-4_turbo_🚀.md), [OpenAI 🌟](topics/Topic_OpenAI_🌟.md) | 2024-01-29 |
| [LangChain v0.1.0](https://blog.langchain.dev/langchain-v0-1-0/) 🟢 | LangChain has released its first stable and backward-compatible version, v0.1.0. This release brings better observability and debugging capabilities, including performance tracking and insight tools, and introduces a new versioning system for clear API and feature updates. | [LangChain and LlamaIndex 🔗](topics/Topic_LangChain_and_LlamaIndex_🔗.md), [Model release 🎉](topics/Topic_Model_release_🎉.md) | 2024-01-15 |
| [Midjourney v6 release](https://mid-journey.ai/midjourney-v6-release/) 🟢 | The latest update for Midjourney, version 6, introduces features such as enhanced prompt accuracy and length, increased coherence, better image prompting, and an improved remix mode. Additionally, a new feature for minor text drawing has been added, which can be utilized by including text within quotations. | [Midjourney 🛤️](topics/Topic_Midjourney_🛤️.md), [Model release 🎉](topics/Topic_Model_release_🎉.md) | 2024-01-02 |
| [Phi-2: The surprising power of small language models](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/) 🟢 | Microsoft has released Phi-2, a language model with 2.7 billion parameters that outperforms models up to 25 times its size. Phi-2 achieves remarkable reasoning and language understanding abilities using high-quality data and synthetic datasets. It outperforms larger models on challenging benchmarks, especially in tasks like coding and math. | [AI for coding 👨‍💻](topics/Topic_AI_for_coding_👨‍💻.md), [Model release 🎉](topics/Topic_Model_release_🎉.md), [Microsoft 🪟](topics/Topic_Microsoft_🪟.md) | 2023-12-19 |
| [Introducing gigaGPT: GPT-3 sized models in 565 lines of code](https://www.cerebras.net/blog/introducing-gigagpt-gpt-3-sized-models-in-565-lines-of-code) 🟢 | Cerebras has released gigaGPT, a model implementation similar to nanoGPT but with over 100 billion parameters. By leveraging Cerebras hardware and different optimizers, gigaGPT overcomes the limitations of GPU memory and the need for complex scaling frameworks, offering a simplified approach for training large models. | [Model release 🎉](topics/Topic_Model_release_🎉.md), [AI Chips and GPUs 🖥️](topics/Topic_AI_Chips_and_GPUs_🖥️.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md) | 2023-12-19 |
| [Mistral AI has released Mixtral 8x7B, an open-source sparse mixture-of-experts model](https://mistral.ai/news/mixtral-of-experts/) 🟢 | Mistral AI has introduced Mixtral 8x7B, an open-source, sparse mixture-of-experts model with 45B parameters. It outperforms LLaMA 2 70B and GPT-3.5 in NLP benchmarks, while also providing 6x faster inference speed. The model selectively uses only 12B parameters per inference, keeping operational costs equivalent to a 12B model. The Mixtral model is accessible through Mistral’s beta API and dev platform. | [Mixture-of-Experts 🤝](topics/Topic_Mixture-of-Experts_🤝.md), [Model release 🎉](topics/Topic_Model_release_🎉.md), [Mistral 🌬️](topics/Topic_Mistral_🌬️.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md) | 2023-12-11 |
| [Kyutai is a French AI research lab with a $330 million budget that will make everything open source](https://techcrunch.com/2023/11/17/kyutai-is-an-french-ai-research-lab-with-a-330-million-budget-that-will-make-everything-open-source/) 🟢 | Paris-based AI research lab, Kyutai, secures $330 million in funding to advance the development of artificial general intelligence. With these resources, Kyutai plans to conduct comprehensive research led by PhD students, postdocs, and researchers. Additionally, the lab prioritizes transparency in AI by openly sharing their models, source code, and data. | [Funding 💰](topics/Topic_Funding_💰.md), [Model release 🎉](topics/Topic_Model_release_🎉.md), [AI datasets 📊](topics/Topic_AI_datasets_📊.md) | 2023-11-20 |
| [Jina AI Launches World’s First Open-Source 8K Text Embedding, Rivaling OpenAI](https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/) 🟢 | Jina AI introduces jina-embeddings-v2, an open-source embedding model that supports 8K context length. It matches OpenAI’s 8K model in key areas like Classification Average, Reranking Average, Retrieval Average, and Summarization Average in the MTEB leaderboard. | [Model release 🎉](topics/Topic_Model_release_🎉.md), [OpenAI 🌟](topics/Topic_OpenAI_🌟.md) | 2023-10-30 |
| [Meta quietly unveils Llama 2 Long AI that beats GPT-3.5 Turbo and Claude 2 on some tasks](https://venturebeat.com/ai/meta-quietly-releases-llama-2-long-ai-that-outperforms-gpt-3-5-and-claude-2-on-some-tasks/) 🟢 | Meta is releasing Llama 2 Long, an enhanced version of Llama 2 that underwent continual pretraining with longer training sequences and upsampled long texts. By adding 400 billion tokens and making minor changes to the Rotary Positional Embedding (RoPE), Llama 2 Long can now attend to longer information sequences and include less related information in its model’s knowledge base. | [Model release 🎉](topics/Topic_Model_release_🎉.md), [LLaMA 🦙](topics/Topic_LLaMA_🦙.md), [Meta ♾](topics/Topic_Meta_♾.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md) | 2023-10-10 |
| [Adobe’s Firefly generative AI models are now generally available](https://techcrunch.com/2023/09/13/adobes-firefly-generative-ai-models-are-now-generally-available-get-pricing-plans/) 🟢 | Adobe has released commercially available generative AI models in their Creative Cloud, including a standalone web app called Firefly. The new “generative credits” system controls user interactions with Firefly’s AI models, with each click on ‘generate’ using one credit. | [AI for images 🖼️](topics/Topic_AI_for_images_🖼️.md), [Multimodal AI (image, video, audio) 📸](topics/Topic_Multimodal_AI_(image_video_audio)_📸.md), [Model release 🎉](topics/Topic_Model_release_🎉.md) | 2023-09-18 |
| [Spread Your Wings: Falcon 180B is here](https://huggingface.co/blog/falcon-180b) 🟢 | TII has just released Falcon 180B, a powerful language model with 180 billion parameters trained on 3.5 trillion tokens. Outperforming Llama 2 70B and GPT-3.5 on MMLU, Falcon 180B performs great and ranks high on the Hugging Face Leaderboard. This model is available for commercial use but has strict terms excluding “hosting use.” | [Hugging Face 🤗](topics/Topic_Hugging_Face_🤗.md), [Model release 🎉](topics/Topic_Model_release_🎉.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md) | 2023-09-11 |
| [Releasing Persimmon-8B](https://www.adept.ai/blog/persimmon-8b) 🟢 | Adept.ai introduces Persimmon-8B, an open-source LLM with impressive performance and a compact size. Trained on less data, it achieves comparable results to LLaMA2 and offers fast C++ implementation combined with flexible Python inference. | [Model release 🎉](topics/Topic_Model_release_🎉.md), [LLaMA 🦙](topics/Topic_LLaMA_🦙.md) | 2023-09-11 |
| [AudioCraft: A simple one-stop shop for audio modeling](https://ai.meta.com/blog/audiocraft-musicgen-audiogen-encodec-generative-ai-audio/) 🟢 | Meta has released the code and weights for their AudioCraft models, including MusicGen and AudioGen. These models generate music and audio respectively, based on text-based user inputs. The release also includes the EnCodec decoder, which improves music quality. | [Multimodal AI (image, video, audio) 📸](topics/Topic_Multimodal_AI_(image_video_audio)_📸.md), [Model release 🎉](topics/Topic_Model_release_🎉.md), [Meta ♾](topics/Topic_Meta_♾.md) | 2023-08-07 |
| [NASA and IBM Openly Release Geospatial AI Foundation Model for NASA Earth Observation Data](https://www.earthdata.nasa.gov/news/impact-ibm-hls-foundation-model) 🟢 | NASA and IBM Research have collaborated to release the HLS Geospatial FM, an open-source geospatial AI model for Earth observation data. This model has shown success in various applications such as flood mapping, burn scar identification, and predicting crop yields. | [AI datasets 📊](topics/Topic_AI_datasets_📊.md), [AI for images 🖼️](topics/Topic_AI_for_images_🖼️.md), [Multimodal AI (image, video, audio) 📸](topics/Topic_Multimodal_AI_(image_video_audio)_📸.md), [Model release 🎉](topics/Topic_Model_release_🎉.md) | 2023-08-07 |
| [Meet FreeWilly, Our Large And Mighty Instruction Fine-Tuned Models](https://stability.ai/blog/freewilly-large-instruction-fine-tuned-models) 🟢 | Stability AI and CarperAI Lab have collaborated to release FreeWilly, a LLaMA 2 model fine-tuned using Supervised Fine-Tune (SFT) techniques. FreeWilly2 performs comparably to GPT-3.5 in certain tasks, and its capabilities have been verified by both Stability AI researchers and Hugging Face. Both models are publicly available under a non-commercial license. | [Model release 🎉](topics/Topic_Model_release_🎉.md), [Stability AI ⚖️](topics/Topic_Stability_AI_⚖️.md), [LLaMA 🦙](topics/Topic_LLaMA_🦙.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md) | 2023-07-31 |
| [Meta releases Llama 2](https://ai.meta.com/resources/models-and-libraries/llama/) 🟢 | Meta has released Llama-2, an open-source model with a commercial license, that showcases similar performance to ChatGPT. Trained on 2T tokens with varying parameter sizes, Llama-2 was further fine-tuned and improved using a combination of instruction and reinforcement learning, outperforming other open-source models like Falcon and MPT. | [Model release 🎉](topics/Topic_Model_release_🎉.md), [LLaMA 🦙](topics/Topic_LLaMA_🦙.md), [Meta ♾](topics/Topic_Meta_♾.md), [ChatGPT 💬](topics/Topic_ChatGPT_💬.md) | 2023-07-24 |
| [GPT-4 API general availability and deprecation of older models in the Completions API](https://openai.com/blog/gpt-4-api-general-availability) 🟢 | OpenAI has announced the general availability of the GPT-4 API and the automatic upgrade of GPT-3 models to new models from January 4, 2024. Developers using text-davinci-003 are advised to upgrade to gpt-3.5-turbo-instruct and specify it as the “model” in API requests for a smooth transition. OpenAI also offers priority access to GPT-3.5 Turbo and GPT-4 fine-tuning for users with fine-tuned older models, understanding the challenges of transitioning from these models. | [Model release 🎉](topics/Topic_Model_release_🎉.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md), [GPT-4 and GPT-4 turbo 🚀](topics/Topic_GPT-4_and_GPT-4_turbo_🚀.md), [OpenAI 🌟](topics/Topic_OpenAI_🌟.md) | 2023-07-10 |
| [Function calling and other API updates](https://openai.com/blog/function-calling-and-other-api-updates) 🟢 | OpenAI has released several new updates, including a 16k context version of GPT-3.5-turbo, a new API for calling user-defined functions, cost reductions for models and input tokens, and updated versions of GPT-4 and GPT-3.5-turbo. The updates aim to improve GPT’s capabilities and make it easier to connect with external tools and APIs. | [Model release 🎉](topics/Topic_Model_release_🎉.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md), [GPT-4 and GPT-4 turbo 🚀](topics/Topic_GPT-4_and_GPT-4_turbo_🚀.md), [OpenAI 🌟](topics/Topic_OpenAI_🌟.md) | 2023-06-20 |
| [RedPajama 7B now available, instruct model outperforms all open 7B models on HELM benchmarks](https://www.together.xyz/blog/redpajama-7b) 🟢 | Introducing the new RedPajama-INCITE models, optimized for few-shot tasks, which outperform similar models on HELM benchmarks. The project analyzed differences with previous models and incorporated community feedback. The models are available under Apache 2.0 license for AI professionals. | [Model release 🎉](topics/Topic_Model_release_🎉.md) | 2023-06-12 |
| [Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs, by MosaicML](https://www.mosaicml.com/blog/mpt-7b) 🟢 | MPT-7B is a transformer trained from scratch on 1T tokens of text and code. It is open-source, available for commercial use, and matches the quality of LLaMA-7B. | [Model release 🎉](topics/Topic_Model_release_🎉.md), [LLaMA 🦙](topics/Topic_LLaMA_🦙.md) | 2023-05-09 |
| [Releasing 3B and 7B RedPajama-INCITE family of models including base, instruction-tuned & chat models](https://www.together.xyz/blog/redpajama-models-v1) 🟢 | The release includes the first models trained on the RedPajama base dataset: a 3 billion and a 7B parameter base model that aims to replicate the LLaMA recipe as closely as possible. | [Model release 🎉](topics/Topic_Model_release_🎉.md), [LLaMA 🦙](topics/Topic_LLaMA_🦙.md) | 2023-05-09 |
| [Hugging Face and ServiceNow release a free code-generating model](https://techcrunch.com/2023/05/04/hugging-face-and-servicenow-release-a-free-code-generating-model/) 🟢 | AI startup Hugging Face and ServiceNow Research have released StarCoder, a free alternative to code-generating AI systems along the lines of GitHub’s Copilot. | [Hugging Face 🤗](topics/Topic_Hugging_Face_🤗.md), [AI for coding 👨‍💻](topics/Topic_AI_for_coding_👨‍💻.md), [Model release 🎉](topics/Topic_Model_release_🎉.md) | 2023-05-09 |
| [Meta introduced Segment Anything: Working toward the first foundation model for image segmentation](https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/) 🟢 | Introducing Segment Anything: democratizing image segmentation with SAM — a versatile, promptable model trained on a versatile dataset under Apache 2.0. | [AI for images 🖼️](topics/Topic_AI_for_images_🖼️.md), [Multimodal AI (image, video, audio) 📸](topics/Topic_Multimodal_AI_(image_video_audio)_📸.md), [Model release 🎉](topics/Topic_Model_release_🎉.md), [Meta ♾](topics/Topic_Meta_♾.md) | 2023-04-11 |
| [Releasing Alpaca-30B](https://abuqader.substack.com/p/releasing-alpaca-30b) 🟢 | Article explains release of Alpaca-30B, “instruction-tuned” version of Facebook’s Llama model, benefits of fine-tuning, usage steps and community involvement. | [Model release 🎉](topics/Topic_Model_release_🎉.md), [LLaMA 🦙](topics/Topic_LLaMA_🦙.md), [Meta ♾](topics/Topic_Meta_♾.md) | 2023-03-27 |
| [A New Open Source Flan 20B with UL2](https://www.yitay.net/blog/flan-ul2-20b) 🟢 | A UL2 model finetuned on the FLAN dataset (instruction tuning). An alternative to Flan-T5. | [AI datasets 📊](topics/Topic_AI_datasets_📊.md), [Model release 🎉](topics/Topic_Model_release_🎉.md) | 2023-03-06 |
| [Hugging Face releases SpeechT5](https://huggingface.co/blog/speecht5) 🟢 | a model able to do speech-to-text, text-to-speech, and speech-to-speech. | [Hugging Face 🤗](topics/Topic_Hugging_Face_🤗.md), [Text-to-speech 📢](topics/Topic_Text-to-speech_📢.md), [Speech-to-text 🎤](topics/Topic_Speech-to-text_🎤.md), [Multimodal AI (image, video, audio) 📸](topics/Topic_Multimodal_AI_(image_video_audio)_📸.md), [Model release 🎉](topics/Topic_Model_release_🎉.md) | 2023-02-13 |