# Model release news

| Title | Summary | Topics | Week |
| --- | --- | --- | --- |
| [OpenAI releases o1-preview model](https://openai.com/index/introducing-openai-o1-preview/) 🟢 | OpenAI introduces the o1-preview, the first in a new series of reasoning models significantly adept at complex tasks in science, coding, and math. These models outperform predecessors by employing advanced reasoning before responding, with test performances comparable to PhD students in rigorous fields. Despite lacking some GPT-4o features, o1-preview excels in specialized reasoning tasks, promising substantial AI advancements. | [Model release 🎉](Topic_Model_release.md), [OpenAI 🌟](Topic_OpenAI.md) | 2024-09-16 |
| [Mistral releases Pixtral 12B, its first multimodal model](https://techcrunch.com/2024/09/11/mistral-releases-pixtral-its-first-multimodal-model/) 🟢 | Mistral has introduced Pixtral 12B, a 12-billion-parameter multimodal AI model that processes both text and images. Building on features from their previous text model, Nemo 12B, Pixtral 12B is available for free download on GitHub and Hugging Face under an Apache 2.0 license. | [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md), [Model release 🎉](Topic_Model_release.md), [Mistral 🌬️](Topic_Mistral.md) | 2024-09-16 |
| [Alibaba releases new AI model Qwen2-VL that can analyze videos more than 20 minutes long](https://venturebeat.com/ai/alibaba-releases-new-ai-model-qwen2-vl-that-can-analyze-videos-more-than-20-minutes-long/) 🟢 | Alibaba Cloud’s new AI model, Qwen2-VL, excels in video analysis and multilingual comprehension, outperforming Meta’s Llama 3.1 and Google’s Gemini-1.5 in benchmarks. It supports multiple languages and extended video content analysis, and is available in three sizes, with two being open-sourced. | [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md), [Model release 🎉](Topic_Model_release.md), [Google Gemini 🌌](Topic_Google_Gemini.md) | 2024-09-09 |
| [OpenAI’s Strawberry AI is reportedly the secret sauce behind next-gen Orion language model](https://the-decoder.com/openais-strawberry-ai-is-reportedly-the-secret-sauce-behind-next-gen-orion-language-model/) 🟢 | OpenAI is working on “Strawberry,” an AI model focused on solving math and programming challenges, aimed at supporting “Orion,” the anticipated successor to GPT-4. Strawberry is slated for release in the fall and may augment ChatGPT with improved data generation and search abilities, having already displayed promising performance in tests and to national security stakeholders. | [AI for coding 👨‍💻](Topic_AI_for_coding.md), [Model release 🎉](Topic_Model_release.md), [ChatGPT 💬](Topic_ChatGPT.md), [GPT-4 and GPT-4 turbo 🚀](Topic_GPT-4_and_GPT-4_turbo.md), [OpenAI 🌟](Topic_OpenAI.md) | 2024-09-02 |
| [Runway’s Gen-3 Alpha Turbo is here and can make AI videos faster than you can type](https://venturebeat.com/ai/runways-gen-3-alpha-turbo-is-here-and-can-make-ai-videos-faster-than-you-can-type/) 🟢 | Runway ML introduces Gen-3 Alpha Turbo, an AI video generation model delivering 7x speed improvements and 50% cost reduction. Widely available across subscription plans, the model addresses diverse needs while promising advancements amidst ethical scrutiny, signaling Runway’s ambition for market leadership. | [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md), [Model release 🎉](Topic_Model_release.md) | 2024-08-26 |
| [Introducing SAM 2: The next generation of Meta Segment Anything Model for videos and images](https://ai.meta.com/blog/segment-anything-2/) 🟢 | Meta has launched SAM 2, an improved AI model for prompt-based real-time video and image segmentation, featuring zero-shot learning and requiring three times fewer interactions. SAM 2 is now available as open-source under the Apache 2.0 license. | [AI for images 🖼️](Topic_AI_for_images.md), [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md), [Model release 🎉](Topic_Model_release.md), [Meta ♾](Topic_Meta.md) | 2024-08-06 |
| [Google releases Gemma 2 2B](https://developers.googleblog.com/en/smaller-safer-more-transparent-advancing-responsible-ai-with-gemma/) 🟢 | Gemma 2, the latest AI release, features three key models: the efficient Gemma 2 2B which outperforms GPT-3.5 models in the Chatbot Arena, ShieldGemma for enhanced safety content classification, and Gemma Scope which provides advanced model interpretability. | [Model release 🎉](Topic_Model_release.md), [Google Gemini 🌌](Topic_Google_Gemini.md), [Google 🔍](Topic_Google.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2024-08-06 |
| [Meta releases Llama 3.1](https://ai.meta.com/blog/meta-llama-3-1/) 🟢 | Meta unveiled the Llama 3.1 405B model, a cutting-edge open-source large language AI with advanced multilingual, reasoning, and tool-use features, alongside improvements to its 8B and 70B models. The update offers extended context lengths, better training, and thorough evaluations, available for download on various platforms. | [Model release 🎉](Topic_Model_release.md), [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md) | 2024-07-29 |
| [Mistral releases Mistral Large 2](https://mistral.ai/news/mistral-large-2407/) 🟢 | Mistral launched their new model, Mistral Large 2, with 123 billion parameters and a 128k context window, offering multi-language and programming language support, optimized for high-throughput single-node inference. It delivers 84.0% accuracy on the MMLU benchmark, exhibits enhanced code generation, and reasoning capabilities. The model is available with research and commercial licensing options. | [AI for coding 👨‍💻](Topic_AI_for_coding.md), [Model release 🎉](Topic_Model_release.md), [Mistral 🌬️](Topic_Mistral.md) | 2024-07-29 |
| [GPT-4o mini: advancing cost-efficient intelligence](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) 🟢 | OpenAI has released GPT-4o mini, an advanced, cost-efficient AI model priced at $0.15/million input tokens and $0.60/million output tokens, offering superior performance at a lower cost than GPT-3.5 Turbo. | [Model release 🎉](Topic_Model_release.md), [GPT-4 and GPT-4 turbo 🚀](Topic_GPT-4_and_GPT-4_turbo.md), [OpenAI 🌟](Topic_OpenAI.md) | 2024-07-22 |
| [Mistral NeMo](https://mistral.ai/news/mistral-nemo/) 🟢 | Mistral, in collaboration with NVIDIA, has launched the 12B parameter Mistral NeMo model, featuring a 128k token context window, FP8 inference compatibility, and a cutting-edge Tekken tokenizer. It is open-sourced under Apache 2.0, boasts enhanced multilingual capabilities, and outperforms the previous 7B version in instruction-following tasks. | [Model release 🎉](Topic_Model_release.md), [NVIDIA 🎮](Topic_NVIDIA.md), [Mistral 🌬️](Topic_Mistral.md) | 2024-07-22 |
| [Codestral Mamba](https://mistral.ai/news/codestral-mamba/) 🟢 | Mistral has introduced Codestral Mamba, a new coding-centric Mamba model known for efficiently managing long sequences with linear time inference and theoretical support for unlimited sequence lengths. It competes with leading SOTA models and is open-source, accessible for extension through the GitHub repository with integration options like mistral-inference SDK, TensorRT-LLM, and an upcoming llama.cpp. | [AI for coding 👨‍💻](Topic_AI_for_coding.md), [Model release 🎉](Topic_Model_release.md), [Mistral 🌬️](Topic_Mistral.md) | 2024-07-22 |
| [Meta to drop Llama 3 400b next week — here’s why you should care](https://www.tomsguide.com/ai/meta-to-drop-llama-3-400b-next-week-heres-why-you-should-care) 🟢 | Meta plans to launch Llama 3 400B in July 2024, expanding the Llama 3 AI model series. This open-source model will offer improved features for chatbots and multilingual applications, aiming to provide wide access to the latest AI advancements. | [Model release 🎉](Topic_Model_release.md), [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md) | 2024-07-22 |
| [Stability AI releases a sound generator](https://techcrunch.com/2024/06/05/stability-ai-releases-a-sound-generator/) 🟢 | Stability AI has launched “Stable Audio Open,” an AI model that generates sound from text descriptions using royalty-free samples, geared towards non-commercial use. | [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md), [Model release 🎉](Topic_Model_release.md), [Stability AI ⚖️](Topic_Stability_AI.md) | 2024-06-10 |
| [Mistral releases Codestral](https://mistral.ai/news/codestral/) 🟢 | Codestral is Mistral AI’s new generative AI model focused on coding, boasting proficiency in over 80 programming languages and a large 32k context window for superior performance in benchmarks. | [AI for coding 👨‍💻](Topic_AI_for_coding.md), [Model release 🎉](Topic_Model_release.md), [Mistral 🌬️](Topic_Mistral.md) | 2024-06-03 |
| [Microsoft introduces Phi-Silica, a 3.3B parameter model made for Copilot+ PC NPUs](https://venturebeat.com/ai/microsoft-introduces-phi-silica-a-3-3b-parameter-model-made-for-copilot-pc-npus/) 🟢 | Microsoft has unveiled Phi-Silica, a compact language model with 3.3 billion parameters, tailored for Copilot+ PCs equipped with NPUs. This model is engineered for rapid on-device inferencing, improving productivity and accessibility for Windows users with optimal power efficiency. Phi-Silica is Microsoft’s inaugural local language model, with a release slated for June. | [Model release 🎉](Topic_Model_release.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [Microsoft 🪟](Topic_Microsoft.md) | 2024-05-27 |
| [mistralai/Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) 🟢 | Mistral has launched version 3 of their 7B model, the models “Mistral-7B-v0.3” and “Mistral-7B-Instruct-v0.3”. Enhancements include an expanded vocabulary of 32,768 terms, integration with the v3 Tokenizer, and new function calling capabilities. | [Mistral 🌬️](Topic_Mistral.md), [Model release 🎉](Topic_Model_release.md) | 2024-05-27 |
| [OpenAI releases GPT-4o](https://openai.com/index/spring-update/) 🟢 | OpenAI released the new model GPT-4o, capable of processing and generating text, audio, and image inputs and outputs. It boasts quick audio response times on par with humans, enhanced non-English language processing, and cost-efficient API usage, while maintaining GPT-4 Turbo’s performance levels. | [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md), [Model release 🎉](Topic_Model_release.md), [GPT-4 and GPT-4 turbo 🚀](Topic_GPT-4_and_GPT-4_turbo.md), [OpenAI 🌟](Topic_OpenAI.md) | 2024-05-21 |
| [IBM’s Granite code model family is going open source](https://research.ibm.com/blog/granite-code-models-open-source) 🟢 | IBM has released its Granite code models as open source. These models, trained on 116 languages with up to 34 billion parameters, facilitate code generation, bug fixing, and explanation tasks, and are accessible via GitHub and Hugging Face under the Apache 2.0 license. | [AI for coding 👨‍💻](Topic_AI_for_coding.md), [Model release 🎉](Topic_Model_release.md) | 2024-05-21 |
| [DeepMind releases AlphaFold 3](https://blog.google/technology/ai/google-deepmind-isomorphic-alphafold-3-ai-model/#life-molecules) 🟢 | AlphaFold 3 is an advanced AI model by Google DeepMind and Isomorphic Labs, capable of accurately predicting biomolecular structures and interactions. Providing a significant advancement over prior models, it enhances scientific research and drug development, and is available globally through the AlphaFold Server. | [AI in healthcare 🏥](Topic_AI_in_healthcare.md), [Model release 🎉](Topic_Model_release.md), [DeepMind 🧩](Topic_DeepMind.md), [Google 🔍](Topic_Google.md) | 2024-05-13 |
| [Apple releases OpenELM: small, open source AI models designed to run on-device](https://venturebeat.com/ai/apple-releases-openelm-small-open-source-ai-models-designed-to-run-on-device/) 🟢 | Apple has introduced OpenELM, a suite of open-source AI text generation models with 270M to 3B parameters, optimized for on-device deployment. Available on Hugging Face, these models are released under a sample code license to enable AI functionalities independently of the cloud. | [Hugging Face 🤗](Topic_Hugging_Face.md), [Apple 🍏](Topic_Apple.md), [Model release 🎉](Topic_Model_release.md) | 2024-04-29 |
| [Snowflake releases Arctic, an open LLM for Enterprise AI](https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/) 🟢 | Snowflake AI Research has released Arctic, a cost-effective enterprise AI LLM featuring a Dense-MoE Hybrid transformer architecture with 480 billion parameters. Trained for less than $2 million, Arctic excels in tasks like SQL generation and coding. It’s fully open-source under Apache 2.0, providing free access to model weights and code. | [Mixture-of-Experts 🤝](Topic_Mixture-of-Experts.md), [AI for coding 👨‍💻](Topic_AI_for_coding.md), [Model release 🎉](Topic_Model_release.md), [Funding 💰](Topic_Funding.md) | 2024-04-29 |
| [Introducing Meta Llama 3: The most capable openly available LLM to date](https://ai.meta.com/blog/meta-llama-3/) 🟢 | Meta has introduced Meta Llama 3, a state-of-the-art open-source large language model (LLM) with versions up to 70 billion parameters, providing enhanced reasoning and multilingual capabilities. The current best models are pretrained and instruction-fine-tuned at both 8B and 70B scales. Additionally, even larger models exceeding 400 billion parameters are in development, promising to push the boundaries further upon their release in the coming months. | [Model release 🎉](Topic_Model_release.md), [Meta ♾](Topic_Meta.md) | 2024-04-23 |
| [Mistral released Mixtral 8x22B](https://mistral.ai/news/mixtral-8x22b/) 🟢 | Mistral unveiled Mixtral 8x22B, an efficient sparse Mixture-of-Experts model with 39B active out of 141B total parameters, specializing in multilingual communication, coding, mathematics, and excelling in reasoning and knowledge tasks. The model boasts a 64K token context window, is compatible with multiple platforms, and is available under the open-source Apache 2.0 license. | [Mixture-of-Experts 🤝](Topic_Mixture-of-Experts.md), [AI for coding 👨‍💻](Topic_AI_for_coding.md), [Model release 🎉](Topic_Model_release.md), [Mistral 🌬️](Topic_Mistral.md) | 2024-04-23 |
| [Introducing DBRX: A New State-of-the-Art Open LLM](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm) 🟢 | Databricks has unveiled DBRX, a new open-source large language model (LLM) that surpasses GPT-3.5 in programming and general tasks, and is competitive with Gemini 1.0 Pro. DBRX features a mixture-of-experts architecture with a massive 132 billion parameters, though only 36 billion are active for any given input. Both DBRX Base and DBRX Instruct models are openly accessible on Hugging Face. | [Hugging Face 🤗](Topic_Hugging_Face.md), [Mixture-of-Experts 🤝](Topic_Mixture-of-Experts.md), [AI for coding 👨‍💻](Topic_AI_for_coding.md), [Model release 🎉](Topic_Model_release.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2024-04-02 |
| [Grok open release](https://github.com/xai-org/grok-1) 🟢 | xAI has released Grok-1, a language Mixture-of-Experts model with 314 billion parameters, following its pre-training in October 2023. This base model checkpoint is intended for further research and the development of conversational applications, and it is accessible under the Apache 2.0 license. | [Mixture-of-Experts 🤝](Topic_Mixture-of-Experts.md), [Grok 🐦](Topic_Grok.md), [Model release 🎉](Topic_Model_release.md) | 2024-03-18 |
| [Introducing the next generation of Claude](https://www.anthropic.com/news/claude-3-family) 🟢 | Anthropic has launched Claude 3, a new AI that surpasses GPT-4, with three models: Opus, Sonnet, and Haiku. Each supports a 200k context window, vision abilities, and multiple languages. Opus is touted as the top performer. Sonnet is integrated with Amazon Bedrock and Google Cloud’s Vertex AI, while Opus and Haiku are slated for future release along with new features like function calling and REPL. | [Claude 🖋️](Topic_Claude.md), [Anthropic 🌐](Topic_Anthropic.md), [Model release 🎉](Topic_Model_release.md), [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md), [Google 🔍](Topic_Google.md), [Amazon 🌳](Topic_Amazon.md), [GPT-4 and GPT-4 turbo 🚀](Topic_GPT-4_and_GPT-4_turbo.md) | 2024-03-11 |
| [Inflection-2.5: meet the world’s best personal AI](https://inflection.ai/inflection-2-5) 🟢 | Inflection has launched its latest AI version, Inflection-2.5, enhancing its AI model, Pi, with advanced cognitive capabilities that challenge leading language models like GPT-4. Notably, Inflection-2.5 achieves competitive performance in AI tasks, particularly in coding and math, with 40% less computational power required during its training phase. In addition to its improved processing efficiency, Pi now features the ability to conduct real-time web searches to provide updated news and information. | [AI for coding 👨‍💻](Topic_AI_for_coding.md), [Model release 🎉](Topic_Model_release.md), [GPT-4 and GPT-4 turbo 🚀](Topic_GPT-4_and_GPT-4_turbo.md) | 2024-03-11 |
| [Mistral AI releases new model to rival GPT-4 and its own chat assistant](https://techcrunch.com/2024/02/26/mistral-ai-releases-new-model-to-rival-gpt-4-and-its-own-chat-assistant/) 🟢 | Mistral has launched Mistral Large, ranking just below GPT4. It boasts a 32K token context window and multilingual support for English, French, Spanish, German, and Italian. The model excels in following precise instructions, allowing for tailored moderation policies. | [Model release 🎉](Topic_Model_release.md), [Mistral 🌬️](Topic_Mistral.md), [GPT-4 and GPT-4 turbo 🚀](Topic_GPT-4_and_GPT-4_turbo.md) | 2024-03-04 |
| [Gemma — a family of lightweight, state-of-the art open models from Google.](https://ai.google.dev/gemma/) 🟢 | Google has released Gemma, an open-source large language model based on Gemini, in two versions with 2 billion (2B) and 7 billion (7B) parameters. Both versions come with a basic pretrained model and an instruction-tuned variant to enhance performance. | [Model release 🎉](Topic_Model_release.md), [Google Gemini 🌌](Topic_Google_Gemini.md), [Google 🔍](Topic_Google.md) | 2024-02-26 |
| [OpenAI releases new embedding models and API updates](https://openai.com/blog/new-embedding-models-and-api-updates) 🟢 | OpenAI has announced updates to their AI model suite, including the launch of more efficient embedding models and cost-reduced versions of GPT-3.5 Turbo and a new GPT-4 Turbo model. The “text-embedding-3-large” leads with a 64.6% MTEB score at $0.00013 per 1k tokens, while the “text-embedding-3-small” offers improved performance over its predecessor at a fivefold cost reduction. Additionally, the “gpt-3.5-turbo-0125” is now 50% cheaper, priced at $0.0005 per 1k tokens, and a new “gpt-4–0125-preview” model has been introduced. | [Model release 🎉](Topic_Model_release.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md), [GPT-4 and GPT-4 turbo 🚀](Topic_GPT-4_and_GPT-4_turbo.md), [OpenAI 🌟](Topic_OpenAI.md) | 2024-01-29 |
| [LangChain v0.1.0](https://blog.langchain.dev/langchain-v0-1-0/) 🟢 | LangChain has released its first stable and backward-compatible version, v0.1.0. This release brings better observability and debugging capabilities, including performance tracking and insight tools, and introduces a new versioning system for clear API and feature updates. | [LangChain and LlamaIndex 🔗](Topic_LangChain_and_LlamaIndex.md), [Model release 🎉](Topic_Model_release.md) | 2024-01-15 |
| [Midjourney v6 release](https://mid-journey.ai/midjourney-v6-release/) 🟢 | The latest update for Midjourney, version 6, introduces features such as enhanced prompt accuracy and length, increased coherence, better image prompting, and an improved remix mode. Additionally, a new feature for minor text drawing has been added, which can be utilized by including text within quotations. | [Midjourney 🛤️](Topic_Midjourney.md), [Model release 🎉](Topic_Model_release.md) | 2024-01-02 |
| [Phi-2: The surprising power of small language models](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/) 🟢 | Microsoft has released Phi-2, a language model with 2.7 billion parameters that outperforms models up to 25 times its size. Phi-2 achieves remarkable reasoning and language understanding abilities using high-quality data and synthetic datasets. It outperforms larger models on challenging benchmarks, especially in tasks like coding and math. | [AI for coding 👨‍💻](Topic_AI_for_coding.md), [Model release 🎉](Topic_Model_release.md), [Microsoft 🪟](Topic_Microsoft.md) | 2023-12-19 |
| [Introducing gigaGPT: GPT-3 sized models in 565 lines of code](https://www.cerebras.net/blog/introducing-gigagpt-gpt-3-sized-models-in-565-lines-of-code) 🟢 | Cerebras has released gigaGPT, a model implementation similar to nanoGPT but with over 100 billion parameters. By leveraging Cerebras hardware and different optimizers, gigaGPT overcomes the limitations of GPU memory and the need for complex scaling frameworks, offering a simplified approach for training large models. | [Model release 🎉](Topic_Model_release.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2023-12-19 |
| [Mistral AI has released Mixtral 8x7B, an open-source sparse mixture-of-experts model](https://mistral.ai/news/mixtral-of-experts/) 🟢 | Mistral AI has introduced Mixtral 8x7B, an open-source, sparse mixture-of-experts model with 45B parameters. It outperforms LLaMA 2 70B and GPT-3.5 in NLP benchmarks, while also providing 6x faster inference speed. The model selectively uses only 12B parameters per inference, keeping operational costs equivalent to a 12B model. The Mixtral model is accessible through Mistral’s beta API and dev platform. | [Mixture-of-Experts 🤝](Topic_Mixture-of-Experts.md), [Model release 🎉](Topic_Model_release.md), [Mistral 🌬️](Topic_Mistral.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2023-12-11 |
| [Kyutai is a French AI research lab with a $330 million budget that will make everything open source](https://techcrunch.com/2023/11/17/kyutai-is-an-french-ai-research-lab-with-a-330-million-budget-that-will-make-everything-open-source/) 🟢 | Paris-based AI research lab, Kyutai, secures $330 million in funding to advance the development of artificial general intelligence. With these resources, Kyutai plans to conduct comprehensive research led by PhD students, postdocs, and researchers. Additionally, the lab prioritizes transparency in AI by openly sharing their models, source code, and data. | [Funding 💰](Topic_Funding.md), [Model release 🎉](Topic_Model_release.md), [AI datasets 📊](Topic_AI_datasets.md) | 2023-11-20 |
| [Jina AI Launches World’s First Open-Source 8K Text Embedding, Rivaling OpenAI](https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/) 🟢 | Jina AI introduces jina-embeddings-v2, an open-source embedding model that supports 8K context length. It matches OpenAI’s 8K model in key areas like Classification Average, Reranking Average, Retrieval Average, and Summarization Average in the MTEB leaderboard. | [Model release 🎉](Topic_Model_release.md), [OpenAI 🌟](Topic_OpenAI.md) | 2023-10-30 |
| [Meta quietly unveils Llama 2 Long AI that beats GPT-3.5 Turbo and Claude 2 on some tasks](https://venturebeat.com/ai/meta-quietly-releases-llama-2-long-ai-that-outperforms-gpt-3-5-and-claude-2-on-some-tasks/) 🟢 | Meta is releasing Llama 2 Long, an enhanced version of Llama 2 that underwent continual pretraining with longer training sequences and upsampled long texts. By adding 400 billion tokens and making minor changes to the Rotary Positional Embedding (RoPE), Llama 2 Long can now attend to longer information sequences and include less related information in its model’s knowledge base. | [Model release 🎉](Topic_Model_release.md), [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2023-10-10 |
| [Adobe’s Firefly generative AI models are now generally available](https://techcrunch.com/2023/09/13/adobes-firefly-generative-ai-models-are-now-generally-available-get-pricing-plans/) 🟢 | Adobe has released commercially available generative AI models in their Creative Cloud, including a standalone web app called Firefly. The new “generative credits” system controls user interactions with Firefly’s AI models, with each click on ‘generate’ using one credit. | [AI for images 🖼️](Topic_AI_for_images.md), [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md), [Model release 🎉](Topic_Model_release.md) | 2023-09-18 |
| [Spread Your Wings: Falcon 180B is here](https://huggingface.co/blog/falcon-180b) 🟢 | TII has just released Falcon 180B, a powerful language model with 180 billion parameters trained on 3.5 trillion tokens. Outperforming Llama 2 70B and GPT-3.5 on MMLU, Falcon 180B performs great and ranks high on the Hugging Face Leaderboard. This model is available for commercial use but has strict terms excluding “hosting use.” | [Hugging Face 🤗](Topic_Hugging_Face.md), [Model release 🎉](Topic_Model_release.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2023-09-11 |
| [Releasing Persimmon-8B](https://www.adept.ai/blog/persimmon-8b) 🟢 | Adept.ai introduces Persimmon-8B, an open-source LLM with impressive performance and a compact size. Trained on less data, it achieves comparable results to LLaMA2 and offers fast C++ implementation combined with flexible Python inference. | [Model release 🎉](Topic_Model_release.md), [LLaMA 🦙](Topic_LLaMA.md) | 2023-09-11 |
| [AudioCraft: A simple one-stop shop for audio modeling](https://ai.meta.com/blog/audiocraft-musicgen-audiogen-encodec-generative-ai-audio/) 🟢 | Meta has released the code and weights for their AudioCraft models, including MusicGen and AudioGen. These models generate music and audio respectively, based on text-based user inputs. The release also includes the EnCodec decoder, which improves music quality. | [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md), [Model release 🎉](Topic_Model_release.md), [Meta ♾](Topic_Meta.md) | 2023-08-07 |
| [NASA and IBM Openly Release Geospatial AI Foundation Model for NASA Earth Observation Data](https://www.earthdata.nasa.gov/news/impact-ibm-hls-foundation-model) 🟢 | NASA and IBM Research have collaborated to release the HLS Geospatial FM, an open-source geospatial AI model for Earth observation data. This model has shown success in various applications such as flood mapping, burn scar identification, and predicting crop yields. | [AI datasets 📊](Topic_AI_datasets.md), [AI for images 🖼️](Topic_AI_for_images.md), [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md), [Model release 🎉](Topic_Model_release.md) | 2023-08-07 |
| [Meet FreeWilly, Our Large And Mighty Instruction Fine-Tuned Models](https://stability.ai/blog/freewilly-large-instruction-fine-tuned-models) 🟢 | Stability AI and CarperAI Lab have collaborated to release FreeWilly, a LLaMA 2 model fine-tuned using Supervised Fine-Tune (SFT) techniques. FreeWilly2 performs comparably to GPT-3.5 in certain tasks, and its capabilities have been verified by both Stability AI researchers and Hugging Face. Both models are publicly available under a non-commercial license. | [Model release 🎉](Topic_Model_release.md), [Stability AI ⚖️](Topic_Stability_AI.md), [LLaMA 🦙](Topic_LLaMA.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2023-07-31 |
| [Meta releases Llama 2](https://ai.meta.com/resources/models-and-libraries/llama/) 🟢 | Meta has released Llama-2, an open-source model with a commercial license, that showcases similar performance to ChatGPT. Trained on 2T tokens with varying parameter sizes, Llama-2 was further fine-tuned and improved using a combination of instruction and reinforcement learning, outperforming other open-source models like Falcon and MPT. | [Model release 🎉](Topic_Model_release.md), [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md), [ChatGPT 💬](Topic_ChatGPT.md) | 2023-07-24 |
| [GPT-4 API general availability and deprecation of older models in the Completions API](https://openai.com/blog/gpt-4-api-general-availability) 🟢 | OpenAI has announced the general availability of the GPT-4 API and the automatic upgrade of GPT-3 models to new models from January 4, 2024. Developers using text-davinci-003 are advised to upgrade to gpt-3.5-turbo-instruct and specify it as the “model” in API requests for a smooth transition. OpenAI also offers priority access to GPT-3.5 Turbo and GPT-4 fine-tuning for users with fine-tuned older models, understanding the challenges of transitioning from these models. | [Model release 🎉](Topic_Model_release.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md), [GPT-4 and GPT-4 turbo 🚀](Topic_GPT-4_and_GPT-4_turbo.md), [OpenAI 🌟](Topic_OpenAI.md) | 2023-07-10 |
| [Function calling and other API updates](https://openai.com/blog/function-calling-and-other-api-updates) 🟢 | OpenAI has released several new updates, including a 16k context version of GPT-3.5-turbo, a new API for calling user-defined functions, cost reductions for models and input tokens, and updated versions of GPT-4 and GPT-3.5-turbo. The updates aim to improve GPT’s capabilities and make it easier to connect with external tools and APIs. | [Model release 🎉](Topic_Model_release.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md), [GPT-4 and GPT-4 turbo 🚀](Topic_GPT-4_and_GPT-4_turbo.md), [OpenAI 🌟](Topic_OpenAI.md) | 2023-06-20 |
| [RedPajama 7B now available, instruct model outperforms all open 7B models on HELM benchmarks](https://www.together.xyz/blog/redpajama-7b) 🟢 | Introducing the new RedPajama-INCITE models, optimized for few-shot tasks, which outperform similar models on HELM benchmarks. The project analyzed differences with previous models and incorporated community feedback. The models are available under Apache 2.0 license for AI professionals. | [Model release 🎉](Topic_Model_release.md) | 2023-06-12 |
| [Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs, by MosaicML](https://www.mosaicml.com/blog/mpt-7b) 🟢 | MPT-7B is a transformer trained from scratch on 1T tokens of text and code. It is open-source, available for commercial use, and matches the quality of LLaMA-7B. | [Model release 🎉](Topic_Model_release.md), [LLaMA 🦙](Topic_LLaMA.md) | 2023-05-09 |
| [Releasing 3B and 7B RedPajama-INCITE family of models including base, instruction-tuned & chat models](https://www.together.xyz/blog/redpajama-models-v1) 🟢 | The release includes the first models trained on the RedPajama base dataset: a 3 billion and a 7B parameter base model that aims to replicate the LLaMA recipe as closely as possible. | [Model release 🎉](Topic_Model_release.md), [LLaMA 🦙](Topic_LLaMA.md) | 2023-05-09 |
| [Hugging Face and ServiceNow release a free code-generating model](https://techcrunch.com/2023/05/04/hugging-face-and-servicenow-release-a-free-code-generating-model/) 🟢 | AI startup Hugging Face and ServiceNow Research have released StarCoder, a free alternative to code-generating AI systems along the lines of GitHub’s Copilot. | [Hugging Face 🤗](Topic_Hugging_Face.md), [AI for coding 👨‍💻](Topic_AI_for_coding.md), [Model release 🎉](Topic_Model_release.md) | 2023-05-09 |
| [Meta introduced Segment Anything: Working toward the first foundation model for image segmentation](https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/) 🟢 | Introducing Segment Anything: democratizing image segmentation with SAM — a versatile, promptable model trained on a versatile dataset under Apache 2.0. | [AI for images 🖼️](Topic_AI_for_images.md), [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md), [Model release 🎉](Topic_Model_release.md), [Meta ♾](Topic_Meta.md) | 2023-04-11 |
| [Releasing Alpaca-30B](https://abuqader.substack.com/p/releasing-alpaca-30b) 🟢 | Article explains release of Alpaca-30B, “instruction-tuned” version of Facebook’s Llama model, benefits of fine-tuning, usage steps and community involvement. | [Model release 🎉](Topic_Model_release.md), [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md) | 2023-03-27 |
| [A New Open Source Flan 20B with UL2](https://www.yitay.net/blog/flan-ul2-20b) 🟢 | A UL2 model finetuned on the FLAN dataset (instruction tuning). An alternative to Flan-T5. | [AI datasets 📊](Topic_AI_datasets.md), [Model release 🎉](Topic_Model_release.md) | 2023-03-06 |
| [Hugging Face releases SpeechT5](https://huggingface.co/blog/speecht5) 🟢 | a model able to do speech-to-text, text-to-speech, and speech-to-speech. | [Hugging Face 🤗](Topic_Hugging_Face.md), [Text-to-speech 📢](Topic_Text-to-speech.md), [Speech-to-text 🎤](Topic_Speech-to-text.md), [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md), [Model release 🎉](Topic_Model_release.md) | 2023-02-13 |