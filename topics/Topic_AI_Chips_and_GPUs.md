# AI Chips and GPUs news

| Title | Summary | Topics | Week |
| --- | --- | --- | --- |
| [Anthropic raises another $4B from Amazon, makes AWS its “primary” training partner](https://techcrunch.com/2024/11/22/anthropic-raises-an-additional-4b-from-amazon-makes-aws-its-primary-cloud-partner/) 🟢 | Anthropic raised $4 billion from Amazon, making AWS its main AI training partner. They collaborate on Trainium accelerator development and integrate Claude models on Amazon’s platform. Amazon’s investment totals $8 billion, creating regulatory interest. | [Amazon 🌳](Topic_Amazon.md), [Funding 💰](Topic_Funding.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [Claude 🖋️](Topic_Claude.md), [Anthropic 🌐](Topic_Anthropic.md), [AI regulation 📜](Topic_AI_regulation.md) | 2024-11-25 |
| [OpenAI and rivals seek new path to smarter AI as current methods hit limitations](https://finance.yahoo.com/news/openai-rivals-seek-path-smarter-100616130.html) 🟢 | AI companies like OpenAI are innovating training techniques to overcome limits of large language models. Techniques behind OpenAI’s o1 model focus on human-like reasoning, shifting away from “bigger is better.” This approach may reshape AI resources and impact Nvidia’s dominance in AI chips. Rivals are also pursuing similar advancements. | [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [OpenAI 🌟](Topic_OpenAI.md) | 2024-11-18 |
| [Amazon ready to use its own AI chips, reduce its dependence on Nvidia](https://arstechnica.com/ai/2024/11/amazon-ready-to-use-its-own-ai-chips-reduce-its-dependence-on-nvidia/) 🟢 | Amazon is set to launch Trainium 2 AI chips, aiming to reduce reliance on Nvidia and cut costs for Amazon Web Services (AWS) customers. These chips promise efficiency and savings, attracting users like Anthropic and Databricks. This move highlights a growing trend of tech giants developing custom chips to drive AI growth. | [Amazon 🌳](Topic_Amazon.md), [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [Anthropic 🌐](Topic_Anthropic.md) | 2024-11-18 |
| [Meta is using more than 100,000 Nvidia H100 AI GPUs to train Llama-4](https://www.tomshardware.com/tech-industry/artificial-intelligence/meta-is-using-more-than-100-000-nvidia-h100-ai-gpus-to-train-llama-4-mark-zuckerberg-says-that-llama-4-is-being-trained-on-a-cluster-bigger-than-anything-that-ive-seen) 🟢 | Meta is utilizing over 100,000 Nvidia H100 AI GPUs to develop Llama 4, an advanced AI model with improved modalities and reasoning capabilities, positioning itself competitively against Microsoft and Google. Despite the significant power demands, Meta plans to release Llama models for free to encourage broader development and application. | [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md), [Model release 🎉](Topic_Model_release.md) | 2024-11-11 |
| [Introducing the First AMD 1B Language Models: AMD OLMo](https://www.amd.com/en/developer/resources/technical-articles/introducing-the-first-amd-1b-language-model.html) 🟢 | AMD has introduced AMD OLMo, a series of 1 billion parameter language models trained on 1.3 trillion tokens using AMD Instinct MI250 GPUs. These open-sourced models excel in reasoning and instruction-following, outperforming similar-sized models in general reasoning and chat benchmarks. | [Model release 🎉](Topic_Model_release.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2024-11-11 |
| [OpenAI will start using AMD chips and could make its own AI hardware in 2026](https://www.theverge.com/2024/10/29/24282843/openai-custom-hardware-amd-nvidia-ai-chips) 🟢 | OpenAI is enhancing its hardware strategy by integrating AMD chips via Microsoft Azure and aims to develop its own AI-specific hardware by 2026. Collaborating with Broadcom and securing production capacity with TSMC, OpenAI still faces challenges in matching the advanced custom AI chip technology of Google, Microsoft, and Amazon. | [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [Microsoft 🪟](Topic_Microsoft.md), [OpenAI 🌟](Topic_OpenAI.md) | 2024-11-11 |
| [AMD is turning its back on flagship gaming GPUs to chase AI first](https://www.theverge.com/2024/9/9/24240173/amd-udna-gpu-ai-gaming-rdna-cdna-jack-huynh) 🟢 | AMD is prioritizing AI development over flagship gaming GPUs to achieve a larger market share and attract developer support. According to Jack Huynh, the goal is to reach a 40% market share to compete with Nvidia and optimize AMD platforms for developers before potentially re-focusing on gaming GPUs. | [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2024-09-16 |
| [Elon Musk is putting his AI chips to work — and he’s catching up with Mark Zuckerberg](https://www.businessinsider.com/elon-musk-xai-chips-mark-zuckerberg-2024-9) 🟢 | Elon Musk’s xAI has launched Colossus, a major training cluster boasting 100,000 Nvidia H100 GPUs, making it the world’s most powerful AI system. Built in 122 days in Memphis and set to double in capacity soon, this development comes amid a global GPU shortage, with rivals like Meta and Microsoft also competing for AI supremacy. | [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [Meta ♾](Topic_Meta.md), [Microsoft 🪟](Topic_Microsoft.md) | 2024-09-09 |
| [Introducing Cerebras Inference: AI at Instant Speed](https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed) 🟢 | Cerebras has achieved a significant speed advantage in AI language model inference, delivering 1,800 tokens per second on Llama3.1 8B and 450 tokens per second on Llama3.1 70B models, outperforms NVIDIA’s GPU-based solutions by 20-fold and is 2.4 times faster than Groq for the 8B model. Notably, Cerebras stands alone in offering immediate responses at a rate of 450 tokens per second on the 70B model. | [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [LLaMA 🦙](Topic_LLaMA.md) | 2024-09-02 |
| [AMD to acquire infrastructure player ZT Systems for $4.9B to amp up its AI ecosystem play](https://techcrunch.com/2024/08/19/amd-to-acquire-infrastructure-player-zt-systems-for-4-9b-to-amp-up-its-ai-ecosystem-play/) 🟢 | AMD has acquired AI-focused infrastructure company ZT Systems for $4.9 billion, aiming to enhance their data center offerings with ZT’s specialized systems design expertise in AI applications. | [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2024-08-26 |
| [AMD is becoming an AI chip company, just like Nvidia](https://www.theverge.com/2024/7/30/24209938/amd-q2-2024-earnings-datacenter-ai-revenue) 🟢 | AMD’s Q2 2024 earnings highlight a strategic shift towards AI, with data center products like the Instinct MI300 accelerator leading sales, which have surged by 115%. The MI300 broke $1 billion in quarterly sales, indicating AMD’s intent to annually release AI chips to rival Nvidia’s market dominance. | [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2024-08-12 |
| [Groq Raises $640M To Meet Soaring Demand for Fast AI Inference](https://wow.groq.com/news_press/groq-raises-640m-to-meet-soaring-demand-for-fast-ai-inference/) 🟢 | Groq, an AI hardware company, has raised $640 million in a Series D round led by BlackRock, reaching a $2.8 billion valuation. The investment will expand Groq’s capabilities by more than 100,000 LPUs to support growing demand from enterprises and developers, and will enable the company to hire industry experts to drive further growth. | [Funding 💰](Topic_Funding.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2024-08-12 |
| [Elon Musk: Grok 2 AI Arrives in August](https://www.pcmag.com/news/elon-musk-grok-2-ai-arrives-in-august) 🟢 | Elon Musk has unveiled plans for Grok 2, a new AI model expected in August 2024, promising enhanced efficiency. His company anticipates an upgrade to Grok 3 by the end of the same year, utilizing cutting-edge Nvidia GPU technology. | [Grok 🐦](Topic_Grok.md), [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2024-07-08 |
| [Nvidia shipped 3.76M data center GPUs in 2023 — dominates business with 98% revenue share](https://www.tomshardware.com/tech-industry/nvidia-shipped-376m-data-center-gpus-in-2023-dominates-business-with-98-revenue-share) 🟢 | In 2023, Nvidia consolidated its position in the data center GPU market with a 98% share by distributing 3.76 million units and achieved a remarkable 126% revenue increase since 2020, reaching $60.9 billion, even amidst U.S. export restrictions and manufacturing hurdles. | [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2024-06-17 |
| [AMD unveils new AI chips to compete with Nvidia](https://www.fastcompany.com/91134766/amd-unveils-new-ai-chips-to-compete-with-nvidia) 🟢 | AMD is challenging Nvidia’s leadership in AI with upcoming releases: the MI325X in 2024, and the MI350/MI400 series in 2025–2026, promising notable performance boosts to satisfy increasing AI demands. | [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2024-06-10 |
| [China invests $47 billion in largest ever chip fund](https://techxplore.com/news/2024-05-china-invests-billion-largest-chip.html) 🟢 | China allocated $47.48 billion to a new chip fund aimed at advancing domestic semiconductor production, a critical step toward self-sufficiency and competitiveness in technology sectors, including AI. | [Funding 💰](Topic_Funding.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2024-06-03 |
| [Nvidia Stock Surges as Sales Forecast Delivers on AI Hopes](https://finance.yahoo.com/news/nvidia-forecast-shatters-estimates-ai-210754051.html) 🟢 | Nvidia’s stock surged 9.3% after a promising sales forecast, pointing to a robust demand for AI technologies. The $28 billion projected Q2 revenue exceeds expectations, highlighting the company’s strong position in the AI market, buoyed by their new Blackwell chips and significant data-center revenue. | [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2024-05-27 |
| [Microsoft introduces Phi-Silica, a 3.3B parameter model made for Copilot+ PC NPUs](https://venturebeat.com/ai/microsoft-introduces-phi-silica-a-3-3b-parameter-model-made-for-copilot-pc-npus/) 🟢 | Microsoft has unveiled Phi-Silica, a compact language model with 3.3 billion parameters, tailored for Copilot+ PCs equipped with NPUs. This model is engineered for rapid on-device inferencing, improving productivity and accessibility for Windows users with optimal power efficiency. Phi-Silica is Microsoft’s inaugural local language model, with a release slated for June. | [Model release 🎉](Topic_Model_release.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [Microsoft 🪟](Topic_Microsoft.md) | 2024-05-27 |
| [100 things Google announced at I/O 2024](https://blog.google/technology/ai/google-io-2024-100-announcements/) 🟢 | At Google I/O 2024, notable AI developments were announced such as Gemini 1.5 models, Trillium TPU, and enhanced AI in Google Search. Key introductions include Imagen 3 for image creation, Veo for video generation, and upgraded features in the Gemini app for premium users, alongside new generative media tools. | [Google Gemini 🌌](Topic_Google_Gemini.md), [Google 🔍](Topic_Google.md), [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md), [AI for images 🖼️](Topic_AI_for_images.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2024-05-21 |
| [Hugging Face is sharing $10 million worth of compute to help beat the big AI companies](https://www.theverge.com/2024/5/16/24156755/hugging-face-celement-delangue-free-shared-gpus-ai) 🟢 | Hugging Face is dedicating $10M in free GPU resources to support AI developers, startups, and academics. Their ZeroGPU initiative, part of Hugging Face Spaces, offers communal GPU access, aiming to reduce computational access barriers and improve cost-efficiency. | [Hugging Face 🤗](Topic_Hugging_Face.md), [Funding 💰](Topic_Funding.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2024-05-21 |
| [Google’s new chips look to challenge Nvidia, Microsoft and Amazon](https://qz.com/google-ai-chip-nvidia-axion-arm-microsoft-1851397201) 🟢 | Google has unveiled the Cloud TPU v5p, an AI chip that delivers nearly triple the training speed of its predecessor, the TPU v4, reinforcing its position in AI services and hardware. At the Google Cloud Next event, CEO Pichai highlighted the company’s AI advancements and collaborations, including the use of the A3 supercomputer and Blackwell chips in the AI Hypercomputer. Additionally, Google introduced the Google Axion CPU, an Arm-based processor that competes with similar offerings from Microsoft and Amazon, boasting a 30% performance improvement and better energy efficiency. | [Amazon 🌳](Topic_Amazon.md), [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [Google 🔍](Topic_Google.md), [Microsoft 🪟](Topic_Microsoft.md) | 2024-04-23 |
| [Lambda Announces $500M GPU-Backed Facility to Expand Cloud for AI](https://www.businesswire.com/news/home/20240402148086/en/Lambda-Announces-500M-GPU-Backed-Facility-to-Expand-Cloud-for-AI) 🟢 | Lambda has successfully secured $500 million in funding to enhance its AI-oriented cloud services, powered by NVIDIA GPUs, following a Series C investment round. | [Funding 💰](Topic_Funding.md), [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2024-04-08 |
| [OpenAI and Microsoft reportedly planning $100 billion datacenter project for an AI supercomputer](https://www.tomshardware.com/tech-industry/artificial-intelligence/openai-and-microsoft-reportedly-planning-dollar100-billion-datacenter-project-for-an-ai-supercomputer) 🟢 | Microsoft and OpenAI have announced a partnership to construct “Stargate,” an advanced AI supercomputer in the U.S., featuring millions of GPUs. The project, which may exceed $115 billion, represents a major commitment to expanding datacenter capabilities to advance AI research and development. | [Funding 💰](Topic_Funding.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [Microsoft 🪟](Topic_Microsoft.md), [OpenAI 🌟](Topic_OpenAI.md) | 2024-04-02 |
| [Amazon and Anthropic deepen their shared commitment to advancing generative AI](https://www.aboutamazon.com/news/company-news/amazon-anthropic-ai-investment) 🟢 | Amazon has invested $4 billion in AI company Anthropic to further develop AI technologies. Anthropic leverages Amazon Web Services (AWS) Trainium and Inferentia chips for enhancing their AI models. Notably, Anthropic’s Claude 3 models have been incorporated into Amazon Bedrock by AWS. | [Amazon 🌳](Topic_Amazon.md), [Funding 💰](Topic_Funding.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [Claude 🖋️](Topic_Claude.md), [Anthropic 🌐](Topic_Anthropic.md) | 2024-04-02 |
| [‘We Created a Processor for the Generative AI Era,’ NVIDIA CEO Says](https://blogs.nvidia.com/blog/2024-gtc-keynote/) 🟢 | NVIDIA CEO Jensen Huang announced the NVIDIA Blackwell computing platform at the GTC conference, aimed at advancing generative AI with superior training and inference capabilities. The platform includes enhanced interconnects for better performance and scalability. NVIDIA also launched NIM microservices for tailored AI deployment and Omniverse Cloud APIs for sophisticated simulation, signaling a transformative impact on sectors like healthcare and robotics. | [AI in healthcare 🏥](Topic_AI_in_healthcare.md), [Robotics 🤖](Topic_Robotics.md), [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2024-03-25 |
| [Nvidia’s data center GPU sales grow by a stunning 409% on huge demand for AI chips](https://siliconangle.com/2024/02/21/nvidias-data-center-gpu-sales-grow-stunning-409-huge-demand-ai-chips/) 🟢 | Nvidia has experienced a significant surge in GPU sales, reporting a 409% increase due in large part to the rising demand for AI technologies. With Q4 earnings and revenue significantly outpacing Wall Street forecasts, the company’s financials have thrived on the back of robust sales from their Hopper GPU series, including the H100. | [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2024-02-26 |
| [GPU cloud company Together AI to raise $100m](https://www.datacenterdynamics.com/en/news/together-ai-set-to-receive-100m-in-funding-round-led-by-salesforce-ventures/) 🟢 | Together AI, a GPU cloud company specializing in open-source AI tools and Nvidia server chip access, is nearing a $100 million funding round led by Salesforce Ventures, potentially elevating its valuation to $1 billion. | [Funding 💰](Topic_Funding.md), [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2024-02-19 |
| [OpenAI’s CEO Sam Altman is chasing trillions of dollars as investments to disrupt AI, chip industries](https://www.firstpost.com/tech/openai-sam-altman-is-chasing-trillions-of-dollars-as-investments-to-disrupt-ai-chip-industries-13708732.html) 🟢 | Sam Altman, CEO of OpenAI, is actively seeking to secure $5–7 trillion in funding to expand the semiconductor industry to support AI development. This investment aims to address GPU shortages and foster the growth of both AI and artificial general intelligence. Altman is engaging with various stakeholders, including government officials from the UAE and the US, investors, and chip manufacturers, in his efforts to build a robust global chip-making infrastructure that meets the increasing demands and energy requirements of AI facilities. | [Funding 💰](Topic_Funding.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [OpenAI 🌟](Topic_OpenAI.md) | 2024-02-12 |
| [OpenAI’s Sam Altman Is Raising Money to Set Up AI Chip Factories](https://beebom.com/openai-sam-altman-raising-money-ai-chip-factories/) 🟢 | OpenAI CEO Sam Altman is actively seeking investment, potentially over $8 billion, from entities including G42 and SoftBank to establish AI chip factories aimed at meeting the surging demand for specialized processors in the growing AI industry. | [Funding 💰](Topic_Funding.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [OpenAI 🌟](Topic_OpenAI.md) | 2024-01-29 |
| [Hugging Face and Google partner for open AI collaboration](https://huggingface.co/blog/gcp-partnership) 🟢 | Hugging Face has partnered with Google Cloud, providing users with access to enhanced AI models and integration with Google Cloud services like GKE and Vertex AI, utilizing Google TPUs and NVIDIA H100 GPUs. | [Hugging Face 🤗](Topic_Hugging_Face.md), [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [Google 🔍](Topic_Google.md) | 2024-01-29 |
| [Mark Zuckerberg indicates Meta is spending billions of dollars on Nvidia AI chips](https://www.cnbc.com/2024/01/18/mark-zuckerberg-indicates-meta-is-spending-billions-on-nvidia-ai-chips.html) 🟢 | Meta plans a significant investment in AI research by integrating 350,000 Nvidia H100 GPUs by 2024. Given their high cost — estimated between $25K-$30K — this investment underlines Meta’s commitment to scaling up computing power. Overall, Meta’s strategy to amass the computational equivalent of 600K H100 GPUs highlights a substantial push to enhance its AI capabilities. | [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [Meta ♾](Topic_Meta.md) | 2024-01-22 |
| [Cristiano Amon: generative AI is ‘evolving very, very fast’ into mobile devices](https://www.ft.com/content/dbc0984b-4801-4aeb-bcab-480704c34161) 🟢 | Qualcomm CEO Cristiano Amon envisions generative AI rapidly integrating into mobiles, PCs, and cars, aiming to offer enriched user experiences by complementing cloud AI. Leveraging Qualcomm’s efficient AI processors, these advancements are set to facilitate real-time AI applications on battery- operated devices, proactively meeting user needs. | [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2024-01-08 |
| [Introducing gigaGPT: GPT-3 sized models in 565 lines of code](https://www.cerebras.net/blog/introducing-gigagpt-gpt-3-sized-models-in-565-lines-of-code) 🟢 | Cerebras has released gigaGPT, a model implementation similar to nanoGPT but with over 100 billion parameters. By leveraging Cerebras hardware and different optimizers, gigaGPT overcomes the limitations of GPU memory and the need for complex scaling frameworks, offering a simplified approach for training large models. | [Model release 🎉](Topic_Model_release.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2023-12-19 |
| [Nvidia unveils H200, its newest high-end chip for training AI models](https://www.cnbc.com/2023/11/13/nvidia-unveils-h200-its-newest-high-end-chip-for-training-ai-models.html) 🟢 | Nvidia introduces the H200 GPU, an upgraded version with 141GB of high-bandwidth memory. This enhancement helps with the inference process in training large AI models. Set to be released in Q2 2024, the H200 will compete with AMD’s MI300X GPU, which also offers increased memory capacity for handling big models. | [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2023-11-20 |
| [NVIDIA makes Pandas much faster leveraging GPUs](https://rapids.ai/cudf-pandas/) 🟢 | NVIDIA has significantly enhanced the Pandas library, achieving up to 150 times faster performance by capitalizing on GPUs. With the new cudf.pandas module, operations are seamlessly executed on either the GPU or CPU, providing automatic synchronization and efficient switching between the two. | [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2023-11-13 |
| [New AWS service lets customers rent Nvidia GPUs for quick AI projects](https://techcrunch.com/2023/11/01/new-aws-service-lets-customers-rent-nvidia-gpus-for-quick-ai-projects/) 🟢 | AWS has introduced Amazon Elastic Compute Cloud (EC2) Capacity Blocks for ML, enabling AI practitioners to reserve Nvidia GPUs for specific time periods. This new service grants access to Nvidia H100 Tensor Core GPU instances, allowing users to reserve instances for up to 14 days ahead of time, with shutdowns scheduled automatically. | [Amazon 🌳](Topic_Amazon.md), [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2023-11-13 |
| [New AWS service lets customers rent Nvidia GPUs for quick AI projects](https://techcrunch.com/2023/11/01/new-aws-service-lets-customers-rent-nvidia-gpus-for-quick-ai-projects/) 🟢 | AWS launches Amazon Elastic Compute Cloud (EC2) Capacity Blocks for ML, allowing customers to rent Nvidia GPUs for specific time frames. | [Amazon 🌳](Topic_Amazon.md), [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2023-11-06 |
| [Microsoft to Unveil In-House AI Chip, Reducing Reliance on NVIDIA](https://www.maginative.com/article/microsoft-to-unveil-in-house-ai-chip-reducing-reliance-on-nvidia/) 🔴 | Microsoft is soon launching its own AI chip called Athena, aiming to reduce reliance on NVIDIA GPUs and compete against NVIDIA’s H100 GPU for AI acceleration in data centers. | [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [Microsoft 🪟](Topic_Microsoft.md) | 2023-10-10 |
| [OpenAI is exploring making its own AI chips](https://www.businessinsider.com/openai-is-considering-making-its-own-ai-chips-chatgpt-2023-10) 🟢 | OpenAI is considering developing its own AI chips for ChatGPT due to a global shortage of processors for training AI models. This move could help reduce the high operating costs of ChatGPT, which currently amount to $700,000 per day. OpenAI’s decision may diverge from Microsoft, their partner, who is also working on their own AI chips. | [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [Microsoft 🪟](Topic_Microsoft.md), [ChatGPT 💬](Topic_ChatGPT.md), [OpenAI 🌟](Topic_OpenAI.md) | 2023-10-10 |
| [Introducing Stable LM 3B: Bringing Sustainable, High-Performance Language Models to Smart Devices](https://stability.ai/blog/stable-lm-3b-sustainable-high-performance-language-models-smart-devices) 🟢 | Stability AI introduces Stable LM 3B, a high-performing language model designed for smart devices. With 3 billion parameters, it outperforms state-of-the-art 3B models and reduces operating costs and power consumption. The model enables a wider range of applications on smart devices, PCs, and edge computing. | [Stability AI ⚖️](Topic_Stability_AI.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2023-10-10 |
| [Amazon will invest up to $4 billion in Anthropic](https://www.anthropic.com/index/anthropic-amazon) 🟢 | Amazon has made a significant $4 billion investment in Anthropic. This partnership will enable Anthropic to benefit from Amazon Web Services (AWS), specifically leveraging AWS’s Trainium and Inferentia chips to enhance model training and deployment capabilities. Additionally, Anthropic will use Amazon Bedrock to optimize Claude versions and explore finetuning options. | [Amazon 🌳](Topic_Amazon.md), [Funding 💰](Topic_Funding.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [Claude 🖋️](Topic_Claude.md), [Anthropic 🌐](Topic_Anthropic.md) | 2023-10-02 |
| [LLM Startup Embraces AMD GPUs, Says ROCm Has ‘Parity’ With Nvidia’s CUDA Platform](https://www.crn.com/news/components-peripherals/llm-startup-embraces-amd-gpus-says-rocm-has-parity-with-nvidia-s-cuda-platform) 🟢 | A startup called Lamini is using over 100 AMD Instinct MI200 GPUs and found that AMD’s ROCm software platform rivals Nvidia’s CUDA platform. They claim that running a large language model on their platform is 10x cheaper than on Amazon Web Services. | [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [NVIDIA 🎮](Topic_NVIDIA.md) | 2023-10-02 |
| [NVIDIA and Hugging Face to Connect Millions of Developers to Generative AI Supercomputing](https://nvidianews.nvidia.com/news/nvidia-and-hugging-face-to-connect-millions-of-developers-to-generative-ai-supercomputing) 🟢 | NVIDIA and Hugging Face have partnered to offer AI developers access to high-performance GPUs for deep learning through DGX Cloud integration. | [Hugging Face 🤗](Topic_Hugging_Face.md), [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2023-08-14 |
| [Cerebras Systems signs $100 million AI supercomputer deal with UAE’s G42](https://www.reuters.com/technology/cerebras-systems-signs-100-mln-ai-supercomputer-deal-with-uaes-g42-2023-07-20/) 🟢 | Cerebras Systems has struck a $100 million deal with G42, marking the debut of AI supercomputers that could potentially challenge Nvidia’s market position. In response to chip shortages, cloud computing providers are seeking alternative solutions. To accelerate the rollout, Cerebras will construct three Condor Galaxy systems in the United States, with the first supercomputer set to go online this year, followed by two others in early 2024. | [Funding 💰](Topic_Funding.md), [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2023-07-24 |
| [Nvidia deepens bets on AI in drug discovery with Recursion investment](https://www.reuters.com/technology/nvidia-invests-50-mln-recursion-train-ai-models-drug-discovery-2023-07-12/) 🟢 | Nvidia invests $50M in Recursion, a biotech company using AI to revolutionize drug discovery. This partnership allows Recursion to utilize Nvidia’s platform and access their advanced AI technology. Recursion’s share price surged by 83% post-announcement, highlighting the market’s recognition of AI’s significance in drug discovery. | [AI in healthcare 🏥](Topic_AI_in_healthcare.md), [Funding 💰](Topic_Funding.md), [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2023-07-17 |
| [Miner Pivots 38,000 GPUs From Crypto to AI](https://www.tomshardware.com/news/hive-blockchain-pivoting-to-ai) 🟢 | Cryptomining firm Hive Blockchain is redirecting their efforts from Ethereum mining to AI workloads. With 38,000 GPUs at their disposal, they aim to generate revenue while still utilizing some GPU power for crypto mining. However, transitioning to AI compute poses challenges as older ETH mining GPUs have limited value in this market. | [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2023-07-10 |
| [Inside China’s underground market for high-end Nvidia AI chips](https://www.reuters.com/technology/inside-chinas-underground-market-high-end-nvidia-ai-chips-2023-06-19/) 🔴 | Despite U.S. export restrictions, Chinese demand for high-end Nvidia AI chips remains strong and vendors in Shenzhen and Hong Kong have emerged to offer them at steep prices in a de facto underground market. The exact volume of chip flow is unknown, but local authorities are also reported to be buyers. | [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [AI regulation 📜](Topic_AI_regulation.md) | 2023-06-26 |
| [China’s ByteDance Has Gobbled Up $1 Billion of Nvidia GPUs for AI This Year](https://www.tomshardware.com/news/chinas-bytedance-has-gobbled-up-dollar1-billion-of-nvidia-gpus-for-ai-this-year) 🟢 | Chinese tech giant ByteDance has reportedly invested $1 billion in Nvidia’s HPC products, including the A100 and H800 cards, totaling about 100,000 units, to fulfill the high demand for AI technology. China recognizes the impact of AI on the global market and has embraced it, investing heavily in HPC hardware. | [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2023-06-26 |
| [OpenAI’s plans according to Sam Altman](https://website-754fwhahs-humanloopml.vercel.app/blog/open_ai_talk) 🟢 | OpenAI plans to prioritize creating a cheaper and faster GPT-4, extending context windows, and releasing multimodality in 2024. However, they are currently limited by GPU shortages and are unable to make models millions of times bigger in the near future. The finetuning API will be extended to the latest models. | [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [GPT-4 and GPT-4 turbo 🚀](Topic_GPT-4_and_GPT-4_turbo.md), [OpenAI 🌟](Topic_OpenAI.md) | 2023-06-06 |
| [Why Nvidia is suddenly one of the most valuable companies in the world](https://www.washingtonpost.com/technology/2023/05/25/nvidia-ai-stock-gpu-chatbots/) 🟢 | NVIDIA’s GPUs have become a crucial component in developing AI, leading its worth to $939.3 billion; with AI applications requiring huge amounts of data, companies are buying thousands of NVIDIA’s expensive chips. NVIDIA’s dominance in the industry is predicted to persist as startups compete with big tech for access to its costly GPUs. | [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2023-06-06 |
| [Intel Announces Aurora genAI, Generative AI Model With 1 Trillion Parameters](https://wccftech.com/intel-aurora-genai-chatgpt-competitor-generative-ai-model-with-1-trillion-parameters/) 🟢 | Intel has announced the Aurora genAI model, with 1 trillion parameters, which will be trained on scientific texts and structured scientific data to target cancer research, systems biology, cosmology, polymer chemistry, materials, and climate science. It will be powered by the Aurora supercomputer and has potential to suggest experiments and accelerate drug design targets. | [AI in healthcare 🏥](Topic_AI_in_healthcare.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2023-05-29 |
| [Meta bets big on AI with custom chips and a supercomputer](https://techcrunch.com/2023/05/18/meta-bets-big-on-ai-with-custom-chips-and-a-supercomputer/) 🟢 | Meta lifted the curtains on its efforts to develop in-house infrastructure for AI workloads, including generative AI. | [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [Meta ♾](Topic_Meta.md) | 2023-05-22 |
| [Microsoft Readies AI Chip as Machine Learning Costs Surge](https://www.theinformation.com/articles/microsoft-readies-ai-chip-as-machine-learning-costs-surge) 🟢 | Microsoft creating Athena chip for AI’s large-language models, aiming to save money and time. Amazon, Google & Facebook are creating their own AI chips too. | [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [Microsoft 🪟](Topic_Microsoft.md) | 2023-04-24 |
| [Elon Musk is moving forward with a new generative-AI project at Twitter after purchasing thousands of GPUs](https://www.businessinsider.com/elon-musk-twitter-investment-generative-ai-project-2023-4) 🟢 | Elon Musk is pursuing a Twitter AI project with a large language model despite recently calling to halt AI training. | [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md) | 2023-04-17 |
| [Microsoft spent hundreds of millions of dollars on a ChatGPT supercomputer](https://www.theverge.com/2023/3/13/23637675/microsoft-chatgpt-bing-millions-dollars-supercomputer-openai) 🟢 | Microsoft says it connected tens of thousands of Nvidia A100 chips and reworked server racks to build the hardware behind ChatGPT and its own Bing AI bot. | [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [Microsoft 🪟](Topic_Microsoft.md), [ChatGPT 💬](Topic_ChatGPT.md) | 2023-03-20 |