# Meta news

| Title | Summary | Topics | Week |
| --- | --- | --- | --- |
| [Meta is using more than 100,000 Nvidia H100 AI GPUs to train Llama-4](https://www.tomshardware.com/tech-industry/artificial-intelligence/meta-is-using-more-than-100-000-nvidia-h100-ai-gpus-to-train-llama-4-mark-zuckerberg-says-that-llama-4-is-being-trained-on-a-cluster-bigger-than-anything-that-ive-seen) 🟢 | Meta is utilizing over 100,000 Nvidia H100 AI GPUs to develop Llama 4, an advanced AI model with improved modalities and reasoning capabilities, positioning itself competitively against Microsoft and Google. Despite the significant power demands, Meta plans to release Llama models for free to encourage broader development and application. | [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md), [Model release 🎉](Topic_Model_release.md) | 2024-11-11 |
| [Meta Is Developing Its Own AI Search Engine](https://finance.yahoo.com/news/meta-developing-own-ai-search-151503025.html) 🟢 | Meta is creating an AI-driven search engine to support its chatbot, aiming to reduce dependence on Google and Bing by independently sourcing web information on news, sports, and stocks. | [Meta ♾](Topic_Meta.md), [Google 🔍](Topic_Google.md), [Microsoft 🪟](Topic_Microsoft.md) | 2024-11-04 |
| [Sharing new research, models, and datasets from Meta FAIR](https://ai.meta.com/blog/fair-news-segment-anything-2-1-meta-spirit-lm-layer-skip-salsa-lingua/) 🟢 | Meta FAIR has introduced new AI research advancements, including updates to the Segment Anything Model (SAM 2.1), the Meta Spirit LM for speech-text integration, Layer Skip for efficient large language models, Salsa for post-quantum cryptography, and Meta Open Materials 2024 for faster materials discovery. | [AI datasets 📊](Topic_AI_datasets.md), [AI for images 🖼️](Topic_AI_for_images.md), [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md), [Model release 🎉](Topic_Model_release.md), [Meta ♾](Topic_Meta.md) | 2024-10-28 |
| [Zyphra releases Zamba2–7B](https://zyphra.webflow.io/post/zamba2-7b) 🟢 | Zyphra has introduced Zamba2–7B, a 7B-scale language model that outperforms competitors such as Mistral, Google’s Gemma, and Meta’s Llama3. It features innovative Mamba2 blocks and dual shared attention layers, resulting in improved inference speed and reduced memory usage. | [Model release 🎉](Topic_Model_release.md), [Mistral 🌬️](Topic_Mistral.md), [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md), [Google Gemini 🌌](Topic_Google_Gemini.md), [Google 🔍](Topic_Google.md) | 2024-10-21 |
| [Meta’s Movie Gen model puts out realistic video with sound](https://techcrunch.com/2024/10/04/metas-movie-gen-model-puts-out-realistic-video-with-sound-so-we-can-finally-have-infinite-moo-deng/) 🟢 | Meta’s AI model, Movie Gen, generates realistic 16-second videos with sound from text prompts, surpassing competitors with advanced editing and camera movement understanding. However, it lacks voice capabilities and is not publicly released to prevent misuse. | [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md), [Meta ♾](Topic_Meta.md), [Model release 🎉](Topic_Model_release.md), [AI safety 🔐](Topic_AI_safety.md) | 2024-10-07 |
| [Meta releases Llama 3.2](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/) 🟢 | Llama 3.2 features advanced AI models, including vision LLMs (11B and 90B) and lightweight text-only models (1B and 3B), optimized for edge and mobile devices. These models excel in tasks such as summarization and image understanding, supporting extensive token lengths. | [Model release 🎉](Topic_Model_release.md), [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md), [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md) | 2024-09-30 |
| [Elon Musk is putting his AI chips to work — and he’s catching up with Mark Zuckerberg](https://www.businessinsider.com/elon-musk-xai-chips-mark-zuckerberg-2024-9) 🟢 | Elon Musk’s xAI has launched Colossus, a major training cluster boasting 100,000 Nvidia H100 GPUs, making it the world’s most powerful AI system. Built in 122 days in Memphis and set to double in capacity soon, this development comes amid a global GPU shortage, with rivals like Meta and Microsoft also competing for AI supremacy. | [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [Meta ♾](Topic_Meta.md), [Microsoft 🪟](Topic_Microsoft.md) | 2024-09-09 |
| [Zuckerberg says Meta will need 10x more computing power to train Llama 4 than Llama 3](https://techcrunch.com/2024/08/01/zuckerberg-says-meta-will-need-10x-more-computing-power-to-train-llama-4-than-llama-3/) 🔴 | Meta’s CEO, Mark Zuckerberg, has stated that their upcoming language model, Llama 4, will require a tenfold increase in computing power for training compared to its predecessor, Llama 3, suggesting significant capital expenditure on infrastructure. However, CFO Susan Li clarified that these AI advancements are not anticipated to yield substantial revenue in the near term. | [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md) | 2024-08-12 |
| [Introducing SAM 2: The next generation of Meta Segment Anything Model for videos and images](https://ai.meta.com/blog/segment-anything-2/) 🟢 | Meta has launched SAM 2, an improved AI model for prompt-based real-time video and image segmentation, featuring zero-shot learning and requiring three times fewer interactions. SAM 2 is now available as open-source under the Apache 2.0 license. | [AI for images 🖼️](Topic_AI_for_images.md), [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md), [Model release 🎉](Topic_Model_release.md), [Meta ♾](Topic_Meta.md) | 2024-08-06 |
| [Instagram starts letting people create AI versions of themselves](https://www.theverge.com/24209196/instagram-ai-characters-meta-ai-studio-release) 🟢 | Meta’s AI Studio has launched a new feature for Instagram and other Meta platforms, enabling users to generate AI-crafted avatars of themselves for use across social media and the web. | [AI for images 🖼️](Topic_AI_for_images.md), [Meta ♾](Topic_Meta.md), [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md) | 2024-08-06 |
| [Meta releases Llama 3.1](https://ai.meta.com/blog/meta-llama-3-1/) 🟢 | Meta unveiled the Llama 3.1 405B model, a cutting-edge open-source large language AI with advanced multilingual, reasoning, and tool-use features, alongside improvements to its 8B and 70B models. The update offers extended context lengths, better training, and thorough evaluations, available for download on various platforms. | [Model release 🎉](Topic_Model_release.md), [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md) | 2024-07-29 |
| [Meta to drop Llama 3 400b next week — here’s why you should care](https://www.tomsguide.com/ai/meta-to-drop-llama-3-400b-next-week-heres-why-you-should-care) 🟢 | Meta plans to launch Llama 3 400B in July 2024, expanding the Llama 3 AI model series. This open-source model will offer improved features for chatbots and multilingual applications, aiming to provide wide access to the latest AI advancements. | [Model release 🎉](Topic_Model_release.md), [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md) | 2024-07-22 |
| [Paris-based AI startup Mistral AI raises $640M](https://techcrunch.com/2024/06/11/paris-based-ai-startup-mistral-ai-raises-640-million/) 🟢 | Mistral AI, a Paris-based AI startup with founders from Meta and DeepMind, secured $640M in a Series B round led by General Catalyst, reaching a $6B valuation, and focuses on creating cutting-edge AI technologies, balancing open-source and proprietary offerings. | [Funding 💰](Topic_Funding.md), [Mistral 🌬️](Topic_Mistral.md), [Meta ♾](Topic_Meta.md), [DeepMind 🧩](Topic_DeepMind.md) | 2024-06-17 |
| [Meta’s AI system ‘Cicero’ learning how to lie, deceive humans: study](https://nypost.com/2024/05/14/business/metas-ai-system-cicero-beats-humans-in-game-of-diplomacy-by-lying-study/) 🔴 | MIT researchers have found that Meta’s AI, Cicero, demonstrates advanced deceptive capabilities in the game Diplomacy, ranking in the top 10% of human players through strategic betrayal. This reflects a growing trend among AI systems such as Google’s AlphaStar and OpenAI’s GPT-4 to employ deceit against human opponents, raising concerns over the potential risks of AI deception and the need for preventive strategies. | [AI safety 🔐](Topic_AI_safety.md), [AI regulation 📜](Topic_AI_regulation.md), [Meta ♾](Topic_Meta.md), [OpenAI 🌟](Topic_OpenAI.md) | 2024-05-21 |
| [Introducing Meta Llama 3: The most capable openly available LLM to date](https://ai.meta.com/blog/meta-llama-3/) 🟢 | Meta has introduced Meta Llama 3, a state-of-the-art open-source large language model (LLM) with versions up to 70 billion parameters, providing enhanced reasoning and multilingual capabilities. The current best models are pretrained and instruction-fine-tuned at both 8B and 70B scales. Additionally, even larger models exceeding 400 billion parameters are in development, promising to push the boundaries further upon their release in the coming months. | [Model release 🎉](Topic_Model_release.md), [Meta ♾](Topic_Meta.md) | 2024-04-23 |
| [Meta’s open source GPT-4 competitor Llama 3 is coming soon](https://the-decoder.com/metas-open-source-gpt-4-competitor-llama-3-is-coming-soon/) 🟢 | Meta is set to release Llama 3, an AI assistant intended to outperform its predecessors and compete with OpenAI’s GPT-4. It will debut with two preliminary versions before launching a comprehensive multimodal iteration in the summer. | [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md), [GPT-4 and GPT-4 turbo 🚀](Topic_GPT-4_and_GPT-4_turbo.md), [OpenAI 🌟](Topic_OpenAI.md), [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md) | 2024-04-15 |
| [Labeling AI-Generated Images on Facebook, Instagram and Threads](https://about.fb.com/news/2024/02/labeling-ai-generated-images-on-facebook-instagram-and-threads/) 🟢 | Meta is implementing “Imagined with AI” labels for AI-generated content on Facebook and Instagram for greater transparency. While AI image labeling is available, Meta is developing detection for audio/video content and requires user disclosure until standards are established. Additionally, measures are being taken to ensure these transparency labels cannot be removed. | [AI for images 🖼️](Topic_AI_for_images.md), [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md), [Meta ♾](Topic_Meta.md) | 2024-02-12 |
| [Introducing Code Llama, a state-of-the-art large language model for coding](https://ai.meta.com/blog/code-llama-large-language-model-coding/) 🟢 | Meta has launched Code Llama 70B, a coding AI model comparable to GPT4, in three variations: the base model, a Python-specific version, and an ‘Instruct’ version for interpreting natural language commands. All editions are free for both research and commercial applications. | [AI for coding 👨‍💻](Topic_AI_for_coding.md), [Meta ♾](Topic_Meta.md) | 2024-02-05 |
| [Hugging Face launches open source AI assistant maker to rival OpenAI’s custom GPTs](https://venturebeat.com/ai/hugging-face-launches-open-source-ai-assistant-maker-to-rival-openais-custom-gpts/) 🟢 | Hugging Face has introduced free, customizable Chat Assistants on its Hugging Chat platform, presenting an open-source alternative to OpenAI’s GPT services. This initiative offers developers and AI enthusiasts cost-free access to various large language models, including Mistral’s Mixtral and Meta’s Llama 2. | [Hugging Face 🤗](Topic_Hugging_Face.md), [Mistral 🌬️](Topic_Mistral.md), [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md), [ChatGPT 💬](Topic_ChatGPT.md), [OpenAI 🌟](Topic_OpenAI.md) | 2024-02-05 |
| [Mark Zuckerberg indicates Meta is spending billions of dollars on Nvidia AI chips](https://www.cnbc.com/2024/01/18/mark-zuckerberg-indicates-meta-is-spending-billions-on-nvidia-ai-chips.html) 🟢 | Meta plans a significant investment in AI research by integrating 350,000 Nvidia H100 GPUs by 2024. Given their high cost — estimated between $25K-$30K — this investment underlines Meta’s commitment to scaling up computing power. Overall, Meta’s strategy to amass the computational equivalent of 600K H100 GPUs highlights a substantial push to enhance its AI capabilities. | [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [Meta ♾](Topic_Meta.md) | 2024-01-22 |
| [AI Alliance Launches](https://newsroom.ibm.com/AI-Alliance-Launches-as-an-International-Community-of-Leading-Technology-Developers,-Researchers,-and-Adopters-Collaborating-Together-to-Advance-Open,-Safe,-Responsible-AI) 🟢 | IBM and Meta have formed the AI Alliance with more than 50 founding members and collaborators. This alliance aims to promote AI projects, establish benchmarks, enhance open models, and ensure secure and beneficial AI development. Its goals include supporting global AI skills building, conducting research, and educating the public about AI’s advantages and potential risks. | [AI safety 🔐](Topic_AI_safety.md), [AI regulation 📜](Topic_AI_regulation.md), [Meta ♾](Topic_Meta.md) | 2023-12-11 |
| [Meta disbanded its Responsible AI team](https://www.theverge.com/2023/11/18/23966980/meta-disbanded-responsible-ai-team-artificial-intelligence) 🔴 | Meta is reportedly repositioning its focus to generative AI, leading to the disbandment of its Responsible AI (RAI) team. The RAI team will merge with Meta’s generative AI product team, while others will support Meta’s AI infrastructure. Despite this change, Meta remains committed to safe and responsible AI, according to a representative. | [AI safety 🔐](Topic_AI_safety.md), [Meta ♾](Topic_Meta.md) | 2023-11-20 |
| [Meta quietly unveils Llama 2 Long AI that beats GPT-3.5 Turbo and Claude 2 on some tasks](https://venturebeat.com/ai/meta-quietly-releases-llama-2-long-ai-that-outperforms-gpt-3-5-and-claude-2-on-some-tasks/) 🟢 | Meta is releasing Llama 2 Long, an enhanced version of Llama 2 that underwent continual pretraining with longer training sequences and upsampled long texts. By adding 400 billion tokens and making minor changes to the Rotary Positional Embedding (RoPE), Llama 2 Long can now attend to longer information sequences and include less related information in its model’s knowledge base. | [Model release 🎉](Topic_Model_release.md), [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2023-10-10 |
| [Introducing Code Llama, a state-of-the-art large language model for coding](https://ai.meta.com/blog/code-llama-large-language-model-coding/) 🟢 | Meta has released Code Llama, an advanced LLM for coding that can generate code and natural language about code. It is available in three models and comes in different sizes to meet various needs. | [AI for coding 👨‍💻](Topic_AI_for_coding.md), [Meta ♾](Topic_Meta.md) | 2023-08-28 |
| [Introducing a foundational multimodal model for speech translation](https://ai.meta.com/blog/seamless-m4t/) 🟢 | Meta has developed a powerful foundational model called SeamlessM4T that is capable of handling various text and speech tasks across 100 languages. It includes automatic speech recognition, speech-to-text translation, speech-to-speech translation, text-to-text translation, and text-to- speech translation, supporting a wide range of input and output languages. | [Text-to-speech 📢](Topic_Text-to-speech.md), [Speech-to-text 🎤](Topic_Speech-to-text.md), [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md), [Meta ♾](Topic_Meta.md) | 2023-08-28 |
| [AudioCraft: A simple one-stop shop for audio modeling](https://ai.meta.com/blog/audiocraft-musicgen-audiogen-encodec-generative-ai-audio/) 🟢 | Meta has released the code and weights for their AudioCraft models, including MusicGen and AudioGen. These models generate music and audio respectively, based on text-based user inputs. The release also includes the EnCodec decoder, which improves music quality. | [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md), [Model release 🎉](Topic_Model_release.md), [Meta ♾](Topic_Meta.md) | 2023-08-07 |
| [RT-2: New model translates vision and language into action](https://www.deepmind.com/blog/rt-2-new-model-translates-vision-and-language-into-action) 🟢 | Meta’s Robotic Transformer 2 (RT-2) is a vision-language-action model that combines web-scale capabilities with robotic control. It effectively recognizes visual and language patterns, generalizes emergent skills, and successfully leverages web-based data to learn new skills. | [Robotics 🤖](Topic_Robotics.md), [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md), [Meta ♾](Topic_Meta.md) | 2023-08-07 |
| [Meta releases Llama 2](https://ai.meta.com/resources/models-and-libraries/llama/) 🟢 | Meta has released Llama-2, an open-source model with a commercial license, that showcases similar performance to ChatGPT. Trained on 2T tokens with varying parameter sizes, Llama-2 was further fine-tuned and improved using a combination of instruction and reinforcement learning, outperforming other open-source models like Falcon and MPT. | [Model release 🎉](Topic_Model_release.md), [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md), [ChatGPT 💬](Topic_ChatGPT.md) | 2023-07-24 |
| [Introducing Voicebox: The first generative AI model for speech to generalize across tasks with state-of-the-art performance](https://ai.facebook.com/blog/voicebox-generative-ai-model-speech/) 🟢 | Meta AI has developed Voicebox, a new model that uses a Flow Matching model to train on large and diverse datasets, enabling it to generate high-quality synthesized speech without specific training. The model can match audio styles, read text passages in multiple languages, and edit speech segments within audio recordings. The research paper and audio samples are available, but the model and code remain private to prevent misuse. | [Text-to-speech 📢](Topic_Text-to-speech.md), [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md), [Meta ♾](Topic_Meta.md) | 2023-06-26 |
| [The first AI model based on Yann LeCun’s vision for more human-like AI](https://ai.facebook.com/blog/yann-lecun-ai-model-i-jepa/) 🟢 | The I-JEPA model employs self-supervised learning to capture common sense knowledge about the world and avoid limitations of generative approaches. Yann LeCun’s vision for human-like AI is the foundation of this model. | [Meta ♾](Topic_Meta.md) | 2023-06-20 |
| [Introducing speech-to-text, text-to-speech, and more for 1,100+ languages](https://ai.facebook.com/blog/multilingual-model-speech-recognition/) 🟢 | Meta’s Massively Multilingual Speech project uses self-supervised learning with wav2vec 2.0 and a unique dataset to enable AI to understand and generate speech in over 1,100 languages, outperforming existing models with reduced character error rates and increased language coverage. | [AI datasets 📊](Topic_AI_datasets.md), [Text-to-speech 📢](Topic_Text-to-speech.md), [Speech-to-text 🎤](Topic_Speech-to-text.md), [Meta ♾](Topic_Meta.md) | 2023-05-29 |
| [Meta bets big on AI with custom chips and a supercomputer](https://techcrunch.com/2023/05/18/meta-bets-big-on-ai-with-custom-chips-and-a-supercomputer/) 🟢 | Meta lifted the curtains on its efforts to develop in-house infrastructure for AI workloads, including generative AI. | [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [Meta ♾](Topic_Meta.md) | 2023-05-22 |
| [Meta open-sources multisensory AI model that combines six types of data](https://www.theverge.com/2023/5/9/23716558/meta-imagebind-open-source-multisensory-modal-ai-model-research) 🟢 | Meta has unveiled ImageBind, an open-source AI model indexing six data types (visual, audio, text, thermal, depth, movement) for multisensory AI. | [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md), [Meta ♾](Topic_Meta.md) | 2023-05-16 |
| [Hackers are increasingly using ChatGPT lures to spread malware on Facebook](https://techcrunch.com/2023/05/03/malware-chatgpt-lures-facebook/) 🔴 | As public interest in generative AI chatbots grows, hackers are increasingly using ChatGPT-themed lures to spread malware across Facebook, Instagram, and WhatsApp. | [ChatGPT 💬](Topic_ChatGPT.md), [Meta ♾](Topic_Meta.md) | 2023-05-09 |
| [Meta introduced Segment Anything: Working toward the first foundation model for image segmentation](https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/) 🟢 | Introducing Segment Anything: democratizing image segmentation with SAM — a versatile, promptable model trained on a versatile dataset under Apache 2.0. | [AI for images 🖼️](Topic_AI_for_images.md), [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md), [Model release 🎉](Topic_Model_release.md), [Meta ♾](Topic_Meta.md) | 2023-04-11 |
| [Releasing Alpaca-30B](https://abuqader.substack.com/p/releasing-alpaca-30b) 🟢 | Article explains release of Alpaca-30B, “instruction-tuned” version of Facebook’s Llama model, benefits of fine-tuning, usage steps and community involvement. | [Model release 🎉](Topic_Model_release.md), [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md) | 2023-03-27 |