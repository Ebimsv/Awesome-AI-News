# GPT-3, GPT-3.5, and GPT-3.5 turbo news

| Title | Summary | Topics | Week |
| --- | --- | --- | --- |
| [Introducing DBRX: A New State-of-the-Art Open LLM](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm) 🟢 | Databricks has unveiled DBRX, a new open-source large language model (LLM) that surpasses GPT-3.5 in programming and general tasks, and is competitive with Gemini 1.0 Pro. DBRX features a mixture-of-experts architecture with a massive 132 billion parameters, though only 36 billion are active for any given input. Both DBRX Base and DBRX Instruct models are openly accessible on Hugging Face. | [Hugging Face 🤗](topics/Topic_Hugging_Face_🤗.md), [Mixture-of-Experts 🤝](topics/Topic_Mixture-of-Experts_🤝.md), [AI for coding 👨‍💻](topics/Topic_AI_for_coding_👨‍💻.md), [Model release 🎉](topics/Topic_Model_release_🎉.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md) | 2024-04-02 |
| [OpenAI releases new embedding models and API updates](https://openai.com/blog/new-embedding-models-and-api-updates) 🟢 | OpenAI has announced updates to their AI model suite, including the launch of more efficient embedding models and cost-reduced versions of GPT-3.5 Turbo and a new GPT-4 Turbo model. The “text-embedding-3-large” leads with a 64.6% MTEB score at $0.00013 per 1k tokens, while the “text-embedding-3-small” offers improved performance over its predecessor at a fivefold cost reduction. Additionally, the “gpt-3.5-turbo-0125” is now 50% cheaper, priced at $0.0005 per 1k tokens, and a new “gpt-4–0125-preview” model has been introduced. | [Model release 🎉](topics/Topic_Model_release_🎉.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md), [GPT-4 and GPT-4 turbo 🚀](topics/Topic_GPT-4_and_GPT-4_turbo_🚀.md), [OpenAI 🌟](topics/Topic_OpenAI_🌟.md) | 2024-01-29 |
| [Introducing gigaGPT: GPT-3 sized models in 565 lines of code](https://www.cerebras.net/blog/introducing-gigagpt-gpt-3-sized-models-in-565-lines-of-code) 🟢 | Cerebras has released gigaGPT, a model implementation similar to nanoGPT but with over 100 billion parameters. By leveraging Cerebras hardware and different optimizers, gigaGPT overcomes the limitations of GPU memory and the need for complex scaling frameworks, offering a simplified approach for training large models. | [Model release 🎉](topics/Topic_Model_release_🎉.md), [AI Chips and GPUs 🖥️](topics/Topic_AI_Chips_and_GPUs_🖥️.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md) | 2023-12-19 |
| [Mistral AI has released Mixtral 8x7B, an open-source sparse mixture-of-experts model](https://mistral.ai/news/mixtral-of-experts/) 🟢 | Mistral AI has introduced Mixtral 8x7B, an open-source, sparse mixture-of-experts model with 45B parameters. It outperforms LLaMA 2 70B and GPT-3.5 in NLP benchmarks, while also providing 6x faster inference speed. The model selectively uses only 12B parameters per inference, keeping operational costs equivalent to a 12B model. The Mixtral model is accessible through Mistral’s beta API and dev platform. | [Mixture-of-Experts 🤝](topics/Topic_Mixture-of-Experts_🤝.md), [Model release 🎉](topics/Topic_Model_release_🎉.md), [Mistral 🌬️](topics/Topic_Mistral_🌬️.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md) | 2023-12-11 |
| [Announcements from OpenAI DevDay](https://openai.com/blog/new-models-and-developer-products-announced-at-devday) 🟢 | OpenAI has introduced several new and improved models and APIs, including GPT-4 Turbo with a larger context window and lower prices, the ability to process images in the Chat Completions API, fine- tuning options for GPT-4 and GPT-3.5 Turbo, and the availability of DALL·E 3 via API. They have also introduced features like JSON mode, improved instruction following, and parallel function calling. Additionally, there are new options for text-to-speech and the creation of “GPT assistants.” OpenAI has also released the Whisper large-v3 model for automatic speech recognition. | [Whisper 🤫](topics/Topic_Whisper_🤫.md), [Text-to-speech 📢](topics/Topic_Text-to-speech_📢.md), [Speech-to-text 🎤](topics/Topic_Speech-to-text_🎤.md), [AI for images 🖼️](topics/Topic_AI_for_images_🖼️.md), [Multimodal AI (image, video, audio) 📸](topics/Topic_Multimodal_AI_(image_video_audio)_📸.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md), [GPT-4 and GPT-4 turbo 🚀](topics/Topic_GPT-4_and_GPT-4_turbo_🚀.md), [OpenAI 🌟](topics/Topic_OpenAI_🌟.md) | 2023-11-13 |
| [Meta quietly unveils Llama 2 Long AI that beats GPT-3.5 Turbo and Claude 2 on some tasks](https://venturebeat.com/ai/meta-quietly-releases-llama-2-long-ai-that-outperforms-gpt-3-5-and-claude-2-on-some-tasks/) 🟢 | Meta is releasing Llama 2 Long, an enhanced version of Llama 2 that underwent continual pretraining with longer training sequences and upsampled long texts. By adding 400 billion tokens and making minor changes to the Rotary Positional Embedding (RoPE), Llama 2 Long can now attend to longer information sequences and include less related information in its model’s knowledge base. | [Model release 🎉](topics/Topic_Model_release_🎉.md), [LLaMA 🦙](topics/Topic_LLaMA_🦙.md), [Meta ♾](topics/Topic_Meta_♾.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md) | 2023-10-10 |
| [Spread Your Wings: Falcon 180B is here](https://huggingface.co/blog/falcon-180b) 🟢 | TII has just released Falcon 180B, a powerful language model with 180 billion parameters trained on 3.5 trillion tokens. Outperforming Llama 2 70B and GPT-3.5 on MMLU, Falcon 180B performs great and ranks high on the Hugging Face Leaderboard. This model is available for commercial use but has strict terms excluding “hosting use.” | [Hugging Face 🤗](topics/Topic_Hugging_Face_🤗.md), [Model release 🎉](topics/Topic_Model_release_🎉.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md) | 2023-09-11 |
| [GPT-3.5 Turbo fine-tuning released](https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates) 🟢 | OpenAI has released fine-tuning for GPT-3.5 Turbo, offering improved performance on specific tasks. The fine-tuned version can even match or surpass the capabilities of base GPT-4. Early testers have significantly reduced prompt size through fine-tuning. The costs for training and usage input/output are provided at $0.008, $0.012, and $0.016 per 1K tokens, respectively. | [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md), [GPT-4 and GPT-4 turbo 🚀](topics/Topic_GPT-4_and_GPT-4_turbo_🚀.md), [OpenAI 🌟](topics/Topic_OpenAI_🌟.md) | 2023-08-28 |
| [Meet FreeWilly, Our Large And Mighty Instruction Fine-Tuned Models](https://stability.ai/blog/freewilly-large-instruction-fine-tuned-models) 🟢 | Stability AI and CarperAI Lab have collaborated to release FreeWilly, a LLaMA 2 model fine-tuned using Supervised Fine-Tune (SFT) techniques. FreeWilly2 performs comparably to GPT-3.5 in certain tasks, and its capabilities have been verified by both Stability AI researchers and Hugging Face. Both models are publicly available under a non-commercial license. | [Model release 🎉](topics/Topic_Model_release_🎉.md), [Stability AI ⚖️](topics/Topic_Stability_AI_⚖️.md), [LLaMA 🦙](topics/Topic_LLaMA_🦙.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md) | 2023-07-31 |
| [GPT-4 API general availability and deprecation of older models in the Completions API](https://openai.com/blog/gpt-4-api-general-availability) 🟢 | OpenAI has announced the general availability of the GPT-4 API and the automatic upgrade of GPT-3 models to new models from January 4, 2024. Developers using text-davinci-003 are advised to upgrade to gpt-3.5-turbo-instruct and specify it as the “model” in API requests for a smooth transition. OpenAI also offers priority access to GPT-3.5 Turbo and GPT-4 fine-tuning for users with fine-tuned older models, understanding the challenges of transitioning from these models. | [Model release 🎉](topics/Topic_Model_release_🎉.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md), [GPT-4 and GPT-4 turbo 🚀](topics/Topic_GPT-4_and_GPT-4_turbo_🚀.md), [OpenAI 🌟](topics/Topic_OpenAI_🌟.md) | 2023-07-10 |
| [Function calling and other API updates](https://openai.com/blog/function-calling-and-other-api-updates) 🟢 | OpenAI has released several new updates, including a 16k context version of GPT-3.5-turbo, a new API for calling user-defined functions, cost reductions for models and input tokens, and updated versions of GPT-4 and GPT-3.5-turbo. The updates aim to improve GPT’s capabilities and make it easier to connect with external tools and APIs. | [Model release 🎉](topics/Topic_Model_release_🎉.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md), [GPT-4 and GPT-4 turbo 🚀](topics/Topic_GPT-4_and_GPT-4_turbo_🚀.md), [OpenAI 🌟](topics/Topic_OpenAI_🌟.md) | 2023-06-20 |
| [Introducing ChatGPT and Whisper APIs](https://openai.com/blog/introducing-chatgpt-and-whisper-apis) 🟢 | The ChatGPT API is x10 cheaper than GPT3 API. | [ChatGPT 💬](topics/Topic_ChatGPT_💬.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md), [OpenAI 🌟](topics/Topic_OpenAI_🌟.md) | 2023-03-06 |
| [OpenAI Foundry will let customers buy dedicated compute to run GPT3 and their other models](https://techcrunch.com/2023/02/21/openai-foundry-will-let-customers-buy-dedicated-capacity-to-run-its-ai-models/) 🟢 | It will soon be possible to have dedicated GPT3 models. | [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md), [OpenAI 🌟](topics/Topic_OpenAI_🌟.md) | 2023-02-27 |
| [A collection of 600+ apps powered by GPT3](https://gpt3demo.com/map) 🟢 | and organized by category. | [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md), [OpenAI 🌟](topics/Topic_OpenAI_🌟.md) | 2023-02-20 |
| [Chatbots with GPT3](https://medium.com/nlplanet/building-chatbots-with-gpt3-62f6567d8fa4) 🟢 | Recent advancements in LLMs, such as GPT-3, can be used for chatbot development. Instead of having many very specific intents, each intent can be broader and leverage a Knowledge Base document. | [ChatGPT 💬](topics/Topic_ChatGPT_💬.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md), [OpenAI 🌟](topics/Topic_OpenAI_🌟.md) | 2023-02-06 |
| [Querying NBA stats with GPT-3 + Statmuse + Langchain](https://www.geoffreylitt.com/2023/01/29/fun-with-compositional-llms-querying-basketball-stats-with-gpt-3-statmuse-langchain.html) 🟢 | Using Langchain, the author composed an AI program that combines GPT-3 with Statmuse, a sports stats search engine, to answer multi-part questions about NBA stats. | [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md) | 2023-02-06 |
| [GPT-3 integrated in Microsoft Teams Premium](https://www.microsoft.com/en-us/microsoft-365/blog/2023/02/01/microsoft-teams-premium-cut-costs-and-add-ai-powered-productivity/) 🟢 | with AI-generated notes and tasks. | [Microsoft 🪟](topics/Topic_Microsoft_🪟.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md) | 2023-02-06 |
| [Transformer models: an introduction and catalog — 2023 Edition](https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/) 🟢 | A catalog of popular transformer models from the last years, including ChatGPT, GPT3.5, and other models from Eleuther, Anthropic, Deepmind, and Stability. | [Anthropic 🌐](topics/Topic_Anthropic_🌐.md), [DeepMind 🧩](topics/Topic_DeepMind_🧩.md), [ChatGPT 💬](topics/Topic_ChatGPT_💬.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md), [OpenAI 🌟](topics/Topic_OpenAI_🌟.md) | 2023-01-23 |