# LLaMA news

| Title | Summary | Topics | Week |
| --- | --- | --- | --- |
| [Meta’s open source GPT-4 competitor Llama 3 is coming soon](https://the-decoder.com/metas-open-source-gpt-4-competitor-llama-3-is-coming-soon/) 🟢 | Meta is set to release Llama 3, an AI assistant intended to outperform its predecessors and compete with OpenAI’s GPT-4. It will debut with two preliminary versions before launching a comprehensive multimodal iteration in the summer. | [LLaMA 🦙](topics/Topic_LLaMA_🦙.md), [Meta ♾](topics/Topic_Meta_♾.md), [GPT-4 and GPT-4 turbo 🚀](topics/Topic_GPT-4_and_GPT-4_turbo_🚀.md), [OpenAI 🌟](topics/Topic_OpenAI_🌟.md), [Multimodal AI (image, video, audio) 📸](topics/Topic_Multimodal_AI_(image_video_audio)_📸.md) | 2024-04-15 |
| [Hugging Face launches open source AI assistant maker to rival OpenAI’s custom GPTs](https://venturebeat.com/ai/hugging-face-launches-open-source-ai-assistant-maker-to-rival-openais-custom-gpts/) 🟢 | Hugging Face has introduced free, customizable Chat Assistants on its Hugging Chat platform, presenting an open-source alternative to OpenAI’s GPT services. This initiative offers developers and AI enthusiasts cost-free access to various large language models, including Mistral’s Mixtral and Meta’s Llama 2. | [Hugging Face 🤗](topics/Topic_Hugging_Face_🤗.md), [Mistral 🌬️](topics/Topic_Mistral_🌬️.md), [LLaMA 🦙](topics/Topic_LLaMA_🦙.md), [Meta ♾](topics/Topic_Meta_♾.md), [ChatGPT 💬](topics/Topic_ChatGPT_💬.md), [OpenAI 🌟](topics/Topic_OpenAI_🌟.md) | 2024-02-05 |
| [Meta quietly unveils Llama 2 Long AI that beats GPT-3.5 Turbo and Claude 2 on some tasks](https://venturebeat.com/ai/meta-quietly-releases-llama-2-long-ai-that-outperforms-gpt-3-5-and-claude-2-on-some-tasks/) 🟢 | Meta is releasing Llama 2 Long, an enhanced version of Llama 2 that underwent continual pretraining with longer training sequences and upsampled long texts. By adding 400 billion tokens and making minor changes to the Rotary Positional Embedding (RoPE), Llama 2 Long can now attend to longer information sequences and include less related information in its model’s knowledge base. | [Model release 🎉](topics/Topic_Model_release_🎉.md), [LLaMA 🦙](topics/Topic_LLaMA_🦙.md), [Meta ♾](topics/Topic_Meta_♾.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md) | 2023-10-10 |
| [Releasing Persimmon-8B](https://www.adept.ai/blog/persimmon-8b) 🟢 | Adept.ai introduces Persimmon-8B, an open-source LLM with impressive performance and a compact size. Trained on less data, it achieves comparable results to LLaMA2 and offers fast C++ implementation combined with flexible Python inference. | [Model release 🎉](topics/Topic_Model_release_🎉.md), [LLaMA 🦙](topics/Topic_LLaMA_🦙.md) | 2023-09-11 |
| [Meet FreeWilly, Our Large And Mighty Instruction Fine-Tuned Models](https://stability.ai/blog/freewilly-large-instruction-fine-tuned-models) 🟢 | Stability AI and CarperAI Lab have collaborated to release FreeWilly, a LLaMA 2 model fine-tuned using Supervised Fine-Tune (SFT) techniques. FreeWilly2 performs comparably to GPT-3.5 in certain tasks, and its capabilities have been verified by both Stability AI researchers and Hugging Face. Both models are publicly available under a non-commercial license. | [Model release 🎉](topics/Topic_Model_release_🎉.md), [Stability AI ⚖️](topics/Topic_Stability_AI_⚖️.md), [LLaMA 🦙](topics/Topic_LLaMA_🦙.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_💡.md) | 2023-07-31 |
| [Meta releases Llama 2](https://ai.meta.com/resources/models-and-libraries/llama/) 🟢 | Meta has released Llama-2, an open-source model with a commercial license, that showcases similar performance to ChatGPT. Trained on 2T tokens with varying parameter sizes, Llama-2 was further fine-tuned and improved using a combination of instruction and reinforcement learning, outperforming other open-source models like Falcon and MPT. | [Model release 🎉](topics/Topic_Model_release_🎉.md), [LLaMA 🦙](topics/Topic_LLaMA_🦙.md), [Meta ♾](topics/Topic_Meta_♾.md), [ChatGPT 💬](topics/Topic_ChatGPT_💬.md) | 2023-07-24 |
| [Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs, by MosaicML](https://www.mosaicml.com/blog/mpt-7b) 🟢 | MPT-7B is a transformer trained from scratch on 1T tokens of text and code. It is open-source, available for commercial use, and matches the quality of LLaMA-7B. | [Model release 🎉](topics/Topic_Model_release_🎉.md), [LLaMA 🦙](topics/Topic_LLaMA_🦙.md) | 2023-05-09 |
| [Releasing 3B and 7B RedPajama-INCITE family of models including base, instruction-tuned & chat models](https://www.together.xyz/blog/redpajama-models-v1) 🟢 | The release includes the first models trained on the RedPajama base dataset: a 3 billion and a 7B parameter base model that aims to replicate the LLaMA recipe as closely as possible. | [Model release 🎉](topics/Topic_Model_release_🎉.md), [LLaMA 🦙](topics/Topic_LLaMA_🦙.md) | 2023-05-09 |
| [RedPajama, a project to create leading open-source models, starts by reproducing LLaMA training dataset of over 1.2 trillion tokens](https://www.together.xyz/blog/redpajama) 🟢 | RedPajama aims to reproduce LLaMA models with a 7B parameter model, using a filtered dataset of 1.2T tokens. Their goal is open-source reproducibility. | [AI datasets 📊](topics/Topic_AI_datasets_📊.md), [LLaMA 🦙](topics/Topic_LLaMA_🦙.md) | 2023-04-24 |
| [Releasing Alpaca-30B](https://abuqader.substack.com/p/releasing-alpaca-30b) 🟢 | Article explains release of Alpaca-30B, “instruction-tuned” version of Facebook’s Llama model, benefits of fine-tuning, usage steps and community involvement. | [Model release 🎉](topics/Topic_Model_release_🎉.md), [LLaMA 🦙](topics/Topic_LLaMA_🦙.md), [Meta ♾](topics/Topic_Meta_♾.md) | 2023-03-27 |