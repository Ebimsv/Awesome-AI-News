# LLaMA news

| Title | Summary | Topics | Week |
| --- | --- | --- | --- |
| [Meta is using more than 100,000 Nvidia H100 AI GPUs to train Llama-4](https://www.tomshardware.com/tech-industry/artificial-intelligence/meta-is-using-more-than-100-000-nvidia-h100-ai-gpus-to-train-llama-4-mark-zuckerberg-says-that-llama-4-is-being-trained-on-a-cluster-bigger-than-anything-that-ive-seen) 🟢 | Meta is utilizing over 100,000 Nvidia H100 AI GPUs to develop Llama 4, an advanced AI model with improved modalities and reasoning capabilities, positioning itself competitively against Microsoft and Google. Despite the significant power demands, Meta plans to release Llama models for free to encourage broader development and application. | [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md), [Model release 🎉](Topic_Model_release.md) | 2024-11-11 |
| [Nvidia just dropped a new AI model on the level of OpenAI’s GPT-4](https://venturebeat.com/ai/nvidia-just-dropped-a-new-ai-model-that-crushes-openais-gpt-4-no-big-launch-just-big-results/) 🟢 | Nvidia has introduced the Llama-3.1-Nemotron-70B-Instruct AI model, outperforming GPT-4 in benchmarks. Built on Meta’s Llama 3.1, it features advanced training for enhanced language capabilities. This marks Nvidia’s strategic pivot from GPU manufacturing to AI software, potentially reshaping the industry with powerful, customizable AI solutions and challenging current AI leaders. | [Model release 🎉](Topic_Model_release.md), [NVIDIA 🎮](Topic_NVIDIA.md), [LLaMA 🦙](Topic_LLaMA.md), [OpenAI 🌟](Topic_OpenAI.md) | 2024-10-21 |
| [Zyphra releases Zamba2–7B](https://zyphra.webflow.io/post/zamba2-7b) 🟢 | Zyphra has introduced Zamba2–7B, a 7B-scale language model that outperforms competitors such as Mistral, Google’s Gemma, and Meta’s Llama3. It features innovative Mamba2 blocks and dual shared attention layers, resulting in improved inference speed and reduced memory usage. | [Model release 🎉](Topic_Model_release.md), [Mistral 🌬️](Topic_Mistral.md), [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md), [Google Gemini 🌌](Topic_Google_Gemini.md), [Google 🔍](Topic_Google.md) | 2024-10-21 |
| [Meta releases Llama 3.2](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/) 🟢 | Llama 3.2 features advanced AI models, including vision LLMs (11B and 90B) and lightweight text-only models (1B and 3B), optimized for edge and mobile devices. These models excel in tasks such as summarization and image understanding, supporting extensive token lengths. | [Model release 🎉](Topic_Model_release.md), [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md), [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md) | 2024-09-30 |
| [Introducing Cerebras Inference: AI at Instant Speed](https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed) 🟢 | Cerebras has achieved a significant speed advantage in AI language model inference, delivering 1,800 tokens per second on Llama3.1 8B and 450 tokens per second on Llama3.1 70B models, outperforms NVIDIA’s GPU-based solutions by 20-fold and is 2.4 times faster than Groq for the 8B model. Notably, Cerebras stands alone in offering immediate responses at a rate of 450 tokens per second on the 70B model. | [NVIDIA 🎮](Topic_NVIDIA.md), [AI Chips and GPUs 🖥️](Topic_AI_Chips_and_GPUs.md), [LLaMA 🦙](Topic_LLaMA.md) | 2024-09-02 |
| [Zuckerberg says Meta will need 10x more computing power to train Llama 4 than Llama 3](https://techcrunch.com/2024/08/01/zuckerberg-says-meta-will-need-10x-more-computing-power-to-train-llama-4-than-llama-3/) 🔴 | Meta’s CEO, Mark Zuckerberg, has stated that their upcoming language model, Llama 4, will require a tenfold increase in computing power for training compared to its predecessor, Llama 3, suggesting significant capital expenditure on infrastructure. However, CFO Susan Li clarified that these AI advancements are not anticipated to yield substantial revenue in the near term. | [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md) | 2024-08-12 |
| [Meta releases Llama 3.1](https://ai.meta.com/blog/meta-llama-3-1/) 🟢 | Meta unveiled the Llama 3.1 405B model, a cutting-edge open-source large language AI with advanced multilingual, reasoning, and tool-use features, alongside improvements to its 8B and 70B models. The update offers extended context lengths, better training, and thorough evaluations, available for download on various platforms. | [Model release 🎉](Topic_Model_release.md), [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md) | 2024-07-29 |
| [Meta to drop Llama 3 400b next week — here’s why you should care](https://www.tomsguide.com/ai/meta-to-drop-llama-3-400b-next-week-heres-why-you-should-care) 🟢 | Meta plans to launch Llama 3 400B in July 2024, expanding the Llama 3 AI model series. This open-source model will offer improved features for chatbots and multilingual applications, aiming to provide wide access to the latest AI advancements. | [Model release 🎉](Topic_Model_release.md), [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md) | 2024-07-22 |
| [Meta’s open source GPT-4 competitor Llama 3 is coming soon](https://the-decoder.com/metas-open-source-gpt-4-competitor-llama-3-is-coming-soon/) 🟢 | Meta is set to release Llama 3, an AI assistant intended to outperform its predecessors and compete with OpenAI’s GPT-4. It will debut with two preliminary versions before launching a comprehensive multimodal iteration in the summer. | [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md), [GPT-4 and GPT-4 turbo 🚀](Topic_GPT-4_and_GPT-4_turbo.md), [OpenAI 🌟](Topic_OpenAI.md), [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md) | 2024-04-15 |
| [Hugging Face launches open source AI assistant maker to rival OpenAI’s custom GPTs](https://venturebeat.com/ai/hugging-face-launches-open-source-ai-assistant-maker-to-rival-openais-custom-gpts/) 🟢 | Hugging Face has introduced free, customizable Chat Assistants on its Hugging Chat platform, presenting an open-source alternative to OpenAI’s GPT services. This initiative offers developers and AI enthusiasts cost-free access to various large language models, including Mistral’s Mixtral and Meta’s Llama 2. | [Hugging Face 🤗](Topic_Hugging_Face.md), [Mistral 🌬️](Topic_Mistral.md), [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md), [ChatGPT 💬](Topic_ChatGPT.md), [OpenAI 🌟](Topic_OpenAI.md) | 2024-02-05 |
| [Meta quietly unveils Llama 2 Long AI that beats GPT-3.5 Turbo and Claude 2 on some tasks](https://venturebeat.com/ai/meta-quietly-releases-llama-2-long-ai-that-outperforms-gpt-3-5-and-claude-2-on-some-tasks/) 🟢 | Meta is releasing Llama 2 Long, an enhanced version of Llama 2 that underwent continual pretraining with longer training sequences and upsampled long texts. By adding 400 billion tokens and making minor changes to the Rotary Positional Embedding (RoPE), Llama 2 Long can now attend to longer information sequences and include less related information in its model’s knowledge base. | [Model release 🎉](Topic_Model_release.md), [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2023-10-10 |
| [Releasing Persimmon-8B](https://www.adept.ai/blog/persimmon-8b) 🟢 | Adept.ai introduces Persimmon-8B, an open-source LLM with impressive performance and a compact size. Trained on less data, it achieves comparable results to LLaMA2 and offers fast C++ implementation combined with flexible Python inference. | [Model release 🎉](Topic_Model_release.md), [LLaMA 🦙](Topic_LLaMA.md) | 2023-09-11 |
| [Meet FreeWilly, Our Large And Mighty Instruction Fine-Tuned Models](https://stability.ai/blog/freewilly-large-instruction-fine-tuned-models) 🟢 | Stability AI and CarperAI Lab have collaborated to release FreeWilly, a LLaMA 2 model fine-tuned using Supervised Fine-Tune (SFT) techniques. FreeWilly2 performs comparably to GPT-3.5 in certain tasks, and its capabilities have been verified by both Stability AI researchers and Hugging Face. Both models are publicly available under a non-commercial license. | [Model release 🎉](Topic_Model_release.md), [Stability AI ⚖️](Topic_Stability_AI.md), [LLaMA 🦙](Topic_LLaMA.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2023-07-31 |
| [Meta releases Llama 2](https://ai.meta.com/resources/models-and-libraries/llama/) 🟢 | Meta has released Llama-2, an open-source model with a commercial license, that showcases similar performance to ChatGPT. Trained on 2T tokens with varying parameter sizes, Llama-2 was further fine-tuned and improved using a combination of instruction and reinforcement learning, outperforming other open-source models like Falcon and MPT. | [Model release 🎉](Topic_Model_release.md), [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md), [ChatGPT 💬](Topic_ChatGPT.md) | 2023-07-24 |
| [Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs, by MosaicML](https://www.mosaicml.com/blog/mpt-7b) 🟢 | MPT-7B is a transformer trained from scratch on 1T tokens of text and code. It is open-source, available for commercial use, and matches the quality of LLaMA-7B. | [Model release 🎉](Topic_Model_release.md), [LLaMA 🦙](Topic_LLaMA.md) | 2023-05-09 |
| [Releasing 3B and 7B RedPajama-INCITE family of models including base, instruction-tuned & chat models](https://www.together.xyz/blog/redpajama-models-v1) 🟢 | The release includes the first models trained on the RedPajama base dataset: a 3 billion and a 7B parameter base model that aims to replicate the LLaMA recipe as closely as possible. | [Model release 🎉](Topic_Model_release.md), [LLaMA 🦙](Topic_LLaMA.md) | 2023-05-09 |
| [RedPajama, a project to create leading open-source models, starts by reproducing LLaMA training dataset of over 1.2 trillion tokens](https://www.together.xyz/blog/redpajama) 🟢 | RedPajama aims to reproduce LLaMA models with a 7B parameter model, using a filtered dataset of 1.2T tokens. Their goal is open-source reproducibility. | [AI datasets 📊](Topic_AI_datasets.md), [LLaMA 🦙](Topic_LLaMA.md) | 2023-04-24 |
| [Releasing Alpaca-30B](https://abuqader.substack.com/p/releasing-alpaca-30b) 🟢 | Article explains release of Alpaca-30B, “instruction-tuned” version of Facebook’s Llama model, benefits of fine-tuning, usage steps and community involvement. | [Model release 🎉](Topic_Model_release.md), [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md) | 2023-03-27 |