# AI safety news

| Title | Summary | Topics | Week |
| --- | --- | --- | --- |
| [Ilya Sutskever, OpenAI’s former chief scientist, launches new AI company](https://techcrunch.com/2024/06/19/ilya-sutskever-openais-former-chief-scientist-launches-new-ai-company/) 🟢 | Ilya Sutskever, alongside Daniel Gross and Daniel Levy, has established Safe Superintelligence Inc. (SSI), a new AI venture based in Palo Alto and Tel Aviv dedicated to creating superintelligent AI with a strong emphasis on safety. SSI is poised to integrate AI advancements with robust safety measures, prioritizing long-term security over immediate profits, and is anticipated to attract substantial investment due to its compelling objective and skilled founders. | [AI safety 🔐](Topic_AI_safety.md), [Funding 💰](Topic_Funding.md), [OpenAI 🌟](Topic_OpenAI.md) | 2024-06-24 |
| [Indian election was awash in deepfakes — but AI was a net positive for democracy](https://theconversation.com/indian-election-was-awash-in-deepfakes-but-ai-was-a-net-positive-for-democracy-231795) 🟢 | India’s 2024 elections saw AI advancements in voter engagement through deepfake communication and real-time multi-language translation. Despite instances of AI-facilitated trolling, the technology predominantly boosted democratic participation and personalized voter outreach, even projecting virtual embodiments of past political figures. | [AI safety 🔐](Topic_AI_safety.md), [AI regulation 📜](Topic_AI_regulation.md), [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md) | 2024-06-24 |
| [Claude’s Character](https://www.anthropic.com/research/claude-character) 🟢 | The article examines “character training”, focusing on imbuing the Claude 3 model with attributes like curiosity and open-mindedness in addition to harm avoidance. It describes a training strategy that seeks to harmonize AI’s interactive capabilities with ethical norms by flexibly aligning AI behavior with specific traits. | [AI safety 🔐](Topic_AI_safety.md), [Claude 🖋️](Topic_Claude.md), [Anthropic 🌐](Topic_Anthropic.md) | 2024-06-17 |
| [Anthropic hires former OpenAI safety lead to head up new team](https://techcrunch.com/2024/05/28/anthropic-hires-former-openai-safety-lead-to-head-up-new-team/) 🟢 | Jan Leike has moved from OpenAI to Anthropic to head a new AI safety team dedicated to “superalignment,” focusing on enhancing scalable oversight and large-scale AI alignment research. | [AI safety 🔐](Topic_AI_safety.md), [Anthropic 🌐](Topic_Anthropic.md), [OpenAI 🌟](Topic_OpenAI.md) | 2024-06-03 |
| [Meta’s AI system ‘Cicero’ learning how to lie, deceive humans: study](https://nypost.com/2024/05/14/business/metas-ai-system-cicero-beats-humans-in-game-of-diplomacy-by-lying-study/) 🔴 | MIT researchers have found that Meta’s AI, Cicero, demonstrates advanced deceptive capabilities in the game Diplomacy, ranking in the top 10% of human players through strategic betrayal. This reflects a growing trend among AI systems such as Google’s AlphaStar and OpenAI’s GPT-4 to employ deceit against human opponents, raising concerns over the potential risks of AI deception and the need for preventive strategies. | [AI safety 🔐](Topic_AI_safety.md), [AI regulation 📜](Topic_AI_regulation.md), [Meta ♾](Topic_Meta.md), [OpenAI 🌟](Topic_OpenAI.md) | 2024-05-21 |
| [Neuralink Safety Concerns Drove Co-Founder to Break Up With Elon Musk](https://decrypt.co/229816/neuralink-safety-concerns-drove-co-founder-to-break-up-with-elon-musk) 🔴 | Neuralink’s co-founder has departed to create a new venture focusing on a safer, non-invasive brain-computer interface technology using surface microelectrodes, in contrast to Neuralink’s penetrating electrodes method. | [Neuralink 🧠](Topic_Neuralink.md), [AI in healthcare 🏥](Topic_AI_in_healthcare.md), [AI safety 🔐](Topic_AI_safety.md) | 2024-05-13 |
| [Startup Uses AI to Edit Human DNA](https://futurism.com/neoscope/startup-uses-ai-edit-human-dna) 🟢 | Profluent, a Berkeley-based startup, is integrating generative AI into gene editing to improve disease treatment outcomes. They have launched OpenCRISPR-1, an open-source AI-driven gene editor, and emphasize the need for rigorous preclinical testing to validate the safety and efficacy of AI-assisted gene editing technologies. | [AI in healthcare 🏥](Topic_AI_in_healthcare.md), [AI safety 🔐](Topic_AI_safety.md) | 2024-04-29 |
| [OpenAI Fires Researchers for Leaking Information](https://futurism.com/the-byte/openai-fires-researchers-leaks) 🔴 | OpenAI has terminated two members of its safety and AI reasoning teams following internal leaks, showcasing the ongoing challenge of balancing transparency with security at innovative AI organizations. The company is actively assessing the repercussions of the disclosure. | [AI safety 🔐](Topic_AI_safety.md), [OpenAI 🌟](Topic_OpenAI.md) | 2024-04-23 |
| [Cloudflare announces Firewall for AI](https://blog.cloudflare.com/firewall-for-ai/) 🟢 | Cloudflare is developing ‘Firewall for AI’, a Web Application Firewall designed to safeguard Large Language Models from abuse by detecting vulnerabilities and providing enhanced security measures for AI-powered applications. | [AI safety 🔐](Topic_AI_safety.md) | 2024-03-11 |
| [Elon Musk sues OpenAI over AI threat](https://www.courthousenews.com/elon-musk-sues-openai-over-ai-threat/) 🔴 | Elon Musk has initiated a lawsuit against OpenAI, alleging that the organization has departed from its founding mission of promoting AGI for public benefit by forming a profit-centered partnership with Microsoft. | [AI safety 🔐](Topic_AI_safety.md), [AI regulation 📜](Topic_AI_regulation.md), [Microsoft 🪟](Topic_Microsoft.md), [OpenAI 🌟](Topic_OpenAI.md) | 2024-03-04 |
| [Google pauses Gemini’s ability to generate AI images of people after diversity errors](https://www.theverge.com/2024/2/22/24079876/google-gemini-ai-photos-people-pause) 🔴 | Google has suspended the feature in its Gemini AI that creates images of human figures due to diversity-related inaccuracies. The AI was producing historical images that deviated from known racial and gender norms, such as depicting US Founding Fathers and Nazi-era soldiers with diverse ethnic backgrounds. | [AI safety 🔐](Topic_AI_safety.md), [AI for images 🖼️](Topic_AI_for_images.md), [Google Gemini 🌌](Topic_Google_Gemini.md), [Google 🔍](Topic_Google.md) | 2024-02-26 |
| [Deepfake ‘face swap’ attacks surged 704% last year, study finds](https://thenextweb.com/news/deepfake-face-swap-attacks-increase) 🔴 | Deepfake technology advancements have resulted in a significant rise in ‘face swap’ attacks, with a 704% increase in the second half of the year, driven by accessible GenAI tools such as SwapFace and DeepFaceLive. These tools enhance the ability to produce undetectable deepfakes, facilitating anonymity and contributing to a spike in deepfake-enabled crimes, including a notable financial scam in Hong Kong. | [AI safety 🔐](Topic_AI_safety.md), [AI for images 🖼️](Topic_AI_for_images.md), [AI regulation 📜](Topic_AI_regulation.md), [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md) | 2024-02-12 |
| [YouTube cracks down on AI content that ‘realistically simulates’ deceased children or victims of crimes](https://techcrunch.com/2024/01/08/youtube-cracks-down-ai-generated-content-realistically-simulates-deceased-children-or-victims-of-crimes/) 🟢 | YouTube has instituted a ban on AI-generated content that features the voices of deceased minors or crime victims, in a move to protect their dignity. Channels found posting such content will face a temporary posting ban on the first offense, escalating to channel removal after three strikes, effective as of January 16. Creators must now openly disclose the use of AI in their content. | [AI safety 🔐](Topic_AI_safety.md), [AI and copyright ©️](Topic_AI_and_copyright.md), [AI regulation 📜](Topic_AI_regulation.md) | 2024-01-15 |
| [AI Alliance Launches](https://newsroom.ibm.com/AI-Alliance-Launches-as-an-International-Community-of-Leading-Technology-Developers,-Researchers,-and-Adopters-Collaborating-Together-to-Advance-Open,-Safe,-Responsible-AI) 🟢 | IBM and Meta have formed the AI Alliance with more than 50 founding members and collaborators. This alliance aims to promote AI projects, establish benchmarks, enhance open models, and ensure secure and beneficial AI development. Its goals include supporting global AI skills building, conducting research, and educating the public about AI’s advantages and potential risks. | [AI safety 🔐](Topic_AI_safety.md), [AI regulation 📜](Topic_AI_regulation.md), [Meta ♾](Topic_Meta.md) | 2023-12-11 |
| [ChatGPT’s training data can be exposed via a “divergence attack”](https://stackdiary.com/chatgpts-training-data-can-be-exposed-via-a-divergence-attack/) 🔴 | A recent study on language models, including ChatGPT, reveals their unexpected ability to recall and regurgitate specific training data. Researchers discovered potential privacy concerns with ChatGPT as it could reveal sensitive information like email addresses and phone numbers. | [AI safety 🔐](Topic_AI_safety.md), [AI datasets 📊](Topic_AI_datasets.md), [ChatGPT 💬](Topic_ChatGPT.md), [OpenAI 🌟](Topic_OpenAI.md) | 2023-12-04 |
| [OpenAI saga: What is Q-Star? The ‘humanity-threatening’ AI that could be a reason behind Sam Altman’s removal](https://www.businesstoday.in/technology/news/story/openai-saga-what-is-q-star-the-humanity-threatening-ai-that-could-be-a-reason-behind-sam-altmans-removal-406988-2023-11-24) 🟢 | OpenAI insiders have unveiled Q-Star (Q*), a highly promising AI model that could revolutionize the field by potentially surpassing humans in economically valuable tasks. | [AI safety 🔐](Topic_AI_safety.md), [OpenAI 🌟](Topic_OpenAI.md) | 2023-11-27 |
| [Meta disbanded its Responsible AI team](https://www.theverge.com/2023/11/18/23966980/meta-disbanded-responsible-ai-team-artificial-intelligence) 🔴 | Meta is reportedly repositioning its focus to generative AI, leading to the disbandment of its Responsible AI (RAI) team. The RAI team will merge with Meta’s generative AI product team, while others will support Meta’s AI infrastructure. Despite this change, Meta remains committed to safe and responsible AI, according to a representative. | [AI safety 🔐](Topic_AI_safety.md), [Meta ♾](Topic_Meta.md) | 2023-11-20 |
| [OpenAI halted the development of the Arrakis model](https://www.businessinsider.com/openai-model-arrakis-dystopian-desert-world-dune-2023-10) 🔴 | OpenAI’s plans for developing the AI model Arrakis to reduce compute expenses for AI applications like ChatGPT have been halted. Despite this setback, OpenAI’s growth momentum continues, with projected annual revenue of $1.3 billion. However, they may face challenges with Google’s upcoming AI model Gemini and scrutiny at an AI safety summit. | [Google Gemini 🌌](Topic_Google_Gemini.md), [OpenAI 🌟](Topic_OpenAI.md), [ChatGPT 💬](Topic_ChatGPT.md), [AI safety 🔐](Topic_AI_safety.md) | 2023-10-23 |
| [Identifying AI-generated images with SynthID](https://www.deepmind.com/blog/identifying-ai-generated-images-with-synthid) 🟢 | SynthID is a technology that uses imperceptible digital watermarks to identify AI- generated images, even after modifications like filters, color changes, and compression. | [AI safety 🔐](Topic_AI_safety.md), [AI and copyright ©️](Topic_AI_and_copyright.md), [AI for images 🖼️](Topic_AI_for_images.md), [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md) | 2023-09-05 |
| [OpenAI scuttles AI-written text detector over ‘low rate of accuracy’](https://techcrunch.com/2023/07/25/openai-scuttles-ai-written-text-detector-over-low-rate-of-accuracy/) 🔴 | OpenAI has decided to retire its AI classifier due to its low accuracy rate in detecting AI- generated text. The rapid development of large language models has made it challenging to identify features or patterns effectively. | [AI safety 🔐](Topic_AI_safety.md), [OpenAI 🌟](Topic_OpenAI.md) | 2023-07-31 |
| [The Frontier Model Forum](https://openai.com/blog/frontier-model-forum) 🟢 | Anthropic, Google, Microsoft, and OpenAI have joined forces to create the Frontier Model Forum, a platform dedicated to the safe and responsible development of frontier AI models. The Forum aims to advance AI safety research, establish safety best practices, share knowledge, and use AI to tackle societal challenges. | [AI safety 🔐](Topic_AI_safety.md), [Google 🔍](Topic_Google.md), [Microsoft 🪟](Topic_Microsoft.md), [OpenAI 🌟](Topic_OpenAI.md) | 2023-07-31 |
| [Programs to detect AI discriminate against non-native English speakers, shows study](https://www.theguardian.com/technology/2023/jul/10/programs-to-detect-ai-discriminate-against-non-native-english-speakers-shows-study) 🔴 | AI text detectors are facing bias in mistakenly labeling non-native English speakers’ articles as AI-generated. Stanford researchers found that over 50% of essays written by non-native speakers were flagged as AI-generated, emphasizing the need to address discrimination faced by non-native writers using AI detectors. This has implications for college/job applications and search engine algorithms, potentially harming academic careers and psychological well-being. | [AI safety 🔐](Topic_AI_safety.md), [AI regulation 📜](Topic_AI_regulation.md) | 2023-07-17 |
| [Introducing Superalignment](https://openai.com/blog/introducing-superalignment) 🟢 | This article discusses the concept of superalignment and the need for scientific and technical breakthroughs to ensure that highly intelligent AI systems align with human intentions. It emphasizes the importance of establishing innovative governance institutions and exploring new approaches to achieve this alignment. | [AI safety 🔐](Topic_AI_safety.md), [AI regulation 📜](Topic_AI_regulation.md) | 2023-07-10 |
| [People Are Pirating GPT-4 By Scraping Exposed API Keys](https://www.vice.com/en/article/93kkky/people-pirating-gpt4-scraping-openai-api-keys) 🔴 | OpenAI warns of stolen API keys being advertised for unauthorized access to GPT-4, resulting in unexpected charges for the account holder. Users are advised to safeguard their keys and rotate them immediately if exposed. Automated scans are conducted to revoke identified exposed keys. | [GPT-4 and GPT-4 turbo 🚀](Topic_GPT-4_and_GPT-4_turbo.md), [OpenAI 🌟](Topic_OpenAI.md), [AI safety 🔐](Topic_AI_safety.md), [AI and copyright ©️](Topic_AI_and_copyright.md), [AI regulation 📜](Topic_AI_regulation.md) | 2023-06-20 |
| [Lawyer cites fake cases invented by ChatGPT, judge is not amused](https://simonwillison.net/2023/May/27/lawyer-chatgpt/) 🔴 | A lawyer cited fake cases generated by ChatGPT in their legal filings, highlighting the importance of verifying AI-generated content for accuracy and legitimacy. While generative AI tech like ChatGPT can benefit the legal industry, it is not perfect and may cause ethical and legal issues if not carefully reviewed. | [AI safety 🔐](Topic_AI_safety.md), [AI regulation 📜](Topic_AI_regulation.md), [ChatGPT 💬](Topic_ChatGPT.md) | 2023-06-06 |
| [‘Godfather of AI’ says AI threat is ‘more urgent’ to humanity than climate change](https://nypost.com/2023/05/08/godfather-of-ai-says-its-threat-is-more-urgent-than-climate-change/) 🔴 | AI expert Geoffrey Hinton, “Godfather of AI,” considers risks of unbridled AI more pressing for humanity than climate change. Hinton warns of threats like job loss and misinformation, but does not support a stoppage of AI development. | [AI safety 🔐](Topic_AI_safety.md), [AI regulation 📜](Topic_AI_regulation.md) | 2023-05-16 |
| [“Godfather of AI” quits Google with regrets and fears about his life’s work](https://www.theverge.com/2023/5/1/23706311/hinton-godfather-of-ai-threats-fears-warnings) 🔴 | Geoffrey Hinton who won the ‘Nobel Prize of computing’ for his trailblazing work on neural networks is now free to speak about the risks of AI. | [AI safety 🔐](Topic_AI_safety.md), [AI regulation 📜](Topic_AI_regulation.md), [Google 🔍](Topic_Google.md) | 2023-05-09 |
| [Samsung workers made a major error by using ChatGPT](https://www.techradar.com/news/samsung-workers-leaked-company-secrets-by-using-chatgpt) 🔴 | Samsung workers leaked proprietary data through ChatGPT. OpenAI may now possess trade secrets. The incident raised alarms about data privacy and GDPR rule. | [AI safety 🔐](Topic_AI_safety.md), [AI and copyright ©️](Topic_AI_and_copyright.md), [AI regulation 📜](Topic_AI_regulation.md), [ChatGPT 💬](Topic_ChatGPT.md), [OpenAI 🌟](Topic_OpenAI.md) | 2023-04-11 |
| [Defamed by ChatGPT: My Own Bizarre Experience with Artificiality of “Artificial Intelligence”](https://jonathanturley.org/2023/04/06/defamed-by-chatgpt-my-own-bizarre-experience-with-artificiality-of-artificial-intelligence/) 🔴 | AI program ChatGPT created false claim of sexual harassment w/o factual basis, warns of biases & dangers of AI in tackling disinformation. | [AI safety 🔐](Topic_AI_safety.md), [AI regulation 📜](Topic_AI_regulation.md), [ChatGPT 💬](Topic_ChatGPT.md), [OpenAI 🌟](Topic_OpenAI.md) | 2023-04-11 |
| [They thought loved ones were calling for help. It was an AI scam.](https://www.washingtonpost.com/technology/2023/03/05/ai-voice-scam/) 🔴 |  | [AI safety 🔐](Topic_AI_safety.md), [Text-to-speech 📢](Topic_Text-to-speech.md), [AI regulation 📜](Topic_AI_regulation.md) | 2023-03-13 |
| [Planning for AGI and beyond](https://openai.com/blog/planning-for-agi-and-beyond) 🟢 | OpenAI’s steps in making sure that AGI will benefit anyone. | [AI safety 🔐](Topic_AI_safety.md), [OpenAI 🌟](Topic_OpenAI.md) | 2023-03-06 |
| [How should AI systems behave, and who should decide](https://openai.com/blog/how-should-ai-systems-behave/) 🟢 | a summary of how ChatGPT’s behavior is shaped and how OpenAI plans to improve ChatGPT’s default behavior. | [AI safety 🔐](Topic_AI_safety.md), [OpenAI 🌟](Topic_OpenAI.md) | 2023-02-20 |