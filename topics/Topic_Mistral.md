# Mistral news

| Title | Summary | Topics | Week |
| --- | --- | --- | --- |
| [Mistral releases Pixtral Large, a 124B multimodal model](https://mistral.ai/news/pixtral-large/) 🟢 | Mistral AI unveils Pixtral Large, a 124B multimodal model excelling in image and text understanding. Built on Mistral Large 2, it surpasses models like GPT-4o on benchmarks such as MathVista and DocVQA. Available for both research and commercial use, it boasts a 128K context window and advanced capabilities. | [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md), [Model release 🎉](Topic_Model_release.md), [Mistral 🌬️](Topic_Mistral.md), [GPT-4 and GPT-4 turbo 🚀](Topic_GPT-4_and_GPT-4_turbo.md) | 2024-11-25 |
| [Mistral releases new AI models optimized for laptops and phones](https://techcrunch.com/2024/10/16/mistral-releases-new-ai-models-optimized-for-edge-devices/) 🟢 | Mistral has introduced “Les Ministraux,” a series of AI models, Ministral 3B and Ministral 8B, optimized for edge devices like laptops and phones. These models focus on privacy-first, low-latency applications such as on-device translation and local analytics. Available for download or via Mistral’s cloud platform, they reportedly outperform competitors in AI benchmarks. This launch follows Mistral’s recent $640 million funding round, signaling continued expansion in AI offerings. | [Model release 🎉](Topic_Model_release.md), [Funding 💰](Topic_Funding.md), [Mistral 🌬️](Topic_Mistral.md) | 2024-10-21 |
| [Zyphra releases Zamba2–7B](https://zyphra.webflow.io/post/zamba2-7b) 🟢 | Zyphra has introduced Zamba2–7B, a 7B-scale language model that outperforms competitors such as Mistral, Google’s Gemma, and Meta’s Llama3. It features innovative Mamba2 blocks and dual shared attention layers, resulting in improved inference speed and reduced memory usage. | [Model release 🎉](Topic_Model_release.md), [Mistral 🌬️](Topic_Mistral.md), [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md), [Google Gemini 🌌](Topic_Google_Gemini.md), [Google 🔍](Topic_Google.md) | 2024-10-21 |
| [Mistral releases Pixtral 12B, its first multimodal model](https://techcrunch.com/2024/09/11/mistral-releases-pixtral-its-first-multimodal-model/) 🟢 | Mistral has introduced Pixtral 12B, a 12-billion-parameter multimodal AI model that processes both text and images. Building on features from their previous text model, Nemo 12B, Pixtral 12B is available for free download on GitHub and Hugging Face under an Apache 2.0 license. | [Multimodal AI (image, video, audio) 📸](Topic_Multimodal_AI_(image_video_audio).md), [Model release 🎉](Topic_Model_release.md), [Mistral 🌬️](Topic_Mistral.md) | 2024-09-16 |
| [Mistral alpha release of agents](https://mistral.ai/news/build-tweak-repeat/) 🟢 | Mistral has introduced customization options for its models, including base prompts, few-shot prompting, and fine-tuning. The platform also launched an alpha version of Agents for workflow automation and debuted a stable client SDK for improved integration and application development. | [Mistral 🌬️](Topic_Mistral.md) | 2024-08-12 |
| [Mistral releases Mistral Large 2](https://mistral.ai/news/mistral-large-2407/) 🟢 | Mistral launched their new model, Mistral Large 2, with 123 billion parameters and a 128k context window, offering multi-language and programming language support, optimized for high-throughput single-node inference. It delivers 84.0% accuracy on the MMLU benchmark, exhibits enhanced code generation, and reasoning capabilities. The model is available with research and commercial licensing options. | [AI for coding 👨‍💻](Topic_AI_for_coding.md), [Model release 🎉](Topic_Model_release.md), [Mistral 🌬️](Topic_Mistral.md) | 2024-07-29 |
| [Mistral NeMo](https://mistral.ai/news/mistral-nemo/) 🟢 | Mistral, in collaboration with NVIDIA, has launched the 12B parameter Mistral NeMo model, featuring a 128k token context window, FP8 inference compatibility, and a cutting-edge Tekken tokenizer. It is open-sourced under Apache 2.0, boasts enhanced multilingual capabilities, and outperforms the previous 7B version in instruction-following tasks. | [Model release 🎉](Topic_Model_release.md), [NVIDIA 🎮](Topic_NVIDIA.md), [Mistral 🌬️](Topic_Mistral.md) | 2024-07-22 |
| [Codestral Mamba](https://mistral.ai/news/codestral-mamba/) 🟢 | Mistral has introduced Codestral Mamba, a new coding-centric Mamba model known for efficiently managing long sequences with linear time inference and theoretical support for unlimited sequence lengths. It competes with leading SOTA models and is open-source, accessible for extension through the GitHub repository with integration options like mistral-inference SDK, TensorRT-LLM, and an upcoming llama.cpp. | [AI for coding 👨‍💻](Topic_AI_for_coding.md), [Model release 🎉](Topic_Model_release.md), [Mistral 🌬️](Topic_Mistral.md) | 2024-07-22 |
| [Paris-based AI startup Mistral AI raises $640M](https://techcrunch.com/2024/06/11/paris-based-ai-startup-mistral-ai-raises-640-million/) 🟢 | Mistral AI, a Paris-based AI startup with founders from Meta and DeepMind, secured $640M in a Series B round led by General Catalyst, reaching a $6B valuation, and focuses on creating cutting-edge AI technologies, balancing open-source and proprietary offerings. | [Funding 💰](Topic_Funding.md), [Mistral 🌬️](Topic_Mistral.md), [Meta ♾](Topic_Meta.md), [DeepMind 🧩](Topic_DeepMind.md) | 2024-06-17 |
| [Mistral releases Codestral](https://mistral.ai/news/codestral/) 🟢 | Codestral is Mistral AI’s new generative AI model focused on coding, boasting proficiency in over 80 programming languages and a large 32k context window for superior performance in benchmarks. | [AI for coding 👨‍💻](Topic_AI_for_coding.md), [Model release 🎉](Topic_Model_release.md), [Mistral 🌬️](Topic_Mistral.md) | 2024-06-03 |
| [mistralai/Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) 🟢 | Mistral has launched version 3 of their 7B model, the models “Mistral-7B-v0.3” and “Mistral-7B-Instruct-v0.3”. Enhancements include an expanded vocabulary of 32,768 terms, integration with the v3 Tokenizer, and new function calling capabilities. | [Mistral 🌬️](Topic_Mistral.md), [Model release 🎉](Topic_Model_release.md) | 2024-05-27 |
| [Mistral released Mixtral 8x22B](https://mistral.ai/news/mixtral-8x22b/) 🟢 | Mistral unveiled Mixtral 8x22B, an efficient sparse Mixture-of-Experts model with 39B active out of 141B total parameters, specializing in multilingual communication, coding, mathematics, and excelling in reasoning and knowledge tasks. The model boasts a 64K token context window, is compatible with multiple platforms, and is available under the open-source Apache 2.0 license. | [Mixture-of-Experts 🤝](Topic_Mixture-of-Experts.md), [AI for coding 👨‍💻](Topic_AI_for_coding.md), [Model release 🎉](Topic_Model_release.md), [Mistral 🌬️](Topic_Mistral.md) | 2024-04-23 |
| [Mistral AI releases new model to rival GPT-4 and its own chat assistant](https://techcrunch.com/2024/02/26/mistral-ai-releases-new-model-to-rival-gpt-4-and-its-own-chat-assistant/) 🟢 | Mistral has launched Mistral Large, ranking just below GPT4. It boasts a 32K token context window and multilingual support for English, French, Spanish, German, and Italian. The model excels in following precise instructions, allowing for tailored moderation policies. | [Model release 🎉](Topic_Model_release.md), [Mistral 🌬️](Topic_Mistral.md), [GPT-4 and GPT-4 turbo 🚀](Topic_GPT-4_and_GPT-4_turbo.md) | 2024-03-04 |
| [Microsoft partners with Mistral in second AI deal beyond OpenAI](https://www.theverge.com/2024/2/26/24083510/microsoft-mistral-partnership-deal-azure-ai) 🟢 | Microsoft enters a multiyear collaboration with Mistral, valued at €2 billion, acquiring a minor stake shortly after its significant investment in OpenAI. | [Mistral 🌬️](Topic_Mistral.md), [Microsoft 🪟](Topic_Microsoft.md), [OpenAI 🌟](Topic_OpenAI.md) | 2024-03-04 |
| [Mistral Confirms New Open Source AI Model Nearing GPT-4 Performance](https://news.slashdot.org/story/24/01/31/2145205/mistral-confirms-new-open-source-ai-model-nearing-gpt-4-performance) 🟢 | Mistral has recently confirmed that the “miqu-1–70b” Large Language Model, released on HuggingFace and exhibiting performance close to that of GPT-4, is a leaked quantized version of their technology. | [Hugging Face 🤗](Topic_Hugging_Face.md), [AI and copyright ©️](Topic_AI_and_copyright.md), [Mistral 🌬️](Topic_Mistral.md), [GPT-4 and GPT-4 turbo 🚀](Topic_GPT-4_and_GPT-4_turbo.md) | 2024-02-05 |
| [Hugging Face launches open source AI assistant maker to rival OpenAI’s custom GPTs](https://venturebeat.com/ai/hugging-face-launches-open-source-ai-assistant-maker-to-rival-openais-custom-gpts/) 🟢 | Hugging Face has introduced free, customizable Chat Assistants on its Hugging Chat platform, presenting an open-source alternative to OpenAI’s GPT services. This initiative offers developers and AI enthusiasts cost-free access to various large language models, including Mistral’s Mixtral and Meta’s Llama 2. | [Hugging Face 🤗](Topic_Hugging_Face.md), [Mistral 🌬️](Topic_Mistral.md), [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md), [ChatGPT 💬](Topic_ChatGPT.md), [OpenAI 🌟](Topic_OpenAI.md) | 2024-02-05 |
| [Mistral AI has released Mixtral 8x7B, an open-source sparse mixture-of-experts model](https://mistral.ai/news/mixtral-of-experts/) 🟢 | Mistral AI has introduced Mixtral 8x7B, an open-source, sparse mixture-of-experts model with 45B parameters. It outperforms LLaMA 2 70B and GPT-3.5 in NLP benchmarks, while also providing 6x faster inference speed. The model selectively uses only 12B parameters per inference, keeping operational costs equivalent to a 12B model. The Mixtral model is accessible through Mistral’s beta API and dev platform. | [Mixture-of-Experts 🤝](Topic_Mixture-of-Experts.md), [Model release 🎉](Topic_Model_release.md), [Mistral 🌬️](Topic_Mistral.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2023-12-11 |
| [Mistral is in discussions to raise a major round at a valuation of at least $2 billion](https://www.businessinsider.com/mistral-in-talks-to-raise-funding-at-2-billion-valuation-2023-11) 🟢 | Mistral, an AI startup aiming to rival OpenAI, is in talks for a significant funding round led by Andreessen Horowitz and General Catalyst. This round could value the company at over $2 billion and propel it to unicorn status in just six months. Mistral’s open-source models have gained attention, fueling excitement for generative AI, and investors are eagerly vying for the opportunity to support the “next OpenAI”. | [Funding 💰](Topic_Funding.md), [Mistral 🌬️](Topic_Mistral.md), [OpenAI 🌟](Topic_OpenAI.md) | 2023-12-11 |
| [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/) 🟢 | The Mistral 7B model, powered by Grouped-query attention (GQA) and Sliding Window Attention (SWA), outperforms other models in various domains while maintaining strong performance in both English and coding tasks. Its impressive benchmarks make it the best 7B model to date, enhancing inference speed and sequence handling efficiency. | [Mistral 🌬️](Topic_Mistral.md), [AI for coding 👨‍💻](Topic_AI_for_coding.md) | 2023-10-02 |
| [Introducing 100K Context Windows: Claude, by Anthropic](https://www.anthropic.com/index/100k-context-windows) 🟢 | AI language model, Claude, now analyzes and synthesizes vast text in seconds with a 100K token context window. Summarize docs, assess risks, and more. | [Mistral 🌬️](Topic_Mistral.md), [Anthropic 🌐](Topic_Anthropic.md) | 2023-05-16 |