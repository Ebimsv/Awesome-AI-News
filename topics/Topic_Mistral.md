# Mistral news

| Title | Summary | Topics | Week |
| --- | --- | --- | --- |
| [Paris-based AI startup Mistral AI raises $640M](https://techcrunch.com/2024/06/11/paris-based-ai-startup-mistral-ai-raises-640-million/) 🟢 | Mistral AI, a Paris-based AI startup with founders from Meta and DeepMind, secured $640M in a Series B round led by General Catalyst, reaching a $6B valuation, and focuses on creating cutting-edge AI technologies, balancing open-source and proprietary offerings. | [Funding 💰](Topic_Funding.md), [Mistral 🌬️](Topic_Mistral.md), [Meta ♾](Topic_Meta.md), [DeepMind 🧩](Topic_DeepMind.md) | 2024-06-17 |
| [Mistral releases Codestral](https://mistral.ai/news/codestral/) 🟢 | Codestral is Mistral AI’s new generative AI model focused on coding, boasting proficiency in over 80 programming languages and a large 32k context window for superior performance in benchmarks. | [AI for coding 👨‍💻](Topic_AI_for_coding.md), [Model release 🎉](Topic_Model_release.md), [Mistral 🌬️](Topic_Mistral.md) | 2024-06-03 |
| [mistralai/Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) 🟢 | Mistral has launched version 3 of their 7B model, the models “Mistral-7B-v0.3” and “Mistral-7B-Instruct-v0.3”. Enhancements include an expanded vocabulary of 32,768 terms, integration with the v3 Tokenizer, and new function calling capabilities. | [Mistral 🌬️](Topic_Mistral.md), [Model release 🎉](Topic_Model_release.md) | 2024-05-27 |
| [Mistral released Mixtral 8x22B](https://mistral.ai/news/mixtral-8x22b/) 🟢 | Mistral unveiled Mixtral 8x22B, an efficient sparse Mixture-of-Experts model with 39B active out of 141B total parameters, specializing in multilingual communication, coding, mathematics, and excelling in reasoning and knowledge tasks. The model boasts a 64K token context window, is compatible with multiple platforms, and is available under the open-source Apache 2.0 license. | [Mixture-of-Experts 🤝](Topic_Mixture-of-Experts.md), [AI for coding 👨‍💻](Topic_AI_for_coding.md), [Model release 🎉](Topic_Model_release.md), [Mistral 🌬️](Topic_Mistral.md) | 2024-04-23 |
| [Mistral AI releases new model to rival GPT-4 and its own chat assistant](https://techcrunch.com/2024/02/26/mistral-ai-releases-new-model-to-rival-gpt-4-and-its-own-chat-assistant/) 🟢 | Mistral has launched Mistral Large, ranking just below GPT4. It boasts a 32K token context window and multilingual support for English, French, Spanish, German, and Italian. The model excels in following precise instructions, allowing for tailored moderation policies. | [Model release 🎉](Topic_Model_release.md), [Mistral 🌬️](Topic_Mistral.md), [GPT-4 and GPT-4 turbo 🚀](Topic_GPT-4_and_GPT-4_turbo.md) | 2024-03-04 |
| [Microsoft partners with Mistral in second AI deal beyond OpenAI](https://www.theverge.com/2024/2/26/24083510/microsoft-mistral-partnership-deal-azure-ai) 🟢 | Microsoft enters a multiyear collaboration with Mistral, valued at €2 billion, acquiring a minor stake shortly after its significant investment in OpenAI. | [Mistral 🌬️](Topic_Mistral.md), [Microsoft 🪟](Topic_Microsoft.md), [OpenAI 🌟](Topic_OpenAI.md) | 2024-03-04 |
| [Mistral Confirms New Open Source AI Model Nearing GPT-4 Performance](https://news.slashdot.org/story/24/01/31/2145205/mistral-confirms-new-open-source-ai-model-nearing-gpt-4-performance) 🟢 | Mistral has recently confirmed that the “miqu-1–70b” Large Language Model, released on HuggingFace and exhibiting performance close to that of GPT-4, is a leaked quantized version of their technology. | [Hugging Face 🤗](Topic_Hugging_Face.md), [AI and copyright ©️](Topic_AI_and_copyright.md), [Mistral 🌬️](Topic_Mistral.md), [GPT-4 and GPT-4 turbo 🚀](Topic_GPT-4_and_GPT-4_turbo.md) | 2024-02-05 |
| [Hugging Face launches open source AI assistant maker to rival OpenAI’s custom GPTs](https://venturebeat.com/ai/hugging-face-launches-open-source-ai-assistant-maker-to-rival-openais-custom-gpts/) 🟢 | Hugging Face has introduced free, customizable Chat Assistants on its Hugging Chat platform, presenting an open-source alternative to OpenAI’s GPT services. This initiative offers developers and AI enthusiasts cost-free access to various large language models, including Mistral’s Mixtral and Meta’s Llama 2. | [Hugging Face 🤗](Topic_Hugging_Face.md), [Mistral 🌬️](Topic_Mistral.md), [LLaMA 🦙](Topic_LLaMA.md), [Meta ♾](Topic_Meta.md), [ChatGPT 💬](Topic_ChatGPT.md), [OpenAI 🌟](Topic_OpenAI.md) | 2024-02-05 |
| [Mistral AI has released Mixtral 8x7B, an open-source sparse mixture-of-experts model](https://mistral.ai/news/mixtral-of-experts/) 🟢 | Mistral AI has introduced Mixtral 8x7B, an open-source, sparse mixture-of-experts model with 45B parameters. It outperforms LLaMA 2 70B and GPT-3.5 in NLP benchmarks, while also providing 6x faster inference speed. The model selectively uses only 12B parameters per inference, keeping operational costs equivalent to a 12B model. The Mixtral model is accessible through Mistral’s beta API and dev platform. | [Mixture-of-Experts 🤝](Topic_Mixture-of-Experts.md), [Model release 🎉](Topic_Model_release.md), [Mistral 🌬️](Topic_Mistral.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo 💡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2023-12-11 |
| [Mistral is in discussions to raise a major round at a valuation of at least $2 billion](https://www.businessinsider.com/mistral-in-talks-to-raise-funding-at-2-billion-valuation-2023-11) 🟢 | Mistral, an AI startup aiming to rival OpenAI, is in talks for a significant funding round led by Andreessen Horowitz and General Catalyst. This round could value the company at over $2 billion and propel it to unicorn status in just six months. Mistral’s open-source models have gained attention, fueling excitement for generative AI, and investors are eagerly vying for the opportunity to support the “next OpenAI”. | [Funding 💰](Topic_Funding.md), [Mistral 🌬️](Topic_Mistral.md), [OpenAI 🌟](Topic_OpenAI.md) | 2023-12-11 |
| [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/) 🟢 | The Mistral 7B model, powered by Grouped-query attention (GQA) and Sliding Window Attention (SWA), outperforms other models in various domains while maintaining strong performance in both English and coding tasks. Its impressive benchmarks make it the best 7B model to date, enhancing inference speed and sequence handling efficiency. | [Mistral 🌬️](Topic_Mistral.md), [AI for coding 👨‍💻](Topic_AI_for_coding.md) | 2023-10-02 |
| [Introducing 100K Context Windows: Claude, by Anthropic](https://www.anthropic.com/index/100k-context-windows) 🟢 | AI language model, Claude, now analyzes and synthesizes vast text in seconds with a 100K token context window. Summarize docs, assess risks, and more. | [Mistral 🌬️](Topic_Mistral.md), [Anthropic 🌐](Topic_Anthropic.md) | 2023-05-16 |