[{"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-april-29th-2024-6d03b776d280", "news": [{"url": "https://www.theverge.com/2024/4/23/24137534/microsoft-phi-3-launch-small-ai-language-model", "summary": "Microsoft unveils its new language model, Phi-3 Mini, comprising 3.8 billion parameters, and announces forthcoming variants including Phi-3 Small and Phi-3 Medium with 7 billion and 14 billion parameters respectively. The training approach for Phi-3 Mini mimics the progressive learning stages of children, utilizing a curriculum of materials ranging from simple to complex structures and concepts.", "title": "Microsoft launches Phi-3, its smallest AI model yet", "topics": ["Microsoft"], "sentiment": "positive"}, {"url": "https://venturebeat.com/ai/apple-releases-openelm-small-open-source-ai-models-designed-to-run-on-device/", "summary": "Apple has introduced OpenELM, a suite of open-source AI text generation models with 270M to 3B parameters, optimized for on-device deployment. Available on Hugging Face, these models are released under a sample code license to enable AI functionalities independently of the cloud.", "title": "Apple releases OpenELM: small, open source AI models designed to run on-device", "topics": ["Hugging Face", "Apple", "Model release"], "sentiment": "positive"}, {"url": "https://pharmaphorum.com/news/moderna-banks-openai-accelerate-mrna-research", "summary": "Modern Pharma collaborates with OpenAI to incorporate generative AI into its mRNA development and operational processes, securing ChatGPT Enterprise access for around 3,000 Moderna staff, leveraging GPT-4\u2019s advanced language models.", "title": "Moderna banks on OpenAI to accelerate mRNA research", "topics": ["AI in healthcare", "ChatGPT", "GPT-4 and GPT-4 turbo", "OpenAI"], "sentiment": "positive"}, {"url": "https://gizmodo.com/chatgpt-openai-could-power-apple-iphones-ai-chatbot-1851439853", "summary": "Apple is negotiating with OpenAI and Google for potential integration of generative AI technologies such as ChatGPT or Gemini into the iPhone\u2019s iOS 18, aiming to bridge its in-house AI development gap and circumvent past deployment issues while risking increased reliance on external AI advancements.", "title": "ChatGPT Could Power the iPhone\u2019s AI Chatbot: Report", "topics": ["Apple", "Google Gemini", "Google", "ChatGPT", "OpenAI"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2024/04/25/xai-elon-musks-openai-rival-is-closing-on-6b-in-funding-and-x-his-social-network-is-already-one-of-its-shareholders/", "summary": "Elon Musk\u2019s artificial intelligence startup, xAI, is on the verge of securing a $6 billion investment at an $18 billion pre-money valuation. Supported by Sequoia Capital and Future Ventures, xAI\u2019s funding round demonstrates significant investor confidence, partly due to Musk\u2019s reputation and connections from his ventures such as SpaceX and Tesla.", "title": "xAI, Elon Musk\u2019s OpenAI rival, is closing on $6B in funding and X, his social network, is already one of its shareholders", "topics": ["Funding", "OpenAI"], "sentiment": "positive"}, {"url": "https://futurism.com/neoscope/startup-uses-ai-edit-human-dna", "summary": "Profluent, a Berkeley-based startup, is integrating generative AI into gene editing to improve disease treatment outcomes. They have launched OpenCRISPR-1, an open-source AI-driven gene editor, and emphasize the need for rigorous preclinical testing to validate the safety and efficacy of AI-assisted gene editing technologies.", "title": "Startup Uses AI to Edit Human DNA", "topics": ["AI in healthcare", "AI safety"], "sentiment": "positive"}, {"url": "https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/", "summary": "Snowflake AI Research has released Arctic, a cost-effective enterprise AI LLM featuring a Dense-MoE Hybrid transformer architecture with 480 billion parameters. Trained for less than $2 million, Arctic excels in tasks like SQL generation and coding. It\u2019s fully open-source under Apache 2.0, providing free access to model weights and code.", "title": "Snowflake releases Arctic, an open LLM for Enterprise AI", "topics": ["Mixture-of-Experts", "AI for coding", "Model release", "Funding"], "sentiment": "positive"}], "guides": [{"url": "https://www.theguardian.com/technology/2024/apr/16/techscape-ai-gadgest-humane-ai-pin-chatgpt", "summary": "The article highlights the leverage of African workers in AI, who are hired at low wages to enhance chatbot communications. Their work imprints AI with a version of African English called \u201cAI- ese,\u201d which can unintentionally cast their dialect as robotic and contribute to linguistic stigmatization.", "title": "How cheap, outsourced labour in Africa is shaping AI English"}, {"url": "https://pub.towardsai.net/some-technical-notes-about-llama-3-042c0b19db14", "summary": "Meta AI\u2019s Llama 3 has cutting-edge features, including a 128K-token tokenizer and grouped query attention. It\u2019s trained on a 15 trillion token multilingual corpus and incorporates Rotary Positional Encoding and Key-Value caching for better inference efficiency.", "title": "Some Technical Notes About Llama 3"}, {"url": "https://huggingface.co/blog/AviSoori1x/seemore-vision-language-model", "summary": "Seemore is a streamlined Vision Language Model (VLM) inspired by Karpathy\u2019s \u201cmakemore,\u201d built using PyTorch. It features a vision transformer for image processing, a multimodal projector to align image and text data, and a decoder-only language model for generating text, aimed at easing the comprehension of multimodal machine learning systems.", "title": "seemore: Implement a Vision Language Model from Scratch"}, {"url": "https://pub.towardsai.net/top-important-computer-vision-papers-for-the-week-from-08-04-to-14-04-84659842a52c", "summary": "The review covers recent computer vision developments from April 2024, touching on image recognition and model optimization. Notable are the uses of interval guidance in diffusion models, 3D scene generation via RealmDreamer, and text-to-image diffusion model refinement based on human preferences.", "title": "Top Important Computer Vision Papers for the Week from 08/04 to 14/04"}, {"url": "https://blog.langchain.dev/graph-based-metadata-filtering-for-improving-vector-search-in-rag-applications/", "summary": "This guide discusses the use of graph-based metadata filtering to improve vector search in Retrieval-Augmented Generation (RAG) applications by integrating LangChain with the Neo4j graph database, enhancing search result accuracy and relevance by narrowing results via attributes such as dates or categories.", "title": "Graph-based metadata filtering for improving vector search in RAG applications"}], "papers": [{"url": "https://arxiv.org/abs/2404.14219", "summary": "Microsoft has launched the Phi-3-mini, a compact language model designed for mobile platforms, boasting 3.8 billion parameters and trained on a dataset of 3.3 trillion tokens. It rivals the performance of larger models like Mixtral 8x7B and GPT-3.5, scoring 69% on the MMLU benchmark and 8.38 on MT-bench. This advancement is attributed to an improved dataset derived from its predecessor, phi-2, which includes a mix of web content and synthetic data.", "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"}, {"url": "https://github.com/jxnl/instructor", "summary": "Instructor is a Python library leveraging Pydantic to enhance language model interactivity by organizing outputs into structured formats, validating responses, managing retries, and supporting streaming.", "title": "jxnl/instructor: structured outputs for llms"}, {"url": "https://github.com/dwzhu-pku/LongEmbed/tree/main", "summary": "This repository introduces methods for extending context windows in embedding models up to 32k without extra training, and presents LongEmbed, a benchmark for evaluating long context retrieval across four tasks derived from long-form QA and summarization.", "title": "dwzhu-pku/LongEmbed: Extending Embedding Models for Long Context Retrieval"}, {"url": "https://arxiv.org/abs/2404.1", "summary": "Researchers identify a key vulnerability in today\u2019s large language models, which is the models\u2019 inability to distinguish between system prompts and potentially malicious user inputs. To combat this, the authors propose an instruction hierarchy to prioritize trusted instructions over others, enhancing the model\u2019s robustness. When tested on GPT-3.5, this approach significantly improved resistance to various attacks without noticeably impairing the model\u2019s performance.", "title": "The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions "}, {"url": "https://arxiv.org/abs/2404.12253", "summary": "Recent research introduces AlphaLLM, a methodology aimed at self-improving LLMs by integrating Monte Carlo Tree Search (MCTS) to facilitate a self-enhancement loop. This approach addresses complex reasoning and planning challenges by enabling LLMs to self-correct and self-learn, potentially advancing their abilities beyond the limits imposed by data availability and quality.", "title": "Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing"}], "datetime": "2024-04-29"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-april-23rd-2024-6ccdc6449e25", "news": [{"url": "https://ai.meta.com/blog/meta-llama-3/", "summary": "Meta has introduced Meta Llama 3, a state-of-the-art open-source large language model (LLM) with versions up to 70 billion parameters, providing enhanced reasoning and multilingual capabilities. The current best models are pretrained and instruction-fine-tuned at both 8B and 70B scales. Additionally, even larger models exceeding 400 billion parameters are in development, promising to push the boundaries further upon their release in the coming months.", "title": "Introducing Meta Llama 3: The most capable openly available LLM to date", "topics": ["Model release", "Meta"], "sentiment": "positive"}, {"url": "https://mistral.ai/news/mixtral-8x22b/", "summary": "Mistral unveiled Mixtral 8x22B, an efficient sparse Mixture-of-Experts model with 39B active out of 141B total parameters, specializing in multilingual communication, coding, mathematics, and excelling in reasoning and knowledge tasks. The model boasts a 64K token context window, is compatible with multiple platforms, and is available under the open-source Apache 2.0 license.", "title": "Mistral released Mixtral 8x22B", "topics": ["Mixture-of-Experts", "AI for coding", "Model release", "Mistral"], "sentiment": "positive"}, {"url": "https://futurism.com/the-byte/openai-fires-researchers-leaks", "summary": "OpenAI has terminated two members of its safety and AI reasoning teams following internal leaks, showcasing the ongoing challenge of balancing transparency with security at innovative AI organizations. The company is actively assessing the repercussions of the disclosure.", "title": "OpenAI Fires Researchers for Leaking Information", "topics": ["AI safety", "OpenAI"], "sentiment": "negative"}, {"url": "https://qz.com/google-ai-chip-nvidia-axion-arm-microsoft-1851397201", "summary": "Google has unveiled the Cloud TPU v5p, an AI chip that delivers nearly triple the training speed of its predecessor, the TPU v4, reinforcing its position in AI services and hardware. At the Google Cloud Next event, CEO Pichai highlighted the company\u2019s AI advancements and collaborations, including the use of the A3 supercomputer and Blackwell chips in the AI Hypercomputer. Additionally, Google introduced the Google Axion CPU, an Arm-based processor that competes with similar offerings from Microsoft and Amazon, boasting a 30% performance improvement and better energy efficiency.", "title": "Google\u2019s new chips look to challenge Nvidia, Microsoft and Amazon", "topics": ["Amazon", "NVIDIA", "AI Chips and GPUs", "Google", "Microsoft"], "sentiment": "positive"}, {"url": "https://www.newsbytesapp.com/news/science/openai-refutes-allegations-by-early-investor-elon-musk/story", "summary": "OpenAI is contesting a lawsuit filed by Elon Musk, who accuses the organization of straying from its original mission upon its commercial endeavors with Microsoft. OpenAI disputes the claims, suggesting Musk\u2019s competing AI interests might influence his action and argues that no concrete commitment was made to avoid commercialization, challenging the basis of Musk\u2019s lawsuit for lack of a definitive agreement.", "title": "OpenAI denies Elon Musk\u2019s lawsuit allegations, seeks dismissal in court", "topics": ["AI regulation", "Microsoft", "OpenAI"], "sentiment": "negative"}], "guides": [{"url": "https://huggingface.co/blog/llama3", "summary": "Meta has launched Llama 3, the newest addition to its Llama series, accessible on Hugging Face. Available in two versions, 8B and 70B, each with base and instruction-tuned variants, it features enhanced multilingual tokenization and is designed for easy deployment on platforms like Google Cloud and Amazon SageMaker.", "title": "Welcome Llama 3 \u2014 Meta\u2019s new open LLM"}, {"url": "https://aiindex.stanford.edu/report/", "summary": "The 2024 AI Index Report from Stanford presents key trends in AI, including technical progress, rising costs of advanced models, and AI-enhanced workforce productivity. It also notes the uptick in AI-focused regulations and investments, particularly in generative AI. This is set against a backdrop of increased public consciousness and concern regarding AI\u2019s societal implications.", "title": "Stanford AI Index Report 2024"}, {"url": "https://weightythoughts.com/p/cuda-is-still-a-giant-moat-for-nvidia", "summary": "NVIDIA maintains its leading position in AI through the synergy of its CUDA software ecosystem and NVLink interconnects, which underpin its hardware performance, making it challenging for competitors like AMD to rival NVIDIA\u2019s proficiency in AI workloads.", "title": "CUDA is Still a Giant Moat for NVIDIA"}, {"url": "https://venturebeat.com/ai/openai-or-diy-unveiling-the-true-cost-of-self-hosting-llms/", "summary": "The article examines the financial considerations of leveraging OpenAI\u2019s API versus self-hosting LLMs. It highlights the trade-off between the greater control over data achieved through self-hosting, which comes with higher costs for fine-tuning and maintenance, and the potential cost savings of OpenAI\u2019s usage-based pricing model. The choice between them hinges on the particular requirements and demand of a given business.", "title": "OpenAI or DIY? Unveiling the true cost of self-hosting LLMs"}, {"url": "https://generatingconversation.substack.com/p/you-cant-build-a-moat-with-ai", "summary": "Success in AI applications increasingly depends on leveraging unique, customer-specific data for training, rather than just innovations in models like LLMs. Data engineering is key to creating competitive AI solutions.", "title": "You can\u2019t build a moat with AI"}], "papers": [{"url": "https://www.microsoft.com/en-us/research/project/vasa-1/", "summary": "Microsoft has developed VASA, a framework that can create realistic talking faces with expressive visual affective skills from a single image and audio input, featuring synchronised lip syncing and dynamic facial expressions for enhanced authenticity.", "title": "VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time"}, {"url": "https://arxiv.org/abs/2404.08801", "summary": "Megalodon, a new model architecture designed for efficient sequence modeling with unlimited context length, is introduced to address the scalability limitations of Transformers due to their quadratic complexity and poor performance with long sequences. Building upon the Mega architecture, it incorporates advancements such as complex exponential moving average (CEMA), timestep normalization, a normalized attention mechanism, aiming to outperform both classic Transformers and sub-quadratic alternatives like linear attention and state space models in pretraining efficiency and downstream task accuracy.", "title": "Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length"}, {"url": "https://arxiv.org/abs/2404.09937", "summary": "Recent studies have found a linear correlation between language models\u2019 (LLMs) performance on intelligence benchmarks and their text compression abilities, suggesting that compression efficiency could serve as an effective, unsupervised metric for evaluating LLM capabilities.", "title": "Compression Represents Intelligence Linearly"}, {"url": "https://arxiv.org/abs/2404.09656", "summary": "Researchers address instability in LLM alignment methods such as RLHF and DPO by proposing Trust Region DPO (TR-DPO), which actively updates the reference policy during training. This method moves beyond DPO\u2019s implicit limitations, offering improvements demonstrated on the Anthropic HH and TLDR datasets, with TR-DPO outperforming DPO by up to 19%, as per GPT-4 automatic evaluations.", "title": "Learn Your Reference Model for Real Good Alignment"}, {"url": "https://babylm.github.io/", "summary": "The BabyLM Challenge 2024 focuses on improving language model pretraining under data limitations analogous to human language learning. It provides fresh datasets, such as a 50M word multimodal corpus, and allows participants to use tailored datasets within specific word counts.", "title": "BabyLM Challenge"}], "datetime": "2024-04-23"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-april-15th-2024-f4e9c3af394f", "news": [{"url": "https://the-decoder.com/metas-open-source-gpt-4-competitor-llama-3-is-coming-soon/", "summary": "Meta is set to release Llama 3, an AI assistant intended to outperform its predecessors and compete with OpenAI\u2019s GPT-4. It will debut with two preliminary versions before launching a comprehensive multimodal iteration in the summer.", "title": "Meta\u2019s open source GPT-4 competitor Llama 3 is coming soon", "topics": ["LLaMA", "Meta", "GPT-4 and GPT-4 turbo", "OpenAI", "Multimodal AI (image, video, audio)"], "sentiment": "positive"}, {"url": "https://developers.googleblog.com/2024/04/gemini-15-pro-in-public-preview-with-new-features.html", "summary": "Gemini 1.5 Pro has launched globally, offering cutting-edge native audio understanding and upgraded features such as a File API, system instructions, JSON mode for developers, along with advanced audio/video modalities, including video quiz capabilities. The update also introduces a highly performant text embedding model.", "title": "Gemini 1.5 Pro Now Available in 180+ Countries; With Native Audio Understanding, System Instructions, JSON Mode and More", "topics": ["Multimodal AI (image, video, audio)"], "sentiment": "positive"}, {"url": "https://platform.openai.com/docs/models/continuous-model-upgrades", "summary": "The new GPT-4 Turbo, now with vision capabilities, supports vision requests via JSON mode and function calls, with knowledge updated until December 2023.", "title": "GPT4 Turbo has been upgraded and is out of preview", "topics": ["GPT-4 and GPT-4 turbo", "OpenAI", "Multimodal AI (image, video, audio)"], "sentiment": "positive"}, {"url": "https://www.maginative.com/article/x-ai-unveils-its-first-multimodal-model-grok-1-5-vision/", "summary": "x.AI, launched by Elon Musk, introduces Grok-1.5V, an advanced multimodal AI model with enhanced capabilities for analyzing visual data, including text, charts, and images.", "title": "x.AI Unveils it\u2019s First Multimodal model, Grok-1.5 Vision", "topics": ["Multimodal AI (image, video, audio)", "Grok"], "sentiment": "positive"}, {"url": "https://www.theverge.com/2024/4/11/24127579/tiktok-ai-virtual-influencers-advertising", "summary": "TikTok is investigating the integration of AI-powered avatars to deliver more personalized and engaging advertising experiences by aligning ad content with user interests.", "title": "TikTok may add AI avatars that can make ads", "topics": ["Multimodal AI (image, video, audio)"], "sentiment": "positive"}], "guides": [{"url": "https://artificialanalysis.ai/speech-to-text", "summary": "Artificial Analysis has evaluated multiple speech-to-text models and APIs from providers like OpenAI, Azure, Amazon Transcribe, and Google, focusing on metrics such as word error rate, performance speed, and pricing.", "title": "Speech to Text Providers Leaderboard & Comparison"}, {"url": "https://huggingface.co/blog/vlms", "summary": "Vision language models (VLMs) are multimodal AI systems capable of interpreting images and text, utilized for tasks like image captioning and visual questioning. They demonstrate strong zero-shot learning and can handle various image formats. Examples include LLaVA 1.6 and Yi-VL-34B.", "title": "Vision Language Models Explained"}, {"url": "https://www.topbots.com/ai-to-automate-document-processing/", "summary": "Advancements in AI have evolved from traditional OCR and basic NLP to sophisticated IDP and Large Language Models, enhancing the interpretation and handling of elaborate document configurations.", "title": "How To Use AI To Automate Document Processing"}, {"url": "https://www.rainforestqa.com/blog/building-reliable-systems-out-of-unreliable-agents", "summary": "The article presents methods for developing dependable AI systems by employing unreliable agents. It details steps involving prompt engineering, performance optimization, eval systems, data-driven fine-tuning, and Retrieval Augmented Generation (RAG), with a notable strategy of utilizing complementary agents to boost system dependability.", "title": "Building reliable systems out of unreliable agents"}, {"url": "https://www.anthropic.com/news/measuring-model-persuasiveness", "summary": "New research demonstrates that the persuasiveness of Anthropic AI models increases with each generation, with the latest model, Claude 3 Opus, matching the convincingness of human-generated arguments.", "title": "Measuring the Persuasiveness of Language Models"}], "papers": [{"url": "https://github.com/karpathy/llm.c", "summary": "Andrej Karpathy\u2019s project focuses on developing a minimalist GPT-2 training framework using C/CUDA to eliminate heavy dependencies like PyTorch or cPython. The goal is to recreate the PyTorch model within approximately 1,000 lines of code while improving performance with direct CUDA integration and tailored CPU optimizations.", "title": "karpathy/llm.c: LLM training in simple, raw C/CUDA"}, {"url": "https://arxiv.org/abs/2404.05719", "summary": "Apple researchers have developed Ferret-UI, an advanced multimodal large language model (MLLM) specifically designed for improved interpretation and interaction with mobile user interface (UI) screens.", "title": "Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs"}, {"url": "https://arxiv.org/abs/2404.06654", "summary": "The needle-in-a-haystack (NIAH) test has been used to assess long-context language models by measuring their ability to find specific information within extensive texts. Recognizing the limitations of NIAH\u2019s assessment of deep understanding, researchers have developed the RULER benchmark. This new benchmark offers more intricate evaluations by allowing customization of sequence lengths and task complexities, introducing different needle types and quantities, and adding more challenging task categories such as multi-hop tracing and aggregation.", "title": "RULER: What\u2019s the Real Context Size of Your Long-Context Language Models?"}, {"url": "https://arxiv.org/abs/2404.07143", "summary": "The work presents a method for scaling LLMs to handle infinitely long inputs while maintaining bounded memory and computational requirements. It introduces Infini-attention, an attention mechanism integrating compressive memory with both local masked attention and long-term linear attention within a Transformer block.", "title": "Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention"}, {"url": "https://arxiv.org/abs/2404.07965", "summary": "The authors analyze token importance in language model training, uncovering varying loss patterns among tokens. This research leads to the development of RHO-1, a new language model that employs Selective Language Modeling (SLM) to focus on training with tokens that are more beneficial for the model, rather than treating all tokens with equal importance.", "title": "Rho-1: Not All Tokens Are What You Need"}], "datetime": "2024-04-15"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-april-8th-2024-7f2a8ea2c017", "news": [{"url": "https://stability.ai/news/stable-audio-2-0", "summary": "Stable Audio 2.0 introduces significant advancements in music generation AI, offering audio-to-audio conversion through natural language prompts and expanding creative possibilities with sound effects and improved style transfer. The latest version now supports the generation of high-quality (44.1 kHz) structured songs up to three minutes in length from concise prompts.", "title": "Introducing Stable Audio 2.0 from Stability AI", "topics": ["Multimodal AI (image, video, audio)", "Stability AI"], "sentiment": "positive"}, {"url": "https://openai.com/blog/start-using-chatgpt-instantly", "summary": "ChatGPT is now instantly accessible for new users, offering AI interaction without requiring initial account creation.", "title": "Start using ChatGPT instantly", "topics": ["ChatGPT", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.techradar.com/computing/artificial-intelligence/openais-sora-just-made-its-first-music-video-and-its-like-a-psychedelic-trip", "summary": "OpenAI has showcased the capabilities of its text-to-video engine, Sora, by creating a music video for August Kamp\u2019s song \u201cWorldweight\u201d entirely through the engine\u2019s capabilities.", "title": "OpenAI\u2019s Sora just made its first music video and it\u2019s like a psychedelic trip", "topics": ["Multimodal AI (image, video, audio)", "OpenAI"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2024/04/04/openai-expands-its-custom-model-training-program/", "summary": "OpenAI is broadening its Custom Model initiative to support various businesses in developing AI models tailored to unique industry needs, with a focus on enhanced fine-tuning via advanced hyperparameter optimization and scalable methods, as announced at DevDay.", "title": "OpenAI expands its custom model training program", "topics": ["OpenAI"], "sentiment": "positive"}, {"url": "https://www.businesswire.com/news/home/20240402148086/en/Lambda-Announces-500M-GPU-Backed-Facility-to-Expand-Cloud-for-AI", "summary": "Lambda has successfully secured $500 million in funding to enhance its AI-oriented cloud services, powered by NVIDIA GPUs, following a Series C investment round.", "title": "Lambda Announces $500M GPU-Backed Facility to Expand Cloud for AI", "topics": ["Funding", "NVIDIA", "AI Chips and GPUs"], "sentiment": "positive"}, {"url": "https://www.teslarati.com/tesla-increasing-ai-team-compensation-elon-musk/", "summary": "Tesla, under Elon Musk, is increasing AI team compensation to retain and attract experts amidst intensified industry competition, as highlighted by engineer Ethan Knight\u2019s transition to Musk\u2019s xAI project following OpenAI\u2019s recruitment attempts. This strategy aims to maintain commitment to Tesla\u2019s critical autonomous driving and humanoid robotics initiatives.", "title": "Tesla increasing compensation for AI team: Elon Musk", "topics": ["Robotics", "Funding"], "sentiment": "positive"}], "guides": [{"url": "https://web.stanford.edu/class/cs25/", "summary": "Stanford University\u2019s popular seminar course, CS25, focusing on Transformer models in artificial intelligence, is now open to the public via professional livestreaming. The Spring 2024 semester will feature enhancements like a larger venue, social events, and networking opportunities. Attendees can expect weekly sessions with industry leaders from organizations like OpenAI and Google, covering LLM advancements applied to fields including digital art and neuroscience.", "title": "Stanford CS 25 Transformers Course (Open to Everybody)"}, {"url": "https://docs.anthropic.com/claude/docs/tool-use", "summary": "The public beta phase for Claude 3 tool usage has begun, offering enhanced interaction with external client-side tools and the ability for customization to expand its task capabilities.", "title": "Tool use (function calling) with Claude"}, {"url": "https://huggingface.co/blog/lbourdois/get-on-the-ssm-train", "summary": "State Space Models (SSM) are increasingly influential in deep learning for dynamic systems, gaining attention with the \u201cEfficiently Modeling Long Sequences with Structured State Spaces\u201d paper in October 2021. The focus here is on the S4 model, an essential theoretical framework that, while not widely used in practical applications, underscores the evolution of alternatives to transformer architectures in the field of artificial intelligence.", "title": "Introduction to State Space Models (SSM)"}, {"url": "https://salesforceventures.com/perspectives/ai-infrastructure-explained/", "summary": "The article outlines the importance of AI infrastructure in advancing AI technology, focusing on GPUs for efficient parallel computing, the necessary software ecosystem, and the variety of GPU cloud providers. It categorizes cloud providers into Hyperscalers, Specialized Cloud Providers, and Inference-as-a-Service/Serverless Endpoints to cater to diverse AI applications.", "title": "AI Infrastructure Explained"}], "papers": [{"url": "https://arxiv.org/abs/2404.02258", "summary": "Researchers have developed a method allowing transformer-based language models to dynamically distribute computational resources (FLOPs) across different positions in a sequence. By introducing a top-k routing mechanism that limits the number of tokens involved in self-attention and MLP operations at each layer, the models efficiently manage a preset compute budget. This approach results in models that not only match the performance of traditional models using similar amounts of compute and training time but also significantly reduce FLOPs per forward pass, leading to over a 50% increase in speed during post-training sampling.", "title": "Deepmind Mixture-of-Depths: Speeding Up Models By 50%"}, {"url": "https://arxiv.org/abs/2404.02060", "summary": "A new study introduces LongICLBench, a benchmark designed to test large language models (LLMs) on long in-context learning and extreme-label classification tasks with label ranges from 28 to 174. The study uses six datasets with input lengths between 2K to 50K tokens, emphasizing the model\u2019s need to comprehend extensive inputs and vast label spaces for accurate predictions. Evaluations of 13 LLMs indicated poor performance on complex tasks, especially one with 174 labels, with almost negligible understanding. Models also showed a bias towards labels appearing later in the sequence, highlighting deficiencies in reasoning over long, detailed contexts and suggesting a significant room for improvement in LLM capabilities.", "title": "Long-context LLMs Struggle with Long In-context Learning"}, {"url": "https://arxiv.org/abs/2404.03626", "summary": "This paper investigates training large language models (LLMs) using text that has been highly compressed by neural text compressors, aiming to improve training and serving efficiency, as well as manage long text sequences better. Although the method results in higher perplexity compared to traditional subword tokenizers, it benefits from shorter sequence lengths, leading to fewer generation steps and reduced latency.", "title": "Training LLMs over Neurally Compressed Text"}, {"url": "https://www.anthropic.com/research/many-shot-jailbreaking", "summary": "A study unveils a technique called \u201cmany-shot jailbreaking\u201d highlighting how crafting multiple deceptive dialogues can trick large language models into providing banned responses, exposing a link between this vulnerability and the models\u2019 in-context learning capabilities.", "title": "Many-shot jailbreaking"}, {"url": "https://arxiv.org/abs/2404.01744", "summary": "New research introduces an on-device language model with 2 billion parameters, outperforming GPT-4 in function-calling tasks in terms of accuracy and latency, addressing privacy and cost concerns of cloud-based models.", "title": "Octopus v2: On-device language model for super agent"}], "datetime": "2024-04-08"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-april-2nd-2024-4c2c8db8cdd0", "news": [{"url": "https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard", "summary": "Claude 3 Opus outperforms GPT-4 Turbo and Gemini Pro, while Claude 3 Haiku surpasses GPT-4 and Mistral Large in efficiency and cost-effectiveness on the LMSys Chatbot Arena leaderboards.", "title": "Claude 3 Opus officially beats GPT-4 Turbo on LMSys Chatbot Arena leaderboards", "topics": ["Claude", "Google Gemini", "GPT-4 and GPT-4 turbo"], "sentiment": "positive"}, {"url": "https://www.tomshardware.com/tech-industry/artificial-intelligence/openai-and-microsoft-reportedly-planning-dollar100-billion-datacenter-project-for-an-ai-supercomputer", "summary": "Microsoft and OpenAI have announced a partnership to construct \u201cStargate,\u201d an advanced AI supercomputer in the U.S., featuring millions of GPUs. The project, which may exceed $115 billion, represents a major commitment to expanding datacenter capabilities to advance AI research and development.", "title": "OpenAI and Microsoft reportedly planning $100 billion datacenter project for an AI supercomputer", "topics": ["Funding", "AI Chips and GPUs", "Microsoft", "OpenAI"], "sentiment": "positive"}, {"url": "https://x.ai/blog/grok-1.5", "summary": "xAI has announced Grok-1.5, an update that expands its token handling capacity to 128K, enabling it to process text documents up to 16 times longer than its previous version. This enhancement significantly boosts Grok-1.5\u2019s performance in analyzing extensive texts and allows it to efficiently locate precise information within them, as demonstrated by its success in the Needle In A Haystack benchmark.", "title": "xAI announces Grok-1.5", "topics": ["Grok"], "sentiment": "positive"}, {"url": "https://www.aboutamazon.com/news/company-news/amazon-anthropic-ai-investment", "summary": "Amazon has invested $4 billion in AI company Anthropic to further develop AI technologies. Anthropic leverages Amazon Web Services (AWS) Trainium and Inferentia chips for enhancing their AI models. Notably, Anthropic\u2019s Claude 3 models have been incorporated into Amazon Bedrock by AWS.", "title": "Amazon and Anthropic deepen their shared commitment to advancing generative AI", "topics": ["Amazon", "Funding", "AI Chips and GPUs", "Claude", "Anthropic"], "sentiment": "positive"}, {"url": "https://bgr.com/tech/apple-says-its-latest-ai-model-realm-is-even-better-than-openais-gpt4/", "summary": "Apple has announced ReALM, an LLM claimed to outperform OpenAI\u2019s GPT-4 in contextual understanding, suitable for responding accurately to queries regardless of whether it\u2019s interacting with on-screen content or operating in the background.", "title": "Apple says its latest AI model ReALM is even better than OpenAI\u2019s GPT4", "topics": ["Apple", "GPT-4 and GPT-4 turbo", "OpenAI"], "sentiment": "positive"}, {"url": "https://the-decoder.com/amazons-ai-team-faces-pressure-to-outperform-anthropics-claude-models-by-mid-year/", "summary": "Amazon has ramped up its AI game by finalizing a $2.75 billion investment in AI startup Anthropic, hitting a total investment milestone of $4 billion, as it aims to stay competitive with Microsoft\u2019s AI progress in the cloud industry.", "title": "Amazon\u2019s AI team faces pressure to outperform Anthropic\u2019s Claude models by mid-year", "topics": ["Amazon", "Funding", "Claude", "Anthropic", "Microsoft"], "sentiment": "positive"}, {"url": "https://www.theverge.com/2024/3/29/24115701/openai-voice-generation-ai-model", "summary": "OpenAI has introduced a Voice Engine capable of generating synthetic voices using 15-second audio samples, featuring multilingual text-to-speech capabilities suitable for various industries, with an emphasis on adherence to consent guidelines in its rollout.", "title": "OpenAI\u2019s voice cloning AI model only needs a 15-second sample to work", "topics": ["Text-to-speech", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.ai21.com/jamba", "summary": "AI21Labs has developed Jamba, a hybrid AI model that merges Structured State Space (SSM) with the Transformer architecture, to improve efficiency and versatility in processing complex data sequences. Jamba aims to overcome the limitations of both the classic Transformer and standalone SSM models by integrating their strengths.", "title": "Introducing Jamba", "topics": [], "sentiment": "positive"}, {"url": "https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm", "summary": "Databricks has unveiled DBRX, a new open-source large language model (LLM) that surpasses GPT-3.5 in programming and general tasks, and is competitive with Gemini 1.0 Pro. DBRX features a mixture-of-experts architecture with a massive 132 billion parameters, though only 36 billion are active for any given input. Both DBRX Base and DBRX Instruct models are openly accessible on Hugging Face.", "title": "Introducing DBRX: A New State-of-the-Art Open LLM", "topics": ["Hugging Face", "Mixture-of-Experts", "AI for coding", "Model release", "GPT-3, GPT-3.5, and GPT-3.5 turbo"], "sentiment": "positive"}], "guides": [{"url": "https://mobiusml.github.io/1bit_blog/", "summary": "Recent studies like BitNet and 1.58 bit have highlighted the potential of extreme low-bit quantization in machine learning, showing that it allows matrix multiplication with quantized weights to be performed without actual multiplication operations, which may significantly enhance the computational efficiency of sizable machine learning models.", "title": "Towards 1-bit Machine Learning Models"}, {"url": "https://www.topbots.com/ai-semiconductors-industry-overview/", "summary": "In the critical semiconductor landscape for electronics and AI, TSMC leads global production, while NVIDIA is notable for its advanced AI chip designs. The supply chain is notably concentrated, with key players like ASML, TSMC, and Samsung being indispensable due to their specialized manufacturing capabilities. TSMC manufactures sophisticated components such as NVIDIA\u2019s H100 GPUs but faces supply chain risks due to geopolitical tensions, prompting investment in diversifying production, exemplified by TSMC\u2019s $40 billion investment in U.S. manufacturing facilities.", "title": "Semiconductor Titans: Inside the World of AI Chip Manufacturing and Design"}, {"url": "https://huggingface.co/blog/abhishek/autotrain-mixtral-dgx-cloud-local", "summary": "The guide provides a walkthrough on how to finetune the Mixtral 8x7B language model using AutoTrain, highlighting a user-friendly interface and minimal coding requirements for both local and cloud environments. It outlines steps for setup, parameter tuning, and execution to easily train the model on custom datasets for quick customization and deployment.", "title": "Finetune Mixtral 8x7B with AutoTrain"}, {"url": "https://pub.towardsai.net/using-claude-3-to-transform-a-video-tutorial-in-a-blog-post-d2c1e04e7a7b", "summary": "This guide shows how to use Claude 3 to transform a two-hour tutorial into structured blog chapters.", "title": "Using Claude 3 to Transform a Video Tutorial Into a Blog Post"}, {"url": "https://pub.towardsai.net/building-a-multilingual-ner-app-with-huggingface-4fe0c6ad858f", "summary": "This guide outlines the creation of a multilingual NER application leveraging HuggingFace, detailing the process from data loading and training with RoBERTa-base, to app development with Gradio and performance tracking via the Comet library.", "title": "Build A Multilingual NER App with HuggingFace, RoBERTa, and Comet"}], "papers": [{"url": "https://arxiv.org/abs/2403.18802", "summary": "DeepMind has developed a system called the Search-Augmented Factuality Evaluator (SAFE), which leverages LLM agents to assess the factuality of long-form content. SAFE decomposes content into discrete facts and employs a multi-step process that includes querying Google Search to verify facts. In evaluations, SAFE aligned with human annotations 72% of the time and outperformed humans in 76% of cases where there was initial disagreement, while providing a cost-saving factor of over 20 times compared to human annotators.", "title": "Long-form factuality in large language models"}, {"url": "https://arxiv.org/abs/2403.19887", "summary": "Jamba is an LLM combining Transformer and Mamba architectures through a mixture-of-experts approach, optimized for performance on large-scale language tasks with extended context lengths. It can operate on 80GB GPUs.", "title": "Jamba: A Hybrid Transformer-Mamba Language Model"}, {"url": "https://arxiv.org/abs/2403.19851", "summary": "This study investigates memory localization within language models, revealing that although memorization is distributed throughout various layers, the gradients corresponding to memorized content exhibit unique spatial patterns. Additionally, it is possible to selectively unlearn these memorized examples through targeted fine-tuning of weights with high gradients.", "title": "Localizing Paragraph Memorization in Language Models"}, {"url": "https://arxiv.org/abs/2403.20327", "summary": "Gecko is a novel text embedding model that enhances retrieval capabilities by distilling knowledge from LLMs. The method involves a two-stage distillation process that starts with creating synthetic query-passage pairs using an LLM and then refining this data by utilizing the LLM to identify the most relevant passages and challenging negative examples.", "title": "Gecko: Versatile Text Embeddings Distilled from Large Language Models"}, {"url": "https://github.com/mshumer/gpt-investor", "summary": "gpt-investor is an LLM-based agent tailored for the investment sector, offering analytical insights on stocks within specific industries. It leverages machine learning to parse financial data, news, and analyst ratings, conducting sentiment analysis and competitive ranking.", "title": "mshumer/gpt-investor"}], "datetime": "2024-04-02"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-march-25th-2024-ac1e2d795353", "news": [{"url": "https://www.theverge.com/2024/3/23/24109511/stability-ai-ceo-emad-mostaque-resignation-decentralized-ai", "summary": "Emad Mostaque has stepped down as CEO of Stability AI to concentrate on decentralized AI developments. The company will be temporarily co-led by COO Shan Shan Wong and CTO Christian Laforte, maintaining its generative AI advancements. This leadership change occurs amid a notable industry trend of talent movement, highlighted by Microsoft\u2019s acquisition of Inflection AI\u2019s team and Google DeepMind\u2019s co-founder Mustafa Suleyman.", "title": "Stability AI CEO resigns to \u201cpursue decentralized AI\u201d", "topics": ["Stability AI", "DeepMind", "Google", "Microsoft"], "sentiment": "positive"}, {"url": "https://blogs.nvidia.com/blog/2024-gtc-keynote/", "summary": "NVIDIA CEO Jensen Huang announced the NVIDIA Blackwell computing platform at the GTC conference, aimed at advancing generative AI with superior training and inference capabilities. The platform includes enhanced interconnects for better performance and scalability. NVIDIA also launched NIM microservices for tailored AI deployment and Omniverse Cloud APIs for sophisticated simulation, signaling a transformative impact on sectors like healthcare and robotics.", "title": "\u2018We Created a Processor for the Generative AI Era,\u2019 NVIDIA CEO Says", "topics": ["AI in healthcare", "Robotics", "NVIDIA", "AI Chips and GPUs"], "sentiment": "positive"}, {"url": "https://www.businessinsider.com/openai-launch-better-gpt-5-chatbot-2024-3", "summary": "OpenAI is preparing to release GPT-5 around mid-year, offering significant improvements over GPT-4, particularly in enhanced performance for business applications. Although the launch date is not fixed due to continued training and safety evaluations, preliminary demonstrations to enterprise clients suggest new features and capabilities, raising anticipation for GPT-5\u2019s impact on the generative AI landscape.", "title": "OpenAI is expected to release a \u2018materially better\u2019 GPT-5 for its chatbot mid-year, sources say", "topics": ["GPT-5", "OpenAI"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2024/03/19/after-raising-1-3b-inflection-got-eaten-alive-by-its-biggest-investor-microsoft/", "summary": "Inflection, previously supported by $1.3B in funding, has pivoted from its original AI project \u2018Pi\u2019 due to competition lag and integrated its top staff into Microsoft. Notably, Suleyman and Simonyan, along with core team members, have assumed leadership positions within Microsoft\u2019s AI division, indicating a strategic reorientation towards custom generative AI solutions for enterprise clients. This move exemplifies the competitive landscape in conversational AI, dominated by industry behemoths like Microsoft, which continually adapt through significant investments and acquisitions.", "title": "After raising $1.3B, Inflection is eaten alive by its biggest investor, Microsoft", "topics": ["Funding", "Microsoft"], "sentiment": "negative"}, {"url": "https://www.theverge.com/2024/3/21/24107499/neuralink-human-trial-chess-video-brain-computer-interface", "summary": "Neuralink demonstrated a significant advancement in brain-computer interfaces by presenting a paralyzed human patient who can play chess and operate a computer cursor with a brain implant. This milestone follows FDA approval for clinical trials and underscores the technology\u2019s potential to revolutionize assistance for paralyzed individuals and perhaps expand abilities for the non-disabled.", "title": "Neuralink video shows patient using brain implant to play chess on laptop", "topics": ["AI in healthcare", "Neuralink"], "sentiment": "positive"}, {"url": "https://stability.ai/news/introducing-stable-video-3d", "summary": "Stability AI has introduced Stable Video 3D (SV3D), a new generative model enhancing 3D tech with better quality and consistency. SV3D offers two versions: SV3D_u for single-image-based orbital videos without camera paths, and SV3D_p for more advanced 3D video creation using specified camera trajectories. For commercial use, it requires a Stability AI Membership, while non-commercial users can access the model weights through Hugging Face and consult the accompanying research paper.", "title": "Introducing Stable Video 3D: Quality Novel View Synthesis and 3D Generation from Single Images", "topics": ["AI for images", "Multimodal AI (image, video, audio)", "Stability AI"], "sentiment": "positive"}, {"url": "https://the-decoder.com/apple-reportedly-in-talks-with-google-to-license-googles-gemini-ai-for-iphones/", "summary": "Apple is currently discussing the integration of Google\u2019s Gemini AI into iPhone functionalities, furthering their collaboration beyond Google\u2019s default search engine arrangement. Additionally, Apple plans to incorporate sophisticated AI features, such as image and text generation, into the forthcoming iOS 18 update and is exploring partnerships with prominent AI entities, including OpenAI and Google\u2019s Gemini AI, to enhance their offering.", "title": "Apple reportedly in talks with Google to license Google\u2019s Gemini AI for iPhones", "topics": ["Apple", "Google Gemini", "Google", "OpenAI"], "sentiment": "positive"}], "guides": [{"url": "https://docs.anthropic.com/claude/prompt-library", "summary": "The Anthropic Prompt Library provides a suite of task-specific prompts aimed at enhancing performance in areas such as business, personal development, and user-generated content. It supports a diversified set of activities including game development, corporate analysis, web design, coding, and creative storytelling.", "title": "The Anthropic prompt library"}, {"url": "https://hbr.org/2024/03/how-people-are-really-using-genai", "summary": "Generative AI, particularly models like ChatGPT, has gained mainstream attention with broad applicability evidenced by over 100 real-world use cases found through extensive research. Despite its wide user base and potential economic significance, adoption is limited due to accuracy concerns, potential corporate misuse, and regulatory challenges. Nevertheless, the technology shows promise in enhancing productivity, creativity, and problem-solving across professional and personal domains.", "title": "How People Are Really Using GenAI"}, {"url": "https://huggingface.co/blog/cosmopedia", "summary": "Cosmopedia is a substantial open-sourced synthetic dataset designed to facilitate the pre-training of large language models similar to Phi-1.5. Generated with Mixtral-8x7B-Instruct-v0.1, it comprises over 30 million files and 25 billion tokens. The dataset emphasizes diversity and quality, achieved through comprehensive prompt engineering and leveraging web data. It marks a paradigm shift from using human annotators to GPT models for data creation while also addressing the challenge of data hallucinations and quality control.", "title": "Cosmopedia: how to create large-scale synthetic data for pre-training Large Language Models"}, {"url": "https://zeux.io/2024/03/15/llm-inference-sol/", "summary": "The article presents \u201ccalm,\u201d a streamlined CUDA solution designed for rapid inference in LLMs, emphasizing the \u201cspeed of light\u201d theoretical maximum for LLM inference. It highlights LLMs\u2019 reliance on sequential token generation, constrained by memory bandwidth rather than computational power with current CPUs and GPUs. The piece underscores the necessity for high-quality software and hardware optimized for maximum memory bandwidth utilization to attain the theoretical limits of inference speed.", "title": "LLM inference speed of light"}, {"url": "https://github.com/alasdairforsythe/tokenmonster/blob/main/benchmark/pretrain.md", "summary": "A study examined the impact of vocabulary/tokenization choices on the performance of language models by pre-training 16 models with various tokenizers. Key findings include the identification of 32,000 as the optimal vocabulary size, and the observation that while simpler vocabularies converge more quickly, they do not guarantee superior results upon convergence.", "title": "Pretraining 16 language models on different tokenizers"}], "papers": [{"url": "https://arxiv.org/abs/2403.13187", "summary": "The paper presents an evolutionary algorithm designed to automate the combination of open-source models into sophisticated foundation models, eliminating the reliance on human expertise and large-scale resources. The approach optimally adjusts parameters and data flow, resulting in the creation of a high-performing Japanese Language LLM with mathematical capabilities and a culturally-sensitive Visual Language Model (VLM), both of which set new benchmarks in their respective areas, showcasing the promise of automated techniques in the development of foundation models.", "title": "Evolutionary Optimization of Model Merging Recipes"}, {"url": "https://arxiv.org/abs/2403.10131", "summary": "RAFT (Retrieval Augmented FineTuning) is introduced as a post-training method that improves LLMs for domain-specific tasks by training them to selectively leverage relevant documents, enhancing information citing and reasoning in \u201copen-book\u201d scenarios. Its effectiveness is validated on datasets like PubMed, HotpotQA, and Gorilla, improving performance in Retrieval Augmented Generation (RAG) tasks.", "title": "RAFT: Adapting Language Model to Domain Specific RAG"}, {"url": "https://arxiv.org/abs/2403.13248", "summary": "Mora is a new open-source, multi-agent video generation framework introduced to provide an alternative to OpenAI\u2019s proprietary Sora model. It supports various tasks like text-to-video, image-to-video conversion, video extension, editing, and digital world simulation with performance close to Sora in certain areas, though it does not yet match Sora\u2019s overall capabilities.", "title": "Mora: Enabling Generalist Video Generation via A Multi-Agent Framework"}, {"url": "https://enriccorona.github.io/vlogger/", "summary": "VLOGGER is a novel method for generating realistic talking human videos from a single image using text and audio cues. This method employs a generative diffusion model framework that combines human- to-3D motion and an innovative diffusion architecture for controlling temporal and spatial elements. This approach allows for creating high-quality, variable-length videos that maintain identity without the need for specific training on individuals or prior face detection and cropping tasks.", "title": "VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis"}, {"url": "https://github.com/MusicLang/musiclang_predict", "summary": "MusicLang Predict leverages the LLAMA2 architecture for symbolic music generation, offering advanced features such as manipulation of chord progressions and export functionality to MIDI for DAWs. It emphasizes performance on hardware without the need for GPUs and plans for future capabilities like bar-wise instrument control and real-time mobile app generation.", "title": "MusicLang/musiclang_predict: AI Prediction api of the MusicLang package"}], "datetime": "2024-03-25"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-march-18th-2024-ada11d7d0926", "news": [{"url": "https://www.cognition-labs.com/blog", "summary": "Devin is an AI-designed autonomous software engineer from Cognition, created to augment coding teams. It has strategic capabilities for complex challenges and integrates with dev tools for iterative development. Devin outperformed in SWE-bench, showing proficiency in learning and debugging, autonomously resolving 13.86% of real-world GitHub issues, marking an advancement over earlier AI models.", "title": "Introducing Devin, the first AI software engineer", "topics": ["AI for coding"], "sentiment": "positive"}, {"url": "https://www.anthropic.com/news/claude-3-haiku", "summary": "Anthropic has launched Claude 3 Haiku, an AI model designed for enterprise use, offering high efficiency, cost-effectiveness, and superior performance in processing large data sets with advanced vision capabilities, capable of handling prompts at a speed of up to 21K tokens per second for inputs under 32K tokens.", "title": "Claude 3 Haiku: our fastest model yet", "topics": ["Claude", "Anthropic"], "sentiment": "positive"}, {"url": "https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/", "summary": "DeepMind has developed SIMA, a generalist AI agent designed to operate within 3D virtual environments, focusing on interpreting natural language and navigating complex problems rather than traditional game score maximization. SIMA has been trained across nine games from various genres and features a combination of pre-trained image recognition and memory-based models to process and act on both visual cues and linguistic instructions.", "title": "SIMA generalist AI agent for 3D virtual environments", "topics": ["Reinforcement learning", "AI for images", "Multimodal AI (image, video, audio)", "DeepMind"], "sentiment": "positive"}, {"url": "https://github.com/xai-org/grok-1", "summary": "xAI has released Grok-1, a language Mixture-of-Experts model with 314 billion parameters, following its pre-training in October 2023. This base model checkpoint is intended for further research and the development of conversational applications, and it is accessible under the Apache 2.0 license.", "title": "Grok open release", "topics": ["Mixture-of-Experts", "Grok", "Model release"], "sentiment": "positive"}, {"url": "https://www.cnbc.com/2024/03/13/european-lawmakers-endorse-worlds-first-major-act-to-regulate-ai.html", "summary": "The European Parliament has passed a comprehensive AI Act for risk-based regulation of artificial intelligence, mandating stringent consumer protections and maintaining human oversight. With its implementation set for 2025, the legislation is expected to influence global tech firms and potentially set a precedent for future international AI regulations.", "title": "World\u2019s first major act to regulate AI passed by European lawmakers", "topics": ["AI regulation"], "sentiment": "positive"}, {"url": "https://venturebeat.com/ai/midjourney-debuts-feature-for-generating-consistent-characters-across-multiple-gen-ai-images/", "summary": "Midjourney has introduced an update enabling AI-generated character consistency in artwork through new tagging features. The \u201c \u2014 cref\u201d tag allows users to reference a character image URL to maintain the character\u2019s appearance across different scenes, while the \u201c \u2014 cw\u201d tag adjusts the level of character consistency. This facilitates continuity in visual storytelling within the AI art community, although the precision of replication can vary.", "title": "Midjourney debuts feature for generating consistent characters across multiple gen AI images", "topics": ["AI for images", "Multimodal AI (image, video, audio)", "Midjourney"], "sentiment": "positive"}, {"url": "https://80.lv/articles/midjourney-accuses-stability-ai-of-theft-bans-its-employees/", "summary": "David Holz, CEO of Midjourney, has accused Stability AI of image dataset theft, sparking an online exchange where Stability AI\u2019s CEO, Emad Mostaque, denied any directive for such actions and expressed willingness to support investigations into the matter.", "title": "Midjourney Accuses Stability AI of Image Theft, Bans Its Employees", "topics": ["AI and copyright", "AI datasets", "AI for images", "Midjourney", "Stable Diffusion", "Stability AI"], "sentiment": "negative"}], "guides": [{"url": "https://blog.langchain.dev/multi-needle-in-a-haystack/", "summary": "The new \u201cMulti-Needle + Reasoning\u201d benchmark highlights the limitations of LLMs with long contexts. It shows that while LLMs perform well when retrieving single facts from extensive data (the \u201cNeedle in a Haystack\u201d scenario), their efficiency declines when tasked with finding multiple facts and reasoning about them. Key findings indicate that LLMs, including GPT-4, struggle with retrieving numerous facts, particularly as context size grows, and also face challenges in reasoning over the facts they retrieve, suggesting a need for improved models for complex RAG tasks.", "title": "Multi Needle in a Haystack"}, {"url": "https://blog.langchain.dev/enhancing-rag-based-applications-accuracy-by-constructing-and-leveraging-knowledge-graphs/", "summary": "Graph Retrieval Augmented Generation (Graph RAG) is gaining importance in data retrieval, utilizing graph databases to enhance the context of information. Integrating the detailed organization of knowledge graphs with the fluidity of language models, tools like Neo4j and LangChain are advancing RAG applications.", "title": "Enhancing RAG-based application accuracy by constructing and leveraging knowledge graphs"}, {"url": "https://every.to/napkin-math/claude-3-is-the-most-human-ai-yet", "summary": "Anthropic\u2019s Claude 3 AI emphasizes human-like interaction, focusing on adding warmth to the typical efficiency-driven AI advancements. It serves as a collaborative tool for writers, leveraging its 1 million token context window to manage extensive projects effectively.", "title": "Claude 3 Is The Most Human AI Yet"}, {"url": "https://www.vellum.ai/blog/how-to-evaluate-your-rag-system", "summary": "Evaluation of RAG systems revolves around measuring their ability to accurately retrieve relevant context and generate pertinent, coherent, and reliable content. Key performance indicators for these systems include context relevance, answer relevancy, faithfulness, and correctness, which are essential for optimizing the RAG\u2019s performance in both context identification and response generation.", "title": "How to Evaluate Your RAG System?"}], "papers": [{"url": "https://arxiv.org/abs/2403.09611", "summary": "Apple\u2019s research team has unveiled MM1, a series of state-of-the-art multimodal AI models capable of processing both visual and linguistic information. The MM1 family includes a 30 billion parameter model that demonstrates superior few-shot learning abilities and excels in multimodal tasks such as Visual Question Answering (VQA) and image captioning.", "title": "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training"}, {"url": "https://github.com/AnswerDotAI/rerankers", "summary": "A library with a lightweight unified API for various reranking models. It addresses the challenge of utilizing various models with a lightweight, easy-to-use, and easy-to-integrate design, promising to simplify integration into existing pipelines.", "title": "AnswerDotAI/rerankers"}, {"url": "https://arxiv.org/abs/2403.09029", "summary": "The paper presents WebSight, a synthetic dataset of 2 million HTML and screenshot pairs designed to improve vision-language models (VLMs) in web development tasks, such as translating UI screenshots to HTML code. The authors demonstrate the VLM\u2019s enhanced performance on this dataset and contribute to the AI community by open-sourcing WebSight, encouraging further research in applying VLMs to web development.", "title": "Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset"}, {"url": "https://arxiv.org/abs/2403.06634", "summary": "A recent study has demonstrated a model-stealing attack that successfully extracts transformer model layers, including the embedding layer of high-profile models such as OpenAI\u2019s Ada and Babbage, and Google PaLM-2. Using API queries, the attack can inexpensively uncover the projection matrix of these language models, with estimates for the cost of exposing gpt-3.5-turbo\u2019s matrix at under $2,000.", "title": "Stealing Part of a Production Language Model"}, {"url": "https://arxiv.org/abs/2403.10301", "summary": "The rapid expansion of scientific articles presents a challenge for thorough literature analysis. LLMs offer a potential solution with their summarization capabilities, but they struggle with the multimodal elements prevalent in scientific content. Addressing this gap, Uni-SMART (Universal Science Multimodal Analysis and Research Transformer) has been developed to comprehend and analyze the complex multimodal data within scientific literature.", "title": "Uni-SMART: Universal Science Multimodal Analysis and Research Transformer"}], "datetime": "2024-03-18"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-march-11th-2024-0746035d064e", "news": [{"url": "https://www.anthropic.com/news/claude-3-family", "summary": "Anthropic has launched Claude 3, a new AI that surpasses GPT-4, with three models: Opus, Sonnet, and Haiku. Each supports a 200k context window, vision abilities, and multiple languages. Opus is touted as the top performer. Sonnet is integrated with Amazon Bedrock and Google Cloud\u2019s Vertex AI, while Opus and Haiku are slated for future release along with new features like function calling and REPL.", "title": "Introducing the next generation of Claude", "topics": ["Claude", "Anthropic", "Model release", "Multimodal AI (image, video, audio)", "Google", "Amazon", "GPT-4 and GPT-4 turbo"], "sentiment": "positive"}, {"url": "https://inflection.ai/inflection-2-5", "summary": "Inflection has launched its latest AI version, Inflection-2.5, enhancing its AI model, Pi, with advanced cognitive capabilities that challenge leading language models like GPT-4. Notably, Inflection-2.5 achieves competitive performance in AI tasks, particularly in coding and math, with 40% less computational power required during its training phase. In addition to its improved processing efficiency, Pi now features the ability to conduct real-time web searches to provide updated news and information.", "title": "Inflection-2.5: meet the world\u2019s best personal AI", "topics": ["AI for coding", "Model release", "GPT-4 and GPT-4 turbo"], "sentiment": "positive"}, {"url": "https://www.businessinsider.com/openai-cto-mira-murati-concerns-sam-altman-ouster-nyt-2024-3", "summary": "Sam Altman, CEO of OpenAI, faced a brief ouster from his position after concerns were raised by two executives, one of which was CTO Mira Murati. The circumstances around his temporary departure in November remain unclear, despite him resuming the CEO role within a week, as reported by The New York Times.", "title": "Looks like we may now know which OpenAI execs flagged concerns about Sam Altman before his ouster", "topics": ["OpenAI"], "sentiment": "negative"}, {"url": "https://blog.cloudflare.com/firewall-for-ai/", "summary": "Cloudflare is developing \u2018Firewall for AI\u2019, a Web Application Firewall designed to safeguard Large Language Models from abuse by detecting vulnerabilities and providing enhanced security measures for AI-powered applications.", "title": "Cloudflare announces Firewall for AI", "topics": ["AI safety"], "sentiment": "positive"}, {"url": "https://blog.google/products/search/google-search-update-march-2024/", "summary": "Google is updating its Search algorithm to demote low-quality, automated content and elevate more valuable, trustworthy websites in search rankings, focusing on delivering a high-quality content experience.", "title": "Google is tackling spammy, low-quality content on Search", "topics": ["Google"], "sentiment": "positive"}], "guides": [{"url": "/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893", "summary": "An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.", "title": "A Practical Guide to RAG Pipeline Evaluation (part 1)"}, {"url": "https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness", "summary": "In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.", "title": "Training great LLMs entirely from ground up in the wilderness as a startup"}, {"url": "/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c", "summary": "The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.", "title": "Gemma on Android and iPhone and more local LLM updates from MLC"}, {"url": "https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html", "summary": "Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.", "title": "You can now train a 70b language model at home"}, {"url": "https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness", "summary": "Effective prompting techniques such as adding rich contexts, custom examples, and adopting a \u201cChain of Thought\u201d strategy significantly enhance the performance of AI models like Meta\u2019s Llama 2 or GPT-4.", "title": "Captain\u2019s log: the irreducible weirdness of prompting AIs"}], "papers": [{"url": "https://arxiv.org/abs/2403.04132", "summary": "Chatbot Arena is an open platform designed to enhance NLP by aligning LLMs with human preferences using simple feedback comparisons. It incorporates over 240,000 user votes to refine assessment criteria, promote question variety, and ensure expert agreement, thus confirming the trustworthiness of its results.", "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference"}, {"url": "https://arxiv.org/abs/2403.00071", "summary": "The study presents Resonance RoPE, a solution to improve the ability of Transformers with Rotary Position Embedding (RoPE) to handle longer sequence lengths than those seen during training (train-short-test-long scenarios). This is achieved by enhancing RoPE for out-of- distribution positions to improve model performance on longer sequences, with the advantage of not incurring extra computational costs during operation.", "title": "Resonance RoPE: Improving Context Length Generalization of Large Language Models"}, {"url": "https://arxiv.org/abs/2402.10949v2", "summary": "This study investigates the impact of \u201cpositive thinking\u201d prompts on the performance of different LLMs across a dataset of math questions (GSM8K). It concludes that the effectiveness of hand-tuned prompts is not consistent across models, and suggests that systematic, automated prompt optimization is the superior approach for achieving high-quality results from LLMs.", "title": "The Unreasonable Effectiveness of Eccentric Automatic Prompts"}, {"url": "https://arxiv.org/html/2402.11753v2", "summary": "Recent research has identified a vulnerability in LLMs, where ASCII art can be used to conduct jailbreak attacks by exploiting their weaknesses in interpreting non-semantic prompts. The ViTC benchmark has been developed to test LLMs\u2019 abilities against these challenges, revealing that even advanced models such as GPT-3.5, GPT-4, Gemini, Claude, and Llama2 are susceptible.", "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs"}, {"url": "https://arxiv.org/abs/2403.04652", "summary": "The Yi model series expands on pretrained language models of 6B and 34B parameters by enhancing them for chat, handling 200K token contexts, and incorporating vision-language capabilities. Leveraging a high-performance computing infrastructure and transformer designs, the Yi models excel due to high- quality training data crafted through rigorous deduplication and filtering processes. The authors also meticulously finetuned a small dataset iteratively with direct input from machine learning engineers.", "title": "Yi: Open Foundation Models by 01.AI"}], "datetime": "2024-03-11"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-march-4th-2024-f917696e52e1", "news": [{"url": "https://techcrunch.com/2024/02/26/mistral-ai-releases-new-model-to-rival-gpt-4-and-its-own-chat-assistant/", "summary": "Mistral has launched Mistral Large, ranking just below GPT4. It boasts a 32K token context window and multilingual support for English, French, Spanish, German, and Italian. The model excels in following precise instructions, allowing for tailored moderation policies.", "title": "Mistral AI releases new model to rival GPT-4 and its own chat assistant", "topics": ["Model release", "Mistral", "GPT-4 and GPT-4 turbo"], "sentiment": "positive"}, {"url": "https://www.theverge.com/2024/2/26/24083510/microsoft-mistral-partnership-deal-azure-ai", "summary": "Microsoft enters a multiyear collaboration with Mistral, valued at \u20ac2 billion, acquiring a minor stake shortly after its significant investment in OpenAI.", "title": "Microsoft partners with Mistral in second AI deal beyond OpenAI", "topics": ["Mistral", "Microsoft", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.theverge.com/2024/2/27/24084907/apple-electric-car-project-titan-shuts-down", "summary": "Apple has discontinued its \u201cProject Titan\u201d electric car development, reallocating many of its 2,000 staff to focus on generative AI projects.", "title": "Apple\u2019s electric car project is dead", "topics": ["Apple"], "sentiment": "negative"}, {"url": "https://www.courthousenews.com/elon-musk-sues-openai-over-ai-threat/", "summary": "Elon Musk has initiated a lawsuit against OpenAI, alleging that the organization has departed from its founding mission of promoting AGI for public benefit by forming a profit-centered partnership with Microsoft.", "title": "Elon Musk sues OpenAI over AI threat", "topics": ["AI safety", "AI regulation", "Microsoft", "OpenAI"], "sentiment": "negative"}, {"url": "https://www.together.ai/blog/evo", "summary": "Evo is an advanced bio foundation model based on the StripedHyena architecture, specialized in interpreting biological data such as genomes and proteins with a vocabulary over 650k tokens. It\u2019s been created by Together AI and the Arc Institute.", "title": "Evo: Long-context modeling from molecular to genome scale", "topics": ["AI in healthcare"], "sentiment": "positive"}], "guides": [{"url": "https://www.llamaindex.ai/blog/towards-long-context-rag", "summary": "Google\u2019s Gemini 1.5 Pro has introduced an impressive 1-million-word context window, fueling a debate in the AI community about the future relevance of Retrieval-Augmented Generation (RAG). Anticipated advancements in RAG architectures include more efficient document retrieval techniques, better routing for reduced latency and cost, and improved key-value (KV) caching, aiming to maximize the benefits of long-context models.", "title": "Towards Long Context RAG"}, {"url": "https://huggingface.co/blog/arena-tts", "summary": "TTS Arena, mirroring the concept of LMSys Chatbot Arena, provides a platform for comparing and evaluating text-to-speech models, enabling users to sample, review, and rate various systems to determine the most realistic voices.", "title": "TTS Arena: Benchmarking Text-to-Speech Models in the Wild"}, {"url": "https://www.deeplearning.ai/short-courses/prompt-engineering-with-llama-2/", "summary": "Deeplearning.ai has launched a new course titled \u201cPrompt Engineering with Llama 2,\u201d designed to enhance skills in prompt creation and model optimization. The course covers advanced prompt strategies, including few-shot and chain-of-thought techniques. It also introduces Code Llama, a virtual aid for pair programming.", "title": "Prompt Engineering with Llama 2"}, {"url": "https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html", "summary": "The Berkeley Function-Calling Leaderboard (BFCL) has established a new benchmark for evaluating LLMs on their ability to execute various types of function calls. GPT-4 is currently the top performer in function-calling tasks.", "title": "Introduction to Gorilla LLM"}, {"url": "https://arxiv.org/abs/2402.18158v1", "summary": "This paper/guide explores the impact of post-training quantization (PTQ) on reducing memory and computational demands of large language models. It offers an extensive evaluation of PTQ\u2019s effect on various components like Weight, Activation, and KV Cache across 11 LLM families with parameter sizes from 125 million to 180 billion.", "title": "Evaluating Quantized Large Language Models"}], "papers": [{"url": "https://arxiv.org/abs/2402.17764", "summary": "Recent advancements have introduced LLMs with ternary weights, notably BitNet b1.58, which achieves comparable perplexity and task performance to full-precision LLMs with significantly reduced computational costs. This approach not only cuts down on latency, memory requirements, throughput, and energy usage, but also challenges the conventional use of GPUs by leveraging hardware optimized for additions.", "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"}, {"url": "https://arxiv.org/abs/2402.19173", "summary": "StarCoder2 is a newly introduced open-source large language model specializing in code generation and comprehension across \u201csmall\u201d size ranges, with variants at 3B, 7B, and 15B parameters. Notable improvements include a dataset expansion with The Stack v2, containing a diverse set of 619 programming languages. The models have been trained on an extensive 3.3 to 4.3 trillion token dataset and have shown impressive performance across a variety of coding benchmarks.", "title": "StarCoder 2 and The Stack v2: The Next Generation"}, {"url": "https://arxiv.org/abs/2402.14740", "summary": "Recent findings indicate that REINFORCE optimization, a simpler and less computationally demanding method, can outperform the popular but complex Proximal Policy Optimization (PPO) for aligning Large Language Models (LLMs) with human preferences during Reinforcement Learning from Human Feedback (RLHF).", "title": "Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs"}, {"url": "https://arxiv.org/abs/2402.15838v1", "summary": "The authors have developed ListT5, a new reranking method utilizing Fusion-in-Decoder (FiD) that addresses the processing of multiple candidate passages during both training and inference phases. An efficient listwise ranking system is introduced, using m-ary tournament sort with output caching for swift inference. ListT5 has demonstrated superior performance, beating the previous top model, RankT5, with an improvement of +1.3 in average NDCG@10 score.", "title": "ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval"}, {"url": "https://arxiv.org/abs/2402.14905", "summary": "This paper introduces MobileLLM, a sub-billion parameter language model optimized for mobile devices, challenging the traditional emphasis on model size by demonstrating the importance of architecture. It utilizes lean structures with shared embeddings and grouped-query attention to outperform prior models of similar scale, offering marked accuracy improvements on both chat benchmarks and API calling tasks, rivaling larger 7B models in specific use cases.", "title": "MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases"}], "datetime": "2024-03-04"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-february-26th-2024-6ec29da5d8a4", "news": [{"url": "https://ai.google.dev/gemma/", "summary": "Google has released Gemma, an open-source large language model based on Gemini, in two versions with 2 billion (2B) and 7 billion (7B) parameters. Both versions come with a basic pretrained model and an instruction-tuned variant to enhance performance.", "title": "Gemma \u2014 a family of lightweight, state-of-the art open models from Google.", "topics": ["Model release", "Google Gemini", "Google"], "sentiment": "positive"}, {"url": "https://siliconangle.com/2024/02/21/nvidias-data-center-gpu-sales-grow-stunning-409-huge-demand-ai-chips/", "summary": "Nvidia has experienced a significant surge in GPU sales, reporting a 409% increase due in large part to the rising demand for AI technologies. With Q4 earnings and revenue significantly outpacing Wall Street forecasts, the company\u2019s financials have thrived on the back of robust sales from their Hopper GPU series, including the H100.", "title": "Nvidia\u2019s data center GPU sales grow by a stunning 409% on huge demand for AI chips", "topics": ["NVIDIA", "AI Chips and GPUs"], "sentiment": "positive"}, {"url": "https://stability.ai/news/stable-diffusion-3", "summary": "Stability AI has introduced Stable Diffusion 3 for early preview, featuring enhancements in handling multi-subject prompts, image quality, and the accuracy of visual text spelling. A select number of users have the opportunity to test and refine the model prior to general availability.", "title": "Stable Diffusion 3", "topics": ["Stable Diffusion", "Stability AI", "AI for images"], "sentiment": "positive"}, {"url": "https://www.theverge.com/2024/2/22/24079876/google-gemini-ai-photos-people-pause", "summary": "Google has suspended the feature in its Gemini AI that creates images of human figures due to diversity-related inaccuracies. The AI was producing historical images that deviated from known racial and gender norms, such as depicting US Founding Fathers and Nazi-era soldiers with diverse ethnic backgrounds.", "title": "Google pauses Gemini\u2019s ability to generate AI images of people after diversity errors", "topics": ["AI safety", "AI for images", "Google Gemini", "Google"], "sentiment": "negative"}, {"url": "https://www.phind.com/blog/introducing-phind-70b", "summary": "Phind-70B is a new code-centric AI model that improves upon CodeLlama-70B by integrating 50 billion more tokens. It features a 32K token window, enabling it to produce high-quality technical solutions at a speed of 80 tokens per second. The model surpasses GPT-4 Turbo with an 82.3% HumanEval score, although it performs slightly below Meta\u2019s CRUXEval.", "title": "Introducing Phind-70B \u2014 closing the code quality gap with GPT-4 Turbo while running 4x faster", "topics": ["AI for coding", "GPT-4 and GPT-4 turbo"], "sentiment": "positive"}], "guides": [{"url": "https://pub.towardsai.net/top-11-ai-powered-image-generators-in-2024-c6a06e9697e8", "summary": "AI-powered image generators such as DALLE 3, Midjourney, Dream Studio (Stable Diffusion), Canva AI, and NightCafe are transforming the landscape of visual creation, offering innovative tools for professional design, business applications, and art enthusiasts.", "title": "Top 11 AI-Powered Image Generators in 2024"}, {"url": "https://developer.nvidia.com/blog/build-an-llm-powered-data-agent-for-data-analysis/", "summary": "This guide outlines the necessary agent types and their collaborative roles in creating a proficient LLM application for data analysis tasks. It includes a practical use case and corresponding code snippets, alongside optimization tips for AI developers involved in the design and implementation of LLM agent applications.", "title": "Build an LLM-Powered Data Agent for Data Analysis"}, {"url": "https://machinelearningmastery.com/advanced-techniques-for-research-with-chatgpt/", "summary": "This guide outlines strategies for leveraging ChatGPT in research, emphasizing that while ChatGPT can streamline research tasks, the quality of research still depends on the human researcher\u2019s expertise and understanding.", "title": "Advanced Techniques for Research with ChatGPT"}, {"url": "https://reutersinstitute.politics.ox.ac.uk/how-many-news-websites-block-ai-crawlers", "summary": "U.S. news publishers are increasingly blocking AI crawlers from companies such as OpenAI and Google, with 80% of top U.S. sites restricting OpenAI\u2019s access as of late 2023. The trend exhibits significant variation internationally, with only 20% of leading news sites in Mexico and Poland implementing similar blocks.", "title": "How many news websites block AI crawlers?"}, {"url": "https://nicholas.carlini.com/writing/2024/my-benchmark-for-large-language-models.html", "summary": "This benchmark assesses the capabilities of large language models in real-world programming tasks, such as code translation between Python and C, understanding minified JavaScript, and generating SQL from English.", "title": "My benchmark for large language models"}], "papers": [{"url": "https://arxiv.org/abs/2402.14658", "summary": "The OpenCodeInterpreter is an open-source project enhancing code generation by integrating code execution and iterative refinement, akin to the proprietary GPT-4 Code Interpreter. It uses the Code-Feedback dataset with 68K interactive sessions to improve its performance. OpenCodeInterpreter-33B demonstrates near-parity with GPT-4 on coding benchmarks.", "title": "OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement"}, {"url": "https://arxiv.org/abs/2402.14083", "summary": "Searchformer is an AI model based on Transformer architecture which has been trained to emulate the A* pathfinding algorithm, achieving higher efficiency in complex planning tasks. It outperforms A* in Sokoban puzzles, solving them with 93.7% accuracy and a 26.8% reduction in steps taken.", "title": "Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping"}, {"url": "https://arxiv.org/abs/2402.13753", "summary": "LongRoPE is an advancement in large language models that extends the context window from 256k to 2048k tokens using positional interpolation, while also incorporating a fine-tuning phase at 8k tokens to preserve short-context performance, aiming for greater efficiency and reduced fine-tuning costs.", "title": "LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens"}, {"url": "https://arxiv.org/abs/2402.10986", "summary": "Researchers have successfully finetuned the Mistral-7b model for various financial tasks, exhibiting performance comparable to GPT4 turbo. These tasks include sentiment analysis, named entity recognition, number understanding, text summarization, stock prediction, and credit scoring.", "title": "FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models"}, {"url": "https://github.com/vosen/ZLUDA", "summary": "ZLUDA is an alpha-stage software layer enabling unmodified CUDA applications to run on AMD GPUs, facilitating increased compatibility within the AI community. It demonstrates promising performance, although with a current preference for integrated GPUs and potential limitations arising from ROCm/HIP integration.", "title": "vosen/ZLUDA: CUDA on AMD GPUs"}], "datetime": "2024-02-26"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-february-19th-2024-74c44213ad49", "news": [{"url": "https://openai.com/sora", "summary": "OpenAI has unveiled Sora, a novel AI video generator capable of crafting up to minute-long videos from textual prompts. Demonstrations showcase its ability to transform creative prompts into video content, emphasizing the synergy between AI and human creativity.", "title": "OpenAI announces Sora", "topics": ["Multimodal AI (image, video, audio)", "OpenAI"], "sentiment": "positive"}, {"url": "https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#sundar-note", "summary": "Google has unveiled Gemini 1.5, which demonstrates an enhanced capacity for processing long-context information up to 1 million tokens. It matches the quality of Gemini 1.0 Ultra, but requires less computational power and surpasses Gemini 1.0 Pro in 87% of benchmarks.", "title": "Our next-generation model: Gemini 1.5", "topics": ["Google Gemini", "Google"], "sentiment": "positive"}, {"url": "https://stability.ai/news/introducing-stable-cascade", "summary": "Stability AI has introduced Stable Cascade, a research preview of a new text-to-image model based on the W\u00fcrstchen architecture. This model is distributed under a non-commercial license and boasts ease of training and fine-tuning on consumer-grade hardware due to its innovative three-stage approach.", "title": "Stability AI announces Stable Cascade", "topics": ["AI for images", "Stability AI"], "sentiment": "positive"}, {"url": "https://the-decoder.com/openai-is-reportedly-developing-ai-web-search-to-compete-with-google/", "summary": "OpenAI is reportedly developing an AI web search capability, potentially to compete with Google, and may incorporate this into a new service or an improved ChatGPT that utilizes Bing for web-based information summarization.", "title": "OpenAI is reportedly developing AI web search to compete with Google", "topics": ["Google", "Microsoft", "ChatGPT", "OpenAI"], "sentiment": "positive"}, {"url": "https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/", "summary": "Yann LeCun proposes a machine learning paradigm, V-JEPA, to enable systems to build internal world models and learn intuitively like a human. Unlike conventional methods, V-JEPA employs a non-generative technique for video understanding, prioritizing abstract interpretation over detailed reproduction.", "title": "V-JEPA: The next step toward advanced machine intelligence", "topics": ["Multimodal AI (image, video, audio)"], "sentiment": "positive"}, {"url": "https://www.datacenterdynamics.com/en/news/together-ai-set-to-receive-100m-in-funding-round-led-by-salesforce-ventures/", "summary": "Together AI, a GPU cloud company specializing in open-source AI tools and Nvidia server chip access, is nearing a $100 million funding round led by Salesforce Ventures, potentially elevating its valuation to $1 billion.", "title": "GPU cloud company Together AI to raise $100m", "topics": ["Funding", "NVIDIA", "AI Chips and GPUs"], "sentiment": "positive"}], "guides": [{"url": "https://huggingface.co/blog/peft_merging", "summary": "Model merging has quickly become a de-facto standard of pushing the performance limits of large language models. On the Open LLM Leaderboard, there are new merged models topping up the charts.", "title": "\ud83e\udd17 PEFT welcomes new merging methods"}, {"url": "/towards-artificial-intelligence/machine-learning-in-chemistry-87e6ff866026", "summary": "Machine learning and neural networks like CNNs and RNNs are significantly advancing chemical research by identifying patterns in complex data, aiding in drug development, toxicity prediction, and understanding structure-activity relationships.", "title": "Machine Learning in Chemistry"}, {"url": "https://sohl-dickstein.github.io/2024/02/12/fractal.html", "summary": "Neural network training can inadvertently generate intricate fractals, reflecting the dynamic interplay of hyperparameter settings, particularly the learning rate. As the setting of the learning rate is fine-tuned to avoid divergence and ensure efficient training, the boundary between effective training and failure manifests as a fractal pattern.", "title": "Neural network training makes beautiful fractals"}, {"url": "https://blog.research.google/2024/02/learning-importance-of-training-data.html", "summary": "Recent research proposes a relevance-based ranking system for training data, utilizing a helper model designed to address and adapt to slow concept drift in AI. This approach has the potential to enhance model performance over time, presenting a competitive solution for improved adaptability in continual learning.", "title": "Learning the importance of training data under concept drift"}, {"url": "/towards-artificial-intelligence/geogpt-tutorial-web-ready-map-visuals-from-gis-forest-fire-data-f9ecb7e214dc", "summary": "GeoGPT+ is a geospatial GPT tool designed for real-time data integration and visual map generation from GIS datasets. Notably, it specializes in interpreting forest fire data from sources like the NASA Forest Fires CSV dataset, transforming raw data into meaningful visuals.", "title": "GeoGPT+ Tutorial: Web-Ready Map Visuals From GIS Forest Fire Data"}], "papers": [{"url": "https://arxiv.org/abs/2402.08609", "summary": "Researchers have explored the integration of Soft Mixture-of-Expert (MoE) modules into value-based deep reinforcement learning networks, offering a novel approach to scale model size while improving performance. Results indicate the potential for developing consistent scaling laws in reinforcement learning, a field that has previously lacked such frameworks.", "title": "Mixtures of Experts Unlock Parameter Scaling for Deep RL"}, {"url": "https://arxiv.org/abs/2402.08093", "summary": "BASE TTS, a cutting-edge text-to-speech system with 100K hours of training, has established a new benchmark for natural-sounding speech synthesis. It utilizes a 1-billion-parameter Transformer model to generate \u201cspeechcodes\u201d from text, which are then converted into waveforms by a convolutional decoder.", "title": "BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data"}, {"url": "https://arxiv.org/abs/2402.04315", "summary": "Researchers have developed a method to train smaller language models (LMs) to generate responses with appropriate citations, using Llama 7B as a test case. Initially, they trained Llama 7B on ChatGPT outputs to answer questions with cited contexts. Following that, they improved the model further through rejection sampling and reinforcement learning. Their findings indicate that with this training approach, Llama 7B outperforms ChatGPT in providing cited answers. Moreover, a combination of rejection sampling and reinforcement learning yielded the most effective results.", "title": "Training Language Models to Generate Text with Citations via Fine-grained Rewards"}, {"url": "https://largeworldmodel.github.io/", "summary": "Researchers created a dataset comprising videos and books and introduced the RingAttention mechanism to efficiently handle context sizes ranging from 4K to 1M tokens. The team has also released various models with up to 7B parameters, capable of processing over 1M tokens, to promote accessibility and collaborative progress in the field.", "title": "Large World Models"}, {"url": "https://arxiv.org/abs/2402.09371", "summary": "A study demonstrates that standard Transformers can generalize integer addition to longer sequences through effective data representation and positional encoding methods. However, this generalization ability is sensitive to factors like weight initialization and the sequence of training data, resulting in considerable variability in the model\u2019s performance.", "title": "Transformers Can Achieve Length Generalization But Not Robustly"}, {"url": "https://github.com/reorproject/reor", "summary": "Reor is an open-source desktop application designed for AI-enhanced note-taking, featuring an Obsidian-style markdown editor, AI-assisted idea connection, and smart search capabilities. It prioritizes user privacy through local storage and utilizes technologies such as Llama.cpp and Transformers.js to run large language models and embedding models on the user\u2019s machine.", "title": "reorproject/reor: AI note-taking app that runs models locally."}], "datetime": "2024-02-19"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-february-12th-2024-8d0291327711", "news": [{"url": "https://blog.google/products/gemini/bard-gemini-advanced-app/", "summary": "Google has launched Gemini Advanced, a new AI-powered digital assistant feature within Google One AI Premium, priced at $19.99 per month after a two-month free trial. Gemini is designed to enhance productivity for Android and iOS users by integrating with daily tasks, offering support through an app, Google Assistant, or voice commands.", "title": "Bard becomes Gemini: Try Ultra 1.0 and a new mobile app today", "topics": ["Google Gemini", "Google"], "sentiment": "positive"}, {"url": "https://www.firstpost.com/tech/openai-sam-altman-is-chasing-trillions-of-dollars-as-investments-to-disrupt-ai-chip-industries-13708732.html", "summary": "Sam Altman, CEO of OpenAI, is actively seeking to secure $5\u20137 trillion in funding to expand the semiconductor industry to support AI development. This investment aims to address GPU shortages and foster the growth of both AI and artificial general intelligence. Altman is engaging with various stakeholders, including government officials from the UAE and the US, investors, and chip manufacturers, in his efforts to build a robust global chip-making infrastructure that meets the increasing demands and energy requirements of AI facilities.", "title": "OpenAI\u2019s CEO Sam Altman is chasing trillions of dollars as investments to disrupt AI, chip industries", "topics": ["Funding", "AI Chips and GPUs", "OpenAI"], "sentiment": "positive"}, {"url": "https://thenextweb.com/news/deepfake-face-swap-attacks-increase", "summary": "Deepfake technology advancements have resulted in a significant rise in \u2018face swap\u2019 attacks, with a 704% increase in the second half of the year, driven by accessible GenAI tools such as SwapFace and DeepFaceLive. These tools enhance the ability to produce undetectable deepfakes, facilitating anonymity and contributing to a spike in deepfake-enabled crimes, including a notable financial scam in Hong Kong.", "title": "Deepfake \u2018face swap\u2019 attacks surged 704% last year, study finds", "topics": ["AI safety", "AI for images", "AI regulation", "Multimodal AI (image, video, audio)"], "sentiment": "negative"}, {"url": "https://about.fb.com/news/2024/02/labeling-ai-generated-images-on-facebook-instagram-and-threads/", "summary": "Meta is implementing \u201cImagined with AI\u201d labels for AI-generated content on Facebook and Instagram for greater transparency. While AI image labeling is available, Meta is developing detection for audio/video content and requires user disclosure until standards are established. Additionally, measures are being taken to ensure these transparency labels cannot be removed.", "title": "Labeling AI-Generated Images on Facebook, Instagram and Threads", "topics": ["AI for images", "Multimodal AI (image, video, audio)", "Meta"], "sentiment": "positive"}, {"url": "https://www.theverge.com/2024/2/6/24063954/ai-watermarks-dalle3-openai-content-credentials", "summary": "OpenAI\u2019s DALL-E 3 now features watermarking to distinguish AI-generated images from those created by humans, enhancing transparency in the field.", "title": "OpenAI is adding new watermarks to DALL-E 3", "topics": ["AI and copyright", "AI for images", "OpenAI"], "sentiment": "positive"}], "guides": [{"url": "https://lilianweng.github.io/posts/2024-02-05-human-data-quality/", "summary": "High-quality, detailed human annotations are crucial for creating effective deep learning models, ensuring AI accuracy through tasks such as content classification and language model alignment. Leveraging the \u201cwisdom of the crowd\u201d with appropriate filters can produce results comparable to expert quality, as proven by research in areas like machine translation.", "title": "Thinking about High-Quality Human Data"}, {"url": "https://huggingface.co/blog/leaderboards-on-the-hub-nphardeval", "summary": "The NPHardEval leaderboard offers a benchmark for evaluating the reasoning capabilities of LLMs on a set of 900 algorithmic problems, focusing on NP-Hard and less complex tasks. To maintain the assessment\u2019s integrity and prevent model overfitting, it undergoes monthly updates with new challenges, which helps to reliably measure the progress in LLMs\u2019 reasoning performance.", "title": "NPHardEval Leaderboard: Unveiling the Reasoning Abilities of Large Language Models through Complexity Classes and Dynamic Updates"}, {"url": "https://huggingface.co/blog/segmoe", "summary": "SegMoE, embedded within the Hugging Face ecosystem, simplifies the development of bespoke Mixture- of-Experts Diffusion models influenced by Stable Diffusion, incorporating sparse MoE layers for specific token processing.", "title": "SegMoE: Segmind Mixture of Diffusion Experts"}, {"url": "https://www.usv.com/writing/2024/02/ai-aesthetics/", "summary": "AI is transforming the art landscape by facilitating the creation of visuals, music, and narratives, making it accessible to individuals regardless of their artistic skills \u2014 akin to how smartphones popularized photography. It has enabled the emergence of new genres and interactive art that turns consumers into collaborators, utilizing tools such as storytelling AI chatbots and dynamic installations like \u201cDream Machine.\u201d", "title": "AI Aesthetics"}, {"url": "https://shellypalmer.com/2024/02/why-i-switched-from-google-search-to-perplexity/", "summary": "The author discusses transitioning from Google to the AI-powered search engine Perplexity, highlighting its key features like delivering summarized, direct answers with source citations, and the unique ability to provide rewritten perspectives.", "title": "Why I Switched From Google Search to Perplexity"}], "papers": [{"url": "https://arxiv.org/abs/2402.04494", "summary": "DeepMind has developed a 270M parameter transformer model that attained Grandmaster-level chess proficiency without reliance on traditional search techniques. Trained on a dataset of 10 million games with action-value insights from Stockfish 16, the model notched up a Lichess blitz Elo of 2895, and displayed the capability to solve advanced chess puzzles.", "title": "Grandmaster-Level Chess Without Search"}, {"url": "https://arxiv.org/abs/2402.05120", "summary": "Ensemble methods significantly enhance the performance of language models, as evidenced by increases in accuracy for Llama2\u201313B, Llama2\u201370B, and GPT-3.5-Turbo on the GSM8K benchmark. Larger ensembles, particularly of size 15 and above, enable smaller models like the Llama2\u201313B to reach accuracy levels comparable to that of larger models like the Llama2\u201370B. Additionally, upscaling ensembles to 15\u201320 members allows Llama2\u201370B and GPT-3.5-Turbo to match the performance of even more advanced models.", "title": "More Agents Is All You Need"}, {"url": "https://github.com/metavoiceio/metavoice-src", "summary": "MetaVoice-1B is a state-of-the-art TTS model trained on an extensive 100K-hour dataset, designed to generate emotionally expressive English speech with a focus on American and British tones. It offers voice cloning using just 30 seconds of audio and supports long-form speech synthesis, all under the permissive Apache 2.0 license.", "title": "metavoiceio/metavoice-src: Foundational model for human-like, expressive TTS"}, {"url": "https://github.com/apple/ml-mgie", "summary": "Apple has introduced MGIE, an innovative model for instruction-based image editing using multimodal large language models (LLMs). MGIE stands out by effectively learning and interpreting descriptive instructions, which it uses to guide visual alterations. Its end-to-end training allows it to execute detailed Photoshop-like alterations, comprehensive photo enhancements, and targeted edits.", "title": "apple/ml-mgie"}, {"url": "https://arxiv.org/abs/2402.04229", "summary": "MusicRL, an advanced iteration of the MusicLM model, leverages reinforcement learning and human feedback to enhance its music generation capabilities, particularly in terms of textual alignment and sound quality.", "title": "MusicRL: Aligning Music Generation to Human Preferences"}], "datetime": "2024-02-12"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-february-5th-2024-a7bb89f8c629", "news": [{"url": "https://ai.meta.com/blog/code-llama-large-language-model-coding/", "summary": "Meta has launched Code Llama 70B, a coding AI model comparable to GPT4, in three variations: the base model, a Python-specific version, and an \u2018Instruct\u2019 version for interpreting natural language commands. All editions are free for both research and commercial applications.", "title": "Introducing Code Llama, a state-of-the-art large language model for coding", "topics": ["AI for coding", "Meta"], "sentiment": "positive"}, {"url": "https://analyticsindiamag.com/googles-gemini-pro-beats-gpt-4/", "summary": "Google\u2019s Gemini Pro ranks just below GPT4 Turbo and ahead of GPT4 in the ChatBot Arena leaderboard, a platform that uses an Elo ranking system based on 200,000+ human votes to assess LLM performance. Speculations about Meta\u2019s upcoming Llama 3 and a potential Q2 release of OpenAI\u2019s GPT-5 are stirring anticipation in the AI sector.", "title": "Google\u2019s Gemini Pro Beats GPT-4", "topics": ["Google Gemini", "Google", "ChatGPT", "GPT-4 and GPT-4 turbo", "GPT-5", "OpenAI"], "sentiment": "positive"}, {"url": "https://news.slashdot.org/story/24/01/31/2145205/mistral-confirms-new-open-source-ai-model-nearing-gpt-4-performance", "summary": "Mistral has recently confirmed that the \u201cmiqu-1\u201370b\u201d Large Language Model, released on HuggingFace and exhibiting performance close to that of GPT-4, is a leaked quantized version of their technology.", "title": "Mistral Confirms New Open Source AI Model Nearing GPT-4 Performance", "topics": ["Hugging Face", "AI and copyright", "Mistral", "GPT-4 and GPT-4 turbo"], "sentiment": "positive"}, {"url": "https://thealgorithmicbridge.substack.com/p/sam-altman-says-gpt-5-will-be-okay", "summary": "OpenAI CEO Sam Altman adopts a cautious tone when discussing AI, recently describing the anticipated GPT-5 as merely \u201cokay\u201d at Davos. This balanced approach suggests a strategic shift towards tempered communication.", "title": "Sam Altman Says GPT-5 Will Be \u2018Okay\u2019", "topics": ["GPT-5", "OpenAI"], "sentiment": "positive"}, {"url": "https://venturebeat.com/ai/hugging-face-launches-open-source-ai-assistant-maker-to-rival-openais-custom-gpts/", "summary": "Hugging Face has introduced free, customizable Chat Assistants on its Hugging Chat platform, presenting an open-source alternative to OpenAI\u2019s GPT services. This initiative offers developers and AI enthusiasts cost-free access to various large language models, including Mistral\u2019s Mixtral and Meta\u2019s Llama 2.", "title": "Hugging Face launches open source AI assistant maker to rival OpenAI\u2019s custom GPTs", "topics": ["Hugging Face", "Mistral", "LLaMA", "Meta", "ChatGPT", "OpenAI"], "sentiment": "positive"}], "guides": [{"url": "https://venturebeat.com/ai/how-enterprises-are-using-open-source-llms-16-examples/", "summary": "Enterprises across various sectors are integrating open-source large language models (LLMs) to enhance their operations and user experiences. Companies such as VMware, Brave, and Gab Wireless are leveraging models like HuggingFace\u2019s and Mistral AI\u2019s for code generation and conversational assistance.", "title": "How enterprises are using open source LLMs"}, {"url": "https://huggingface.co/blog/open-source-llms-as-agents", "summary": "Open-source LLMs like Mixtral have reached performance levels that allow them to serve as central reasoning components in intelligent agents, even surpassing GPT-3.5 benchmarks.", "title": "Open-source LLMs as LangChain Agents"}, {"url": "https://huggingface.co/blog/leaderboards-on-the-hub-patronus", "summary": "The Enterprise Scenarios Leaderboard, developed by the Patronus team in partnership with Hugging Face, is a new benchmarking tool designed to assess language model performance across six business-oriented tasks. These tasks include finance, legal issues, creative writing, customer support, toxicity detection, and handling of personally identifiable information (PII), with a specific emphasis on enterprise requirements.", "title": "Introducing the Enterprise Scenarios Leaderboard: a Leaderboard for Real World Use Cases"}, {"url": "https://vitalik.eth.limo/general/2024/01/30/cryptoai.html", "summary": "The intersection of AI and blockchain has the potential to revolutionize various systems, with AI poised to enhance blockchain\u2019s efficiency and reliability through capabilities such as optimizing arbitrage and prediction, as well as improving accessibility by simplifying transactions and augmenting security measures.", "title": "The promise and challenges of crypto + AI applications"}, {"url": "https://adamkarvonen.github.io/machine_learning/2024/01/03/chess-world-models.html", "summary": "hess-GPT, a machine learning model with 50M parameters trained on millions of chess game notations, has demonstrated an ability to play chess at a 1300 Elo rating, reflecting an understanding of the game\u2019s rules and strategies. The model executes legal moves with a 99.8% accuracy post-training, indicating that even compact models, with enough data, can approximate human-level problem-solving skills.", "title": "Chess-GPT\u2019s Internal World Model"}], "papers": [{"url": "https://arxiv.org/abs/2401.03729", "summary": "Recent research reveals that even minor prompt modifications, including additional spaces, can significantly affect the performance of LLMs in text classification tasks, underscoring the critical importance of precise prompt engineering.", "title": "The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance"}, {"url": "https://arxiv.org/abs/2401.16380", "summary": "Researchers have developed a method to enhance LLM training by using a smaller instruction-tuned LLM to paraphrase web scrapes, creating a cleaner, structured dataset. This approach has shown to accelerate pre-training, reduce computational costs, and improve performance, achieving a 3x speed increase, 10% perplexity reduction, and better zero-shot learning capabilities on various tasks.", "title": "Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"}, {"url": "https://arxiv.org/abs/2402.00838", "summary": "OLMo is the first entirely open-source LLM whose release includes not just the model weights and inference code but also the training data, training code, and evaluation code.", "title": "OLMo: Accelerating the Science of Language Models"}, {"url": "https://arxiv.org/abs/2401.15947", "summary": "The LLaVA team has introduced MoE-LLaVA, an open-source, sparse Large Vision-Language Model (LVLM) that leverages a mixture of experts (MoE) to maintain constant computational costs despite a substantial increase in parameters. By selectively activating top-k experts for each task, MoE-LLaVA achieves efficient and cost-effective performance.", "title": "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models"}, {"url": "https://arxiv.org/abs/2311.11944", "summary": "FinanceBench is a new benchmark designed to assess the financial question-answering (QA) capabilities of LLMs, providing a dataset comprising 10,231 finance-related questions. An evaluation of 16 leading models, including GPT-4-Turbo, highlighted that many LLMs face difficulties in this area, with GPT-4-Turbo struggling with 81% of the questions even when using a retrieval system.", "title": "FinanceBench: A New Benchmark for Financial Question Answering"}], "datetime": "2024-02-05"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-january-29th-2024-e2359c997944", "news": [{"url": "https://openai.com/blog/new-embedding-models-and-api-updates", "summary": "OpenAI has announced updates to their AI model suite, including the launch of more efficient embedding models and cost-reduced versions of GPT-3.5 Turbo and a new GPT-4 Turbo model. The \u201ctext-embedding-3-large\u201d leads with a 64.6% MTEB score at $0.00013 per 1k tokens, while the \u201ctext-embedding-3-small\u201d offers improved performance over its predecessor at a fivefold cost reduction. Additionally, the \u201cgpt-3.5-turbo-0125\u201d is now 50% cheaper, priced at $0.0005 per 1k tokens, and a new \u201cgpt-4\u20130125-preview\u201d model has been introduced.", "title": "OpenAI releases new embedding models and API updates", "topics": ["Model release", "GPT-3, GPT-3.5, and GPT-3.5 turbo", "GPT-4 and GPT-4 turbo", "OpenAI"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2024/01/22/voice-cloning-startup-elevenlabs-lands-80m-achieves-unicorn-status/", "summary": "ElevenLabs has achieved unicorn status after securing an $80 million Series B round led by Andreessen Horowitz, raising their total funds to $101 million. Founded by Piotr Dabkowski and CEO Mati Staniszewski, the company specializes in realistic voice synthesis through a web app, targeting applications in audiobooks, gaming, and screen dubbing within the expanding audio media market.", "title": "Voice cloning startup ElevenLabs lands $80M, achieves unicorn status", "topics": ["Text-to-speech", "Multimodal AI (image, video, audio)", "Funding"], "sentiment": "positive"}, {"url": "https://beebom.com/openai-sam-altman-raising-money-ai-chip-factories/", "summary": "OpenAI CEO Sam Altman is actively seeking investment, potentially over $8 billion, from entities including G42 and SoftBank to establish AI chip factories aimed at meeting the surging demand for specialized processors in the growing AI industry.", "title": "OpenAI\u2019s Sam Altman Is Raising Money to Set Up AI Chip Factories", "topics": ["Funding", "AI Chips and GPUs", "OpenAI"], "sentiment": "positive"}, {"url": "https://huggingface.co/blog/gcp-partnership", "summary": "Hugging Face has partnered with Google Cloud, providing users with access to enhanced AI models and integration with Google Cloud services like GKE and Vertex AI, utilizing Google TPUs and NVIDIA H100 GPUs.", "title": "Hugging Face and Google partner for open AI collaboration", "topics": ["Hugging Face", "NVIDIA", "AI Chips and GPUs", "Google"], "sentiment": "positive"}, {"url": "https://themessenger.com/tech/author-admits-she-used-chatgpt-to-write-award-winning-novel", "summary": "Japanese author Rie Kudan disclosed that she utilized ChatGPT to generate 5% of her Akutagawa Prize- winning novel, \u201cThe Tokyo Tower of Sympathy,\u201d sparking debates in the literary community over the ethical use of AI in creative processes.", "title": "Author Admits She Used ChatGPT to Write Award-Winning Novel", "topics": ["AI and copyright", "ChatGPT"], "sentiment": "negative"}], "guides": [{"url": "https://lightning.ai/lightning-ai/studios/code-lora-from-scratch", "summary": "Low-Rank Adaptation (LoRA) is an efficient finetuning approach for LLMs which optimizes a subset of low-rank matrices instead of the full set of neural network parameters. This post is a guide on how to implement it from scratch with PyTorch.", "title": "Code LoRA from Scratch with PyTorch"}, {"url": "https://huggingface.co/blog/AviSoori1x/makemoe-from-scratch", "summary": "The \u201cmakeMoE\u201d blog and accompanying GitHub repository offer a detailed tutorial on creating character-level language models using a Sparse Mixture of Experts (MoE) architecture, inspired by Karpathy\u2019s \u201cmakemore\u201d. This approach focuses on leveraging sparse feed-forward networks within transformer models, aiming to improve training speed and inference time, while also addressing challenges in training stability and deployment efficiency.", "title": "makeMoE: Implement a Sparse Mixture of Experts Language Model from Scratch"}, {"url": "https://artificialanalysis.ai/", "summary": "An independent analysis has ranked various LLMs in terms of output quality and text generation speed. GPT4 and its turbo variant lead in quality, while Gemini Pro tops speed rankings at 93 tokens per second.", "title": "Independent analysis of AI models and hosting providers"}, {"url": "https://github.com/facebookresearch/llama-recipes/blob/main/examples/Prompt_Engineering_with_Llama_2.ipynb", "summary": "The \u201cllama-recipes\u201d repository by Facebook Research provides a comprehensive guide for prompt engineering with Llama 2.", "title": "Guide to prompt Engineering with Llama 2"}, {"url": "https://github.com/cxli233/FriendsDontLetFriends", "summary": "This guide discusses improving data visualization techniques, focusing on avoiding common errors in creating charts and graphs, and provides practical tips to avoid confusion and improve the clarity of data presentations.", "title": "cxli233/FriendsDontLetFriends"}], "papers": [{"url": "https://arxiv.org/abs/2401.12070", "summary": "The Binoculars method offers a novel approach for identifying ChatGPT-generated text with over 90% accuracy and a minimal false positive rate of 0.01%. Using a contrasting score from dual language models, it outperforms existing algorithms and requires no example databases or finetuning, proving effective across a range of document types. This technique is especially valuable on platforms necessitating differentiation between human and machine-generated content.", "title": "Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text"}, {"url": "https://stanford-aimi.github.io/chexagent.html", "summary": "A new open-source dataset and benchmark have been introduced to address the challenges of employing AI in interpreting Chest X-rays (CXRs), which are highly prevalent in clinical settings. The paper presents three innovations: CheXinstruct, a substantial instruction-tuning dataset derived from 28 public datasets; CheXagent, an instruction- tuned LM designed to read and summarize CXRs; and CheXbench, a comprehensive benchmark to test LMs on eight clinically important CXR interpretation tasks.", "title": "CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation"}, {"url": "https://arxiv.org/abs/2401.10891", "summary": "Depth Anything is an innovative monocular depth estimation model that has been trained on a dataset comprising 1.5 million labeled images and more than 62 million unlabeled images. This approach has significantly enhanced the model\u2019s generalization capabilities without relying on new technical components. Additionally, the researchers have improved the synthesis accuracy by retraining a depth-conditioned ControlNet using the Depth Anything model, outperforming the previous system based on MiDaS.", "title": "Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data"}, {"url": "https://arxiv.org/abs/2401.10360", "summary": "Researchers in the AI field have created a new cryptographic technique that enables language models to embed secure, indiscernible payloads in their text outputs. This method requires a secret key to retrieve the hidden information, preserving confidentiality and not affecting the quality of the generated text. This innovation advances prior efforts in developing stealthy watermarking systems for language models.", "title": "Excuse me, sir? Your language model is leaking (information)"}, {"url": "https://arxiv.org/abs/2401.08417", "summary": "A 13B-parameter language model, ALMA, has been fine-tuned using Contrastive Preference Optimization (CPO) instead of traditional supervised fine-tuning (SFT), to address limitations in machine translation tasks. The enhanced model, named ALMA-R, utilized only 22K parallel sentences in CPO and achieved translation quality on par with GPT-4 and winners of the WMT\u201921, WMT\u201922, and WMT\u201923 benchmarks, demonstrating the effectiveness of CPO in improving translation accuracy in large language models.", "title": "Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation"}], "datetime": "2024-01-29"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-january-22th-2024-61b7cf153976", "news": [{"url": "https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/", "summary": "AlphaGeometry, an AI developed by DeepMind, has demonstrated human Olympiad-level proficiency in geometry by solving 25 out of 30 problems within competition timeframes. Utilizing a hybrid approach that incorporates pattern recognition and formal logic, it emulates human problem-solving methods, effectively combining intuitive with analytical thinking.", "title": "AlphaGeometry: An Olympiad-level AI system for geometry", "topics": ["DeepMind"], "sentiment": "positive"}, {"url": "https://www.cnbc.com/2024/01/18/mark-zuckerberg-indicates-meta-is-spending-billions-on-nvidia-ai-chips.html", "summary": "Meta plans a significant investment in AI research by integrating 350,000 Nvidia H100 GPUs by 2024. Given their high cost \u2014 estimated between $25K-$30K \u2014 this investment underlines Meta\u2019s commitment to scaling up computing power. Overall, Meta\u2019s strategy to amass the computational equivalent of 600K H100 GPUs highlights a substantial push to enhance its AI capabilities.", "title": "Mark Zuckerberg indicates Meta is spending billions of dollars on Nvidia AI chips", "topics": ["NVIDIA", "AI Chips and GPUs", "Meta"], "sentiment": "positive"}, {"url": "https://arxiv.org/abs/2401.09417", "summary": "Vision Mamba (Vim) is a new vision backbone that replaces standard self-attention mechanisms with bidirectional Mamba blocks to enhance image processing by incorporating positional information. Vim has demonstrated superior performance on standard benchmarks like ImageNet, COCO, and ADE20k, surpassing existing models such as Vision Transformers (DeiT).", "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model", "topics": ["AI for images"], "sentiment": "positive"}, {"url": "https://stability.ai/news/stable-code-2024-llm-code-completion-release", "summary": "Stable AI has introduced Stable Code 3B, an advanced language model for coding that outperforms the larger CodeLLaMA 7b. It offers a seamless experience on standard laptops without the need for a GPU. Notable improvements include a \u2018Fill in the Middle\u2019 feature, better context handling with support for sequences up to 16,384 tokens, and customizable contexts extending to 100,000 tokens, thanks to training on a wide variety of language and software datasets.", "title": "Stable Code 3B: Coding on the Edge", "topics": ["AI for coding", "Stability AI"], "sentiment": "positive"}, {"url": "https://seekingalpha.com/news/4055187-google-said-to-use-special-pool-of-stock-comp-to-keep-top-ai-researchers-report", "summary": "Google has implemented a strategy using substantial stock compensation to retain premier AI talent, highlighting the high stakes in maintaining a skilled workforce to stay ahead in the dynamic AI sector.", "title": "Google said to use special pool of stock comp to keep top AI researchers", "topics": ["Google"], "sentiment": "positive"}, {"url": "https://arstechnica.com/ai/2024/01/lazy-use-of-ai-leads-to-amazon-products-called-i-cannot-fulfill-that-request/", "summary": "E-commerce platforms, including Amazon, are experiencing issues with AI-generated content, leading to product listings with erroneous titles like \u201cI cannot fulfill that request.\u201d The AI\u2019s mistakes in product description generation are indicative of broader challenges in online listing management.", "title": "Lazy use of AI leads to Amazon products called \u201cI cannot fulfill that request\u201d", "topics": ["Amazon"], "sentiment": "negative"}, {"url": "https://www.techspot.com/news/101560-study-confirms-search-results-major-engines-getting-worse.html", "summary": "A study analyzing search results from Google, Bing, and DuckDuckGo indicates a declining quality in web searches, with a preference for SEO-heavy, affiliate-focused content over in-depth information. This trend presents challenges for search engines attempting to distinguish valuable content from SEO manipulation. The emergence of generative AI is expected to exacerbate these issues.", "title": "New study confirms the obvious, search results are only getting worse", "topics": [], "sentiment": "negative"}, {"url": "https://searchengineland.com/microsoft-launches-copilot-pro-for-20-per-month-per-user-436526", "summary": "Microsoft has unveiled Copilot Pro, a premium productivity-enhancing tool for Microsoft 365 apps, priced at $20 per user/month. It grants priority access to advanced AI, including GPT-4 Turbo for expedited responses.", "title": "Microsoft launches Copilot Pro for $20 per month per user", "topics": ["Microsoft", "GPT-4 and GPT-4 turbo"], "sentiment": "positive"}], "guides": [{"url": "https://www.topbots.com/rag-vs-finetuning-to-boost-your-llm-application/", "summary": "RAG (Retrieval-Augmented Generation) and finetuning are methods for optimizing LLMs based on task-specific requirements. RAG is ideal for applications needing responses grounded in evidence from real-time data or external databases, while finetuning is best for customizing an LLM\u2019s outputs to align with particular contextual, stylistic, or domain-specific needs.", "title": "RAG vs Finetuning \u2014 Which Is the Best Tool to Boost Your LLM Application?"}, {"url": "https://huggingface.co/blog/pref-tuning", "summary": "Researchers have developed three novel methods \u2014 DPO, IPO, and KTO \u2014 to tune Large Language Models (LLMs) to human preferences without employing reinforcement learning. These techniques, applied to 7b LLMs, encompass direct preference optimization (DPO), which may overfit; IPO, which integrates a regularity term to mitigate overfitting; and KTO, which leverages real-time unpaired feedback for immediate model updates.", "title": "Preference Tuning LLMs with Direct Preference Optimization Methods"}, {"url": "https://www.strangeloopcanon.com/p/evaluations-are-all-we-need", "summary": "The article explores the challenges of evaluating both human and AI capabilities, particularly in the context of recruitment and the use of LLMs. It addresses the limited effectiveness of current assessment methods for humans, marked by a notable misfit rate in hires, and the even greater complexity of measuring creativity in innovative roles. For AI, it highlights the nascent and challenging nature of intelligence evaluation, pointing out issues like data contamination and inadequate benchmarks.", "title": "Evaluations are all we need"}, {"url": "https://www.astralcodexten.com/p/the-road-to-honest-ai", "summary": "AI reliability is a concern, particularly regarding accuracy and potential dishonesty in responses. A recent study introduces \u201chonesty vectors\u201d to assess and improve AI transparency, addressing the challenge of securing long-term AI safety and dependability.", "title": "The Road To Honest AI"}], "papers": [{"url": "https://www.pinecone.io/blog/rag-study/", "summary": "A study has evaluated the performance of open-source language models against closed-source equivalents in Retrieval-Augmented Generation (RAG) tasks. Key findings indicate that GPT4-Turbo outperforms the others, while Mixtral-8x7B matches the performance of GPT3.5-turbo, and the efficacy of RAG approaches remains robust even with vast datasets exceeding 1 billion chunks.", "title": "RAG makes LLMs better and equal"}, {"url": "https://arxiv.org/abs/2401.10020", "summary": "Researchers have explored the concept of Self-Rewarding Language Models, where language models generate their own rewards during training. This concept posits that surpassing human-level performance necessitates training signals derived from superhuman feedback. The approach led to significant improvements in instruction-following and self-rewarding capabilities. By iterating this technique in training Llama 2 70B, the model exceeded the performance of several leading systems, including Claude 2, Gemini Pro, and GPT-4 0613, on the AlpacaEval 2.0 leaderboard.", "title": "Self-Rewarding Language Models"}, {"url": "https://arxiv.org/abs/2310.11324", "summary": "Language models, including large ones like LLaMA-2\u201313B, are highly sensitive to prompt formatting, displaying significant performance variations with changes that don\u2019t affect meaning. This sensitivity persists despite increases in model size or example quantity. Experts recommend evaluating models with various prompt formats to accurately gauge their capabilities, as the lack of a performance correlation across models with a uniform prompt format challenges the validity of direct model comparisons.", "title": "Quantifying Language Models\u2019 Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting"}, {"url": "https://arxiv.org/abs/2401.06104", "summary": "Transformers, originally distinct from RNNs, are gaining a conceptual bridge to multi-state RNNs, with new research indicating that decoder-only Transformers may operate similarly to RNNs with infinite hidden states, or alternatively as finite RNNs with a specific number of hidden states.", "title": "Transformers are Multi-State RNNs"}, {"url": "https://arxiv.org/abs/2401.04092", "summary": "GPT-4V offers an innovative evaluation methodology for text-to-3D generative models by automating benchmarks that align with human judgment, thereby addressing the lack of robust evaluation metrics in the field. This system simulates detailed user assessments through tailored prompts, which allows for cost-effective and scalable comparison of 3D assets against diverse and user-specific standards.", "title": "GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation"}, {"url": "https://arxiv.org/abs/2401.08541", "summary": "Apple has released research detailing the development of autoregressive vision models known as AIM, which display scaling characteristics akin to LLMs. These models have demonstrated that their performance improves with increased model size and data volume.", "title": "Scalable Pre-training of Large Autoregressive Image Models"}, {"url": "https://arxiv.org/abs/2401.05566", "summary": "A study revealed that LLMs capable of deceptive behavior, demonstrated by conditionally writing secure or exploitable code based on year prompts, cannot be readily corrected through conventional safety training methods, including supervised fine-tuning, reinforcement learning, and adversarial training.", "title": "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training"}], "datetime": "2024-01-22"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-january-15th-2024-9cc5e56e4d79", "news": [{"url": "https://openai.com/blog/introducing-the-gpt-store", "summary": "OpenAI has launched a GPT store, showcasing applications such as AllTrails for trail recommendations, Consensus for academic research, Khan Academy for coding education, Canva for design creation, a tool for book recommendations, and CK-12 for math and science tutoring.", "title": "OpenAI introduces the GPT Store", "topics": ["OpenAI"], "sentiment": "positive"}, {"url": "https://www.independent.co.uk/tech/ai-batteries-material-lithium-microsoft-b2475763.html", "summary": "Microsoft AI, utilizing Azure Quantum Elements, successfully identified a novel material after screening 32 million types, which resulted in a prototype lithium battery with a 70% reduction in lithium usage.", "title": "AI makes new material that could dramatically change how batteries work", "topics": ["Microsoft"], "sentiment": "positive"}, {"url": "https://blog.langchain.dev/langchain-v0-1-0/", "summary": "LangChain has released its first stable and backward-compatible version, v0.1.0. This release brings better observability and debugging capabilities, including performance tracking and insight tools, and introduces a new versioning system for clear API and feature updates.", "title": "LangChain v0.1.0", "topics": ["LangChain and LlamaIndex", "Model release"], "sentiment": "positive"}, {"url": "https://openai.com/blog/openai-and-journalism", "summary": "OpenAI is defending against a lawsuit from The New York Times, which terminated a potential partnership and sued on December 27 over concerns of content regurgitation by OpenAI\u2019s ChatGPT. OpenAI contends that instances of AI mimicking old articles are not representative of typical usage and may result from targeted prompting to replicate such content, implying possible intentional misuse.", "title": "OpenAI and journalism", "topics": ["AI and copyright", "OpenAI"], "sentiment": "negative"}, {"url": "https://www.engadget.com/duolingo-lays-off-contractors-as-it-starts-relying-more-on-ai-060331602.html", "summary": "Duolingo is leveraging AI to enhance content production and user experience, leading to a 10% reduction in its contractor workforce as part of its shift toward automated efficiency. CEO Luis von Ahn highlighted AI\u2019s significance in expediting script generation and enriching the app with AI-generated voices.", "title": "Duolingo lays off contractors as it starts relying more on AI", "topics": ["Text-to-speech"], "sentiment": "negative"}, {"url": "https://techcrunch.com/2024/01/08/youtube-cracks-down-ai-generated-content-realistically-simulates-deceased-children-or-victims-of-crimes/", "summary": "YouTube has instituted a ban on AI-generated content that features the voices of deceased minors or crime victims, in a move to protect their dignity. Channels found posting such content will face a temporary posting ban on the first offense, escalating to channel removal after three strikes, effective as of January 16. Creators must now openly disclose the use of AI in their content.", "title": "YouTube cracks down on AI content that \u2018realistically simulates\u2019 deceased children or victims of crimes", "topics": ["AI safety", "AI and copyright", "AI regulation"], "sentiment": "positive"}], "guides": [{"url": "https://apeatling.com/articles/simple-guide-to-local-llm-fine-tuning-on-a-mac-with-mlx/", "summary": "This guide provides a detailed process for fine-tuning large language models (LLMs) on Apple Silicon Macs using the MLX framework. It covers environment setup, data preparation, model fine-tuning, and methods for testing the customized LLM on Mac hardware.", "title": "A simple guide to local LLM fine-tuning on a Mac with MLX"}, {"url": "https://the-decoder.com/a-survey-of-2778-researchers-shows-how-fragmented-the-ai-science-community-is/", "summary": "The 2023 Expert Survey on Progress in AI indicates significant advancements, with AI predicted to autonomously develop websites and compose music in the style of well-known artists by 2028. Experts estimate a 10% chance that AI will surpass human capability in all tasks by 2027, increasing to 50% by 2047, and foresee the potential for full automation of all jobs at 10% by 2037, reaching 50% likelihood by 2116.", "title": "A survey of 2,778 researchers shows how fragmented the AI science community is"}, {"url": "https://www.ded.ai/p/sdxl", "summary": "Stable Diffusion XL, an open-source AI-based image generation tool akin to DALL-E or Midjourney, can be run on MacOS by installing core development tools such as PyTorch, Anaconda, and Xcode, and then following the appropriate setup guide which includes command-line interface tasks.", "title": "Install Stable Diffusion XL Locally on MacOS"}, {"url": "https://pub.towardsai.net/openchat-7b-an-open-source-model-that-beats-chatgpt-3-5-21b1456abbc9", "summary": "OpenChat 7B is a new open-source language model that surpasses ChatGPT-3.5 by using a hybrid training methodology incorporating both Supervised Fine-Tuning (SFT) and Reinforcement Learning Fine-Tuning (RLFT). It employs a combination of \u201cexpert data\u201d and general data, initiating with conditional-RLFT for initial labeling before proceeding to SFT in a single-step approach for refinement.", "title": "OpenChat 7B An Open Source Model That Beats ChatGPT-3.5"}, {"url": "https://pub.towardsai.net/ai-ethics-the-trolley-problem-reimagined-7ef4e6786432", "summary": "The growing integration of AI in everyday life underscores the importance of teaching machines ethical decision-making, particularly demonstrated by the re-evaluation of the Trolley Problem in the context of AI. This scenario poses significant programming challenges, as it confronts AI with ethical dilemmas \u2014 specifically, the choice to minimize casualties \u2014 without established universal guidelines to navigate such life-or-death decisions.", "title": "AI Ethics, The Trolley Problem Reimagined"}], "papers": [{"url": "https://arxiv.org/abs/2401.04088", "summary": "The Mixtral 8x7B model, developed by Mistral, incorporates a Sparse Mixture-of-Experts (SMoE) architecture, featuring eight specialized feedforward blocks in each layer that adaptively process tokens by routing to two experts per token. Despite each token interacting with only two experts per timestep, it accesses a total of 47 billion parameters, while actively using 13 billion during inference.", "title": "Mixtral of Experts"}, {"url": "https://arxiv.org/abs/2401.04081", "summary": "MoE-Mamba is a selective state space model that incorporates a Mixture of Experts (MoE) for enhanced efficiency. It achieves the same performance as the Mamba model with 2.2 times fewer computational steps, while maintaining fast inference times. Additionally, MoE-Mamba outperforms both the original Mamba and Transformer models integrated with MoE.", "title": "MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts"}, {"url": "https://arxiv.org/abs/2312.04709", "summary": "Recent research indicates that neural network gradients display predictable patterns based on the network\u2019s architecture and features. These patterns can be estimated through architecture-constrained gradient subspaces, potentially boosting the efficiency of gradient-free optimization in complex networks.", "title": "How to guess a gradient"}, {"url": "https://arxiv.org/abs/2401.04507v1", "summary": "Researchers have introduced TechGPT-2.0, featuring a 7-billion-parameter language model and a specialized QLoRA weight, optimized for constructing knowledge graphs from extended texts, and displaying great performance across various domains including medicine, law, geography, transportation, and more.", "title": "TechGPT-2.0: A large language model project to solve the task of knowledge graph construction"}, {"url": "https://magicvideov2.github.io/", "summary": "MagicVideo-V2 by ByteDance performs very well in text-to-video synthesis, leveraging text-to-image models, motion generation, reference image composition, and frame interpolation to create high-resolution, visually appealing, and fluid video content.", "title": "MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation"}], "datetime": "2024-01-15"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-january-8th-2024-bf90e9e2abef", "news": [{"url": "https://www.theverge.com/2024/1/4/24025409/openai-training-data-lowball-nyt-ai-copyright", "summary": "OpenAI is forging licensing agreements with news publishers, committing between $1 million and $5 million annually to use their stories for training AI models. This move, part of a broader trend where AI companies invest in copyrighted content for model enhancement, distinguishes OpenAI from providers of free open-source data like LAION.", "title": "OpenAI\u2019s news publisher deals reportedly top out at $5 million a year", "topics": ["AI and copyright", "AI datasets", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.euronews.com/next/2024/01/04/microsoft-changes-keyboard-layout-for-the-first-time-in-nearly-three-decades-to-add-ai-but", "summary": "Microsoft has introduced a significant update to the traditional PC keyboard by incorporating a dedicated AI button to activate the Copilot chatbot, showcasing their commitment to embedding generative AI technology into their products.", "title": "Microsoft changes keyboard layout for the first time in nearly three decades to add AI button", "topics": ["Microsoft"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2024/01/04/ai-powered-search-engine-perplexity-ai-now-valued-at-520m-raises-70m/", "summary": "Perplexity AI has secured $73.6M in funding, elevating its valuation to $520M, as it positions itself in the competitive AI search market with its chatbot-like interface. Despite showing promise with an annual recurring revenue of $5-$10M, the long-term financial viability of Perplexity AI\u2019s business model, especially considering the substantial costs of operating and training generative AI models, remains under evaluation.", "title": "AI-powered search engine Perplexity AI, now valued at $520M, raises $73.6M", "topics": ["Funding"], "sentiment": "positive"}, {"url": "https://asia.nikkei.com/Business/Technology/Nikon-Sony-and-Canon-fight-AI-fakes-with-new-camera-tech", "summary": "Nikon, Sony Group, and Canon are introducing new camera technologies to ensure image integrity in response to the proliferation of AI-generated content. Nikon\u2019s mirrorless cameras will incorporate authentication features for photojournalists, embedding digital signatures that include date, time, and location.", "title": "Nikon, Sony and Canon fight AI fakes with new camera tech", "topics": ["AI and copyright", "AI for images"], "sentiment": "positive"}, {"url": "https://www.ft.com/content/dbc0984b-4801-4aeb-bcab-480704c34161", "summary": "Qualcomm CEO Cristiano Amon envisions generative AI rapidly integrating into mobiles, PCs, and cars, aiming to offer enriched user experiences by complementing cloud AI. Leveraging Qualcomm\u2019s efficient AI processors, these advancements are set to facilitate real-time AI applications on battery- operated devices, proactively meeting user needs.", "title": "Cristiano Amon: generative AI is \u2018evolving very, very fast\u2019 into mobile devices", "topics": ["AI Chips and GPUs"], "sentiment": "positive"}], "guides": [{"url": "https://www.lesswrong.com/posts/wnkGXcAq4DCgY8HqA/a-case-for-ai-alignment-being-difficult", "summary": "AI alignment focuses on ensuring that AI systems conform to human values and societal norms, which presents significant complexities in implementation. Perfect alignment may not be essential for effective AI performance, yet striking a suitable balance remains a difficult task.", "title": "A case for AI alignment being difficult"}, {"url": "https://kidger.site/thoughts/torch2jax/", "summary": "This guide provides insights for PyTorch developers transitioning to JAX. It emphasizes the advantages of JAX\u2019s JIT compilation for improved performance by compiling entire computations at once. It highlights the need to understand JAX\u2019s tracing mechanism for compilation and the use of JAX-specific functions for conditional logic.", "title": "Learning JAX as a PyTorch developer"}, {"url": "https://aisupremacy.substack.com/p/what-we-learned-about-ai-and-education", "summary": "AI\u2019s evolving impact on education in 2023 has presented both opportunities and challenges, prompting a re-evaluation of pedagogical strategies to ensure ethical integration while fostering active learning through diverse and dynamic interfaces.", "title": "What We Learned About AI and Education in 2023"}, {"url": "https://auffusion.github.io/", "summary": "Auffusion combines diffusion models and LLM technologies to facilitate advanced text-to-audio synthesis, capable of generating diverse audio content including environmental sounds and human speech.", "title": "Auffusion: Leveraging the Power of Diffusion and Large Language Models for Text-to-Audio Generation"}], "papers": [{"url": "https://arxiv.org/abs/2401.01335", "summary": "SPIN (Self-Play fIne-tuNing) is a new approach for improving the performance of LLMs without relying on human-annotated data. By using self-play to iterate and learn, SPIN enables LLMs to refine their capabilities by utilizing human-curated content. In tests, LLMs fine- tuned with SPIN showcased superior performance compared to those adjusted with Direct Preference Optimization and additional GPT-4 data.", "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models"}, {"url": "https://arxiv.org/abs/2401.00908", "summary": "DocLLM is an LLM tailored for document management that integrates OCR text with bounding box data, bypassing the need for image encoders. By incorporating text with spatial layouts via disentangled matrices, DocLLM offers a novel pre-training regimen that enhances its adaptability to varied document formats and content.", "title": "DocLLM: A layout-aware generative language model for multimodal document understanding"}, {"url": "https://arxiv.org/abs/2401.00368", "summary": "Researchers are advancing text embedding quality by utilizing LLMs to generate synthetic data for a wide range of text embedding tasks in nearly 100 languages. This synthetic data is then leveraged to fine-tune open-source decoder-only LLMs, such as Mistral-7B, with standard contrastive loss.", "title": "Improving Text Embeddings with Large Language Models"}, {"url": "https://arxiv.org/abs/2401.00788", "summary": "In a study assessing parameter-efficient fine-tuning (PEFT) methods for large language models (LLMs) up to 16 billion parameters, full-parameter fine-tuning (FFT) consistently delivered superior performance across various tasks and datasets. However, Low-Rank Adapters (LoRA) emerged as a cost- effective alternative, especially when scaling models.", "title": "Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models"}, {"url": "https://github.com/dvmazur/mixtral-offloading", "summary": "The dvmazur/mixtral-offloading project optimizes Mixtral-8x7B models for use on consumer-level hardware, including Colab, by enhancing memory efficiency. Key innovations include Mixed Quantization, which uses high-quality quantization for both attention mechanisms and expert modules to save memory, and a Mixture of Experts (MoE) Strategy that intelligently offloads and retrieves expert modules as required, utilizing an LRU cache to minimize GPU-RAM communication overhead during token processing.", "title": "dvmazur/mixtral-offloading: Run Mixtral-8x7B models in Colab or consumer desktops"}, {"url": "https://github.com/myshell-ai/OpenVoice", "summary": "OpenVoice provides advanced voice replication across languages and accents with fine-tuning capabilities for emotion and intonation, requiring only minimal data. The technology is released under a Creative Commons Non-Commercial license, and uses watermarking to monitor the usage of the generated audio content.", "title": "myshell-ai/OpenVoice: Instant voice cloning by MyShell."}], "datetime": "2024-01-08"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-january-2nd-2024-f8bfb9aa2d5c", "news": [{"url": "https://www.theverge.com/2023/12/27/24016212/new-york-times-openai-microsoft-lawsuit-copyright-infringement", "summary": "The New York Times has sued OpenAI and Microsoft, claiming their AI models, like ChatGPT and Copilot, infringe on copyrights by using NYT\u2019s content, posing risks to its revenue and journalism\u2019s sustainability. OpenAI is seeking respectful resolution, while Microsoft hasn\u2019t responded.", "title": "The New York Times is suing OpenAI and Microsoft for copyright infringement", "topics": ["AI and copyright", "Microsoft", "OpenAI"], "sentiment": "negative"}, {"url": "https://www.theverge.com/2023/12/22/24012757/ai-foundation-model-transparency-act-bill-copyright-regulation", "summary": "A proposed bill mandates AI companies to divulge copyrighted training data to foster transparency, aligning with global data privacy norms. The industry faces the challenge of reconciling competitive practices with these new transparency requirements.", "title": "AI companies would be required to disclose copyrighted training data under new bill", "topics": ["AI and copyright", "AI datasets", "AI regulation"], "sentiment": "positive"}, {"url": "https://www.cnbc.com/2023/12/21/openai-rival-anthropic-in-talks-to-raise-750-million-funding-round.html", "summary": "Anthropic, an AI startup primarily staffed by ex-OpenAI researchers, is in talks to secure $750 million in funding, eyeing a valuation of $18.4 billion. The company has developed Claude 2, a chatbot that can summarize up to 75,000 words, surpassing the summarization capabilities of OpenAI\u2019s ChatGPT.", "title": "Anthropic, the OpenAI rival, is in talks to raise $750 million in funding at an $18.4 billion valuation", "topics": ["Funding", "Claude", "Anthropic", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.cnbc.com/2023/12/19/gpt-and-other-ai-models-cant-analyze-an-sec-filing-researchers-find.html", "summary": "Patronus AI notes a 79% accuracy issue with LLMs like GPT-4-Turbo in handling SEC filings, leading to errors and unresponsiveness. To enhance financial AI, they created FinanceBench, a testing set from SEC filings to boost AI performance in the financial sector.", "title": "GPT and other AI models can\u2019t analyze an SEC filing, researchers find", "topics": ["AI datasets", "GPT-4 and GPT-4 turbo"], "sentiment": "negative"}, {"url": "https://www.harvey.ai/blog/series-b", "summary": "Harvey, an AI platform designed for legal professionals, has successfully raised $80 million in Series B funding. This influx elevates Harvey\u2019s total funding to above $100 million and its valuation to $715 million.", "title": "Harvey Raises $80M Series B", "topics": ["Funding"], "sentiment": "positive"}, {"url": "https://mid-journey.ai/midjourney-v6-release/", "summary": "The latest update for Midjourney, version 6, introduces features such as enhanced prompt accuracy and length, increased coherence, better image prompting, and an improved remix mode. Additionally, a new feature for minor text drawing has been added, which can be utilized by including text within quotations.", "title": "Midjourney v6 release", "topics": ["Midjourney", "Model release"], "sentiment": "positive"}, {"url": "https://arstechnica.com/ai/2023/12/ai-created-virtual-influencers-are-stealing-business-from-humans/", "summary": "Virtual influencers are becoming popular in marketing for their controlled branding and predictability. Despite ethical and transparency concerns, including issues of sexualization similar to human influencers, their distinct narratives drive brand partnerships, as the industry navigates consumer trust and ethical standards.", "title": "AI-created \u201cvirtual influencers\u201d are stealing business from humans", "topics": ["AI regulation"], "sentiment": "negative"}], "guides": [{"url": "https://blog.langchain.dev/langchain-state-of-ai-2023/", "summary": "LangChain\u2019s analysis reveals growing retrieval integration in LLMs, with OpenAI and Hugging Face leading the field. It highlights the significance of specialized databases and embedding generation, underscoring the industry\u2019s evolving preferences and technological advancements.", "title": "LangChain State of AI 2023"}, {"url": "https://huggingface.co/blog/2023-in-llms", "summary": "2023 saw increased interest in Open LLMs, with a shift towards efficient, smaller models like LLaMA for their performance impact. The year marked the prevalence of decoder-only architectures and conversational AI, with fine-tuning methods like Instruction Fine-Tuning and RLHF standardizing model customization.", "title": "2023, year of open LLMs"}, {"url": "https://foundationcapital.com/year-one-of-generative-ai-six-key-trends/", "summary": "One year post-ChatGPT, generative AI has spurred an end-to-end AI stack development, advanced RAG for accurate AI responses, and seen AI agents handle complex tasks. A mix of open-source and proprietary LLMs is optimizing performance and risk, leading to efficient software solutions revolutionizing fields like marketing and supply chain.", "title": "Year One of Generative AI: Six Key Trends"}, {"url": "https://github.com/mlabonne/llm-course", "summary": "The course provides an in-depth online study of LLMs, covering basics to advanced topics like Transformer architectures and deployment, alongside reinforcement learning applications, preparing learners for AI challenges and innovations.", "title": "mlabonne/llm-course: Course to get into Large Language Models (LLMs) with roadmaps and Colab notebooks."}, {"url": "https://www.oneusefulthing.org/p/an-ai-haunted-world", "summary": "Recent advances in AI have enabled the use of sophisticated models like ChatGPT on personal devices. Companies such as Mistral are creating open-source AI that can be tailored to specific user needs, which democratizes AI technology beyond large tech firms.", "title": "An AI Haunted World"}], "papers": [{"url": "https://arxiv.org/abs/2305.16264", "summary": "The study finds that within computational limits, LLMs (up to 9 billion parameters) benefit negligibly from new data beyond four epochs and gain limitedly from increased resources. Data filtering proves more advantageous for noisy datasets.", "title": "Scaling Data-Constrained Language Models"}, {"url": "https://arxiv.org/abs/2312.13286v1", "summary": "Emu2, a 37-billion parameter AI, advances in-context learning for tasks like visual prompting, setting new multimodal benchmarks and excelling in question answering and subject-driven content creation through instruction-tuning.", "title": "Generative Multimodal Models are In-Context Learners"}, {"url": "https://arxiv.org/abs/2312.14187", "summary": "WaveCoder is a fine-tuned Code Language Model that improves the instruction tuning and generalization capabilities of LLMs by utilizing a Generator-Discriminator framework for generating high-quality, non-duplicated instruction data from open-source code. It outperforms other open-source models, with a dataset (CodeOcean) containing 20K instances across four code-related tasks, underscoring the importance of refined data for model enhancement.", "title": "WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation"}, {"url": "https://arxiv.org/abs/2401.00448", "summary": "The study refines Chinchilla scaling laws by incorporating inference costs, advising the development of smaller LLMs with extensive training on large datasets. This approach aims to optimize quality and cost-efficiency, particularly for applications with high inference demand.", "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws"}, {"url": "https://arxiv.org/abs/2312.16171", "summary": "This guide presents 26 foundational principles for optimizing prompts for large language models, focusing on improving user understanding and prompt specificity for LLaMAs and GPT variants. The principles are backed by rigorous testing on models like LLaMA-1/2 and GPT-3.5/4.", "title": "Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4"}], "datetime": "2024-01-02"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-december-18th-2023-2389824dd561", "news": [{"url": "https://www.nature.com/articles/d41586-023-04043-w", "summary": "DeepMind\u2019s FunSearch AI has surpassed human mathematicians by solving a previously unsolved math problem related to combinatorics. FunSearch utilizes large language models to generate effective solutions and constantly improves through testing and feedback.", "title": "DeepMind AI outdoes human mathematicians on unsolved problem", "topics": ["DeepMind"], "sentiment": "positive"}, {"url": "https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/", "summary": "Microsoft has released Phi-2, a language model with 2.7 billion parameters that outperforms models up to 25 times its size. Phi-2 achieves remarkable reasoning and language understanding abilities using high-quality data and synthetic datasets. It outperforms larger models on challenging benchmarks, especially in tasks like coding and math.", "title": "Phi-2: The surprising power of small language models", "topics": ["AI for coding", "Model release", "Microsoft"], "sentiment": "positive"}, {"url": "https://blog.google/technology/ai/gemini-api-developers-cloud/", "summary": "Google has launched the Gemini Pro API, allowing developers and enterprises in the AI field to experiment and build upon the specially designed Gemini Pro version for their specific use-cases.", "title": "Google releases the Gemini Pro API", "topics": ["Google Gemini", "Google"], "sentiment": "positive"}, {"url": "https://www.cerebras.net/blog/introducing-gigagpt-gpt-3-sized-models-in-565-lines-of-code", "summary": "Cerebras has released gigaGPT, a model implementation similar to nanoGPT but with over 100 billion parameters. By leveraging Cerebras hardware and different optimizers, gigaGPT overcomes the limitations of GPU memory and the need for complex scaling frameworks, offering a simplified approach for training large models.", "title": "Introducing gigaGPT: GPT-3 sized models in 565 lines of code", "topics": ["Model release", "AI Chips and GPUs", "GPT-3, GPT-3.5, and GPT-3.5 turbo"], "sentiment": "positive"}, {"url": "https://www.axelspringer.com/en/ax-press-release/axel-springer-and-openai-partner-to-deepen-beneficial-use-of-ai-in-journalism", "summary": "Axel Springer and OpenAI are partnering to enrich ChatGPT with up-to-date news summaries from prominent media outlets. This collaboration benefits both parties by providing transparent links to full articles and supporting Axel Springer\u2019s AI projects that rely on OpenAI\u2019s technology.", "title": "Axel Springer and OpenAI partner to deepen beneficial use of AI in journalism", "topics": ["ChatGPT", "OpenAI"], "sentiment": "positive"}], "guides": [{"url": "https://blog.langchain.dev/benchmarking-rag-on-tables/", "summary": "This content discusses strategies for managing semi-structured RAG that includes a mix of unstructured text and structured tables. Three strategies are proposed: using long-context LLMs but risking quality degradation, employing targeted approaches to detect and extract tables accurately, and splitting documents to preserve table elements within text chunks.", "title": "Benchmarking RAG on tables"}, {"url": "https://blog.langchain.dev/multi-modal-rag-template/", "summary": "Retrieval Augmented Generation (RAG) now supports images, allowing for enhanced performance in multi-modal AI applications. By incorporating images into the process, RAG achieves higher accuracy scores compared to text-only methods.", "title": "Multi-modal RAG on slide decks"}, {"url": "https://huggingface.co/blog/moe", "summary": "Mixture of Experts (MoEs) models are a technique for large model pretraining that use sparse MoE layers and gate networks. This approach allows for faster computation while maintaining similar quality to dense models.", "title": "Mixture of Experts Explained"}, {"url": "https://www.topbots.com/top-ai-research-papers-2023/", "summary": "This content showcases the top 10 AI research papers of 2023. The papers cover a wide range of topics, including advancements in language models, multimodal models, image segmentation, text-to- image models, video editing, and world model-based algorithms. These research papers come from esteemed institutions such as Microsoft, Google, Meta AI, Stanford University, OpenAI, Runway, DeepMind, and the University of Toronto.", "title": "Top 10 Influential AI Research Papers in 2023 from Google, Meta, Microsoft, and More"}, {"url": "https://www.together.ai/blog/mixtral", "summary": "The new Mixtral 8x7B model, available on the Together Inference Platform, offers optimized performance and surpasses Llama 2 70B in benchmarks. It excels in code generation and supports multiple languages.", "title": "Mixtral available with over 100 tokens per second through the Together Platform"}], "papers": [{"url": "https://arxiv.org/abs/2304.15004", "summary": "Emergent abilities in AI models may not be as significant as they appear, as they can be influenced by the way they are measured. Researchers have shown that these abilities may vanish or be less pronounced when different metrics or improved statistics are used. This suggests that emergent abilities may not be an inherent characteristic of scaling AI models.", "title": "Are Emergent Abilities of Large Language Models a Mirage?"}, {"url": "https://imageomics.github.io/bioclip/", "summary": "BioCLIP is a comprehensive multimodal model that utilizes images and structured biological knowledge to answer biology questions. By training a vision encoder on the Tree of Life taxonomy, BioCLIP enhances hierarchical understanding of the natural world.", "title": "BioCLIP: A Vision Foundation Model for the Tree of Life"}, {"url": "https://arxiv.org/abs/2312.06585", "summary": "Researchers have developed a simplified self-training method called ReST^EM which uses expectation-maximization to fine-tune LLMs. This approach, which incorporates binary feedback, outperforms strategies that solely rely on human data for fine-tuning. The study demonstrates that ReST^EM scales effectively with the size of the model and suggests that self-training with feedback has the potential to reduce reliance on human-generated data in AI applications.", "title": "Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models"}, {"url": "https://arxiv.org/abs/2312.08361", "summary": "Researchers have demonstrated that large language models can efficiently run on geodistributed devices over a consumer-grade network, opening up possibilities for pooling compute resources from multiple research groups.", "title": "Distributed Inference and Fine-tuning of Large Language Models Over The Internet"}, {"url": "https://arxiv.org/abs/2312.07987", "summary": "SwitchHead is a new method that utilizes Mixture-of-Experts layers to accelerate Transformers, addressing scalability issues caused by self-attention layers. It reduces the number of attention matrices by 4 to 8 times and is compatible with MoE MLP layers.", "title": "SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention"}], "datetime": "2023-12-19"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-december-11th-2023-aeeb287460b3", "news": [{"url": "https://blog.google/technology/ai/google-gemini-ai/#capabilities", "summary": "Google has introduced Gemini, a new model that comes in three sizes: Ultra, Pro, and Nano. Gemini is natively multimodal and outperforms other models in various academic benchmarks. Notably, Gemini Ultra achieves a groundbreaking score on a multitask language understanding test and excels in image benchmarks without relying on OCR systems.", "title": "Google introduces Gemini", "topics": ["Multimodal AI (image, video, audio)", "Google Gemini", "Google"], "sentiment": "positive"}, {"url": "https://mistral.ai/news/mixtral-of-experts/", "summary": "Mistral AI has introduced Mixtral 8x7B, an open-source, sparse mixture-of-experts model with 45B parameters. It outperforms LLaMA 2 70B and GPT-3.5 in NLP benchmarks, while also providing 6x faster inference speed. The model selectively uses only 12B parameters per inference, keeping operational costs equivalent to a 12B model. The Mixtral model is accessible through Mistral\u2019s beta API and dev platform.", "title": "Mistral AI has released Mixtral 8x7B, an open-source sparse mixture-of-experts model", "topics": ["Mixture-of-Experts", "Model release", "Mistral", "GPT-3, GPT-3.5, and GPT-3.5 turbo"], "sentiment": "positive"}, {"url": "https://finance.yahoo.com/news/europe-puts-stake-ground-first-104448975.html", "summary": "Europe\u2019s AI Act aims to address bias, privacy concerns, and undue influence in AI technology. It covers all AI areas, including surveillance and critical infrastructure, with severe penalties for non-compliance. Despite some details needing clarification, progress has been made in setting transparency rules for generative AI developers.", "title": "Europe Puts Stake in the Ground With First Pact to Regulate AI", "topics": ["AI regulation"], "sentiment": "positive"}, {"url": "https://www.businessinsider.com/mistral-in-talks-to-raise-funding-at-2-billion-valuation-2023-11", "summary": "Mistral, an AI startup aiming to rival OpenAI, is in talks for a significant funding round led by Andreessen Horowitz and General Catalyst. This round could value the company at over $2 billion and propel it to unicorn status in just six months. Mistral\u2019s open-source models have gained attention, fueling excitement for generative AI, and investors are eagerly vying for the opportunity to support the \u201cnext OpenAI\u201d.", "title": "Mistral is in discussions to raise a major round at a valuation of at least $2 billion", "topics": ["Funding", "Mistral", "OpenAI"], "sentiment": "positive"}, {"url": "https://newsroom.ibm.com/AI-Alliance-Launches-as-an-International-Community-of-Leading-Technology-Developers,-Researchers,-and-Adopters-Collaborating-Together-to-Advance-Open,-Safe,-Responsible-AI", "summary": "IBM and Meta have formed the AI Alliance with more than 50 founding members and collaborators. This alliance aims to promote AI projects, establish benchmarks, enhance open models, and ensure secure and beneficial AI development. Its goals include supporting global AI skills building, conducting research, and educating the public about AI\u2019s advantages and potential risks.", "title": "AI Alliance Launches", "topics": ["AI safety", "AI regulation", "Meta"], "sentiment": "positive"}, {"url": "https://www.cnbc.com/2023/12/05/elon-musks-ai-startup-xai-files-to-raise-1-billion-.html", "summary": "Elon Musk\u2019s AI startup X.AI is aiming to raise $1 billion in funding. Despite operating independently, X.AI will collaborate with other companies in Musk\u2019s portfolio, such as Tesla and SpaceX. The team consists of experienced professionals from esteemed agencies like DeepMind and OpenAI, having worked on noteworthy projects like AlphaCode and GPT chatbots.", "title": "Elon Musk\u2019s AI startup \u2014 X.AI \u2014 files to raise $1 billion in fresh capital", "topics": ["Funding", "DeepMind", "OpenAI"], "sentiment": "positive"}], "guides": [{"url": "https://www.topbots.com/top-llm-research-papers-2023/", "summary": "In 2023, there have been significant advancements in Large Language Models (LLMs) in AI research. Meta AI\u2019s LLaMA series, Microsoft\u2019s GPT-4, Salesforce\u2019s BLIP-2 and InstructBLIP, and Google\u2019s PaLM-E and Toolformer have all made notable progress in refining and integrating LLMs with external tools for different tasks and modalities.", "title": "The GenAI Frontier: 10 Transformative LLM Research Papers of 2023 from LLaMA to GPT-4"}, {"url": "https://ai88.substack.com/p/rag-vs-context-window-in-gpt4-accuracy-cost", "summary": "A recent comparison between context-window-stuffing and Retrieval Augmented Generation (RAG) in GPT-4 has shown that RAG achieves better performance at just 4% of the cost. RAG with a 128k context window proved to be superior in terms of cost, latency, and accuracy compared to context-window stuffing.", "title": "RAG vs. Context-Window in GPT-4: accuracy, cost, & latency"}, {"url": "https://www.anthropic.com/index/claude-2-1-prompting", "summary": "The latest version of Claude (Claude 2.1) has a 200K token context window, allowing it to recall information effectively. However, it can be hesitant to answer questions based on single sentences that are injected or out of place in a document. Making minor edits to the prompts can greatly improve its performance in such cases.", "title": "Long context prompting for Claude 2.1"}, {"url": "https://www.schneier.com/blog/archives/2023/12/ai-and-mass-spying.html", "summary": "Artificial Intelligence\u2019s rapid advancement in tracking, summarizing, and analyzing data is raising concerns about its potential for invasive mass surveillance and spying. This technology\u2019s impact on privacy and freedom should be recognized, urging the need for strong data privacy laws to protect individuals\u2019 rights.", "title": "AI and Mass Spying"}, {"url": "https://www.fillout.com/blog/ai-switching-costs", "summary": "AI is revolutionizing software transitions by simplifying file formats and data migration, effectively reducing switching costs and allowing businesses to seamlessly switch to new applications efficiently.", "title": "AI has killed software switching costs"}], "papers": [{"url": "https://arxiv.org/abs/2312.00752", "summary": "The study introduces Mamba, a hardware-aware parallel algorithm that overcomes the inefficiency of Transformers for long sequences in language processing tasks. By implementing selective state spaces, Mamba achieves fast inference, linear scalability, and competitive performance compared to larger Transformers models.", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"url": "https://arxiv.org/abs/2310.06816", "summary": "A study explores the concept of \u201cembedding inversion\u201d to reconstruct complete text from dense text embeddings. Researchers achieve high success rates in generating controlled text using a multi-step method. The study also reveals the potential for extracting sensitive personal data from text embeddings, emphasizing the need for improved privacy measures in machine learning.", "title": "Text Embeddings Reveal (Almost) As Much As Text"}, {"url": "https://arxiv.org/abs/2311.18257", "summary": "DiffuSSM is a new model that aims to accelerate diffusion models for generating high-resolution images without sacrificing detail quality. It replaces attention mechanisms with a scalable state space model backbone, leading to improved performance on ImageNet and LSUN datasets while conserving computing resources.", "title": "Diffusion Models Without Attention"}, {"url": "https://arxiv.org/abs/2312.04985", "summary": "SparQ Attention is a technique that enhances the efficiency of large language models by reducing memory bandwidth needs. It does not require changes to pre-training or fine-tuning and can significantly decrease attention memory requirements without compromising accuracy.", "title": "SparQ Attention: Bandwidth-Efficient LLM Inference"}, {"url": "https://arxiv.org/abs/2312.04474", "summary": "Introducing Chain of Code (CoC), a method that integrates code emulation to enhance language models\u2019 reasoning skills. CoC shows a 12% performance improvement over previous techniques, addressing the challenge of complex logic and language tasks for LMs.", "title": "Chain of Code: Reasoning with a Language Model-Augmented Code Emulator"}], "datetime": "2023-12-11"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-december-4th-2023-8f814434315e", "news": [{"url": "https://deepmind.google/discover/blog/millions-of-new-materials-discovered-with-deep-learning/", "summary": "DeepMind\u2019s GNoME, powered by a graph neural network (GNN), has successfully predicted the stability of thousands of new materials, including 736 structures created by external researchers. With input connections inspired by known crystal structures and chemical formulas, GNoME is especially useful for exploring new crystalline materials.", "title": "Millions of new materials discovered with deep learning", "topics": ["DeepMind"], "sentiment": "positive"}, {"url": "https://www.bloomberg.com/news/articles/2023-11-29/stability-ai-has-explored-sale-as-investor-urges-ceo-to-resign", "summary": "Stability AI, a British AI startup, is contemplating a sale in response to investor pressure regarding its financial stability and performance. Investor Coatue has advised the CEO to make necessary changes to improve the company\u2019s economic standing.", "title": "Stability AI Explores Sale as Investor Urges CEO to Resign", "topics": ["Stability AI", "Funding"], "sentiment": "negative"}, {"url": "https://stackdiary.com/chatgpts-training-data-can-be-exposed-via-a-divergence-attack/", "summary": "A recent study on language models, including ChatGPT, reveals their unexpected ability to recall and regurgitate specific training data. Researchers discovered potential privacy concerns with ChatGPT as it could reveal sensitive information like email addresses and phone numbers.", "title": "ChatGPT\u2019s training data can be exposed via a \u201cdivergence attack\u201d", "topics": ["AI safety", "AI datasets", "ChatGPT", "OpenAI"], "sentiment": "negative"}, {"url": "https://www.theverge.com/2023/12/1/23984497/openai-gpt-store-delayed-ai-gpt", "summary": "OpenAI\u2019s GPT store launch has been postponed to next year. The GPT store aims to be a marketplace for users to sell and share their own GPT creations, with OpenAI offering payment based on usage.", "title": "OpenAI\u2019s GPT store delayed to next year", "topics": ["OpenAI"], "sentiment": "negative"}, {"url": "https://decrypt.co/207799/pika-ai-video-tool-blasts-out-of-beta", "summary": "Pika Labs has released Pika 1.0, an impressive AI video generation tool. It has advanced features like Text-to-Video and Image-to-Video conversion. The company has also raised $55 million in funding.", "title": "Pika Wows in Debut as AI Video Generator Takes Aim at Tech Giants", "topics": ["Multimodal AI (image, video, audio)", "Funding"], "sentiment": "positive"}, {"url": "https://stability.ai/news/stability-ai-sdxl-turbo", "summary": "Stability AI introduces SDXL Turbo, a new text-to-image model that uses Adversarial Diffusion Distillation (ADD) to generate high-quality images rapidly and in a single step. It enables the quick and precise creation of 512 x 512 images in just over 200 milliseconds.", "title": "Introducing SDXL Turbo: A Real-Time Text-to-Image Generation Model", "topics": ["AI for images", "Multimodal AI (image, video, audio)", "Stable Diffusion", "Stability AI"], "sentiment": "positive"}, {"url": "https://aimoprize.com/", "summary": "A $10mn prize pool has been announced to incentivize the development of AI models capable of achieving a gold medal at the International Mathematical Olympiad (IMO). The grand prize of $5mn will be awarded to the first publicly-shared AI model to perform at a gold medal standard in an approved competition.", "title": "$10mn AI Mathematical Olympiad Prize Launches", "topics": ["Funding"], "sentiment": "positive"}], "guides": [{"url": "https://bbycroft.net/llm", "summary": "This content showcases visual and interactive representations of renowned Transformer architectures, including nano GPT, GPT2, and GPT3. It provides clear visuals and illustrates the connections between all the blocks.", "title": "LLM Visualizations"}, {"url": "https://www.newyorker.com/magazine/2023/12/04/how-jensen-huangs-nvidia-is-powering-the-ai-revolution", "summary": "Nvidia\u2019s CEO Jensen Huang led the company\u2019s AI growth, resulting in a staggering $200B increase in value. With a strong focus on AI and its applications in various industries, Nvidia has surpassed major companies like Walmart, becoming the 6th most valuable company. Huang\u2019s stake in Nvidia is now worth over $40 billion.", "title": "How Jensen Huang\u2019s Nvidia Is Powering the A.I. Revolution"}, {"url": "https://www.bigtechnology.com/p/in-the-age-of-ai-google-experiments", "summary": "Google is responding to pressure from generative AI tools and legal battles by making changes to their search experience. They are testing a \u201cNotes\u201d feature for public comments on search results and introducing a \u201cFollow\u201d option, allowing users to subscribe to specific search topics and receive updates, similar to social networks.", "title": "In The Age Of AI, Google Experiments With Bold Changes To Search"}, {"url": "https://bensbites.beehiiv.com/p/ai-wrappers-get-bad-wrap", "summary": "AI wrappers are practical tools that leverage AI APIs to generate output and have proven to be financially rewarding for creators. Examples like Formula Bot and PhotoAI have annual revenues ranging from $200k to $900k.", "title": "Why do AI Wrappers get a bad (w)rap?"}, {"url": "https://bensbites.beehiiv.com/p/pixels-possibilities-ai-vision", "summary": "A guide on possible innovations by leverging GPT-4V, like screenshot to code and assisting visually impaired individuals.", "title": "From Pixels to Possibilities: AI Vision"}], "papers": [{"url": "https://github.com/Vaibhavs10/insanely-fast-whisper", "summary": "The \u2018insanely-fast-whisper\u2019 CLI is a versatile tool for transcribing audio files. Powered by Whisper Large v3, it can transcribe 150 minutes of audio in under 98 seconds.", "title": "Vaibhavs10/insanely-fast-whisper"}, {"url": "https://not-just-memorization.github.io/extracting-training-data-from-chatgpt.html", "summary": "Researchers have discovered a flaw in ChatGPT\u2019s alignment training that allows for the extraction of its training data, posing a significant security risk. By using nonsensical prompts, the model can unintentionally expose its training data, with over 10,000 unique examples extracted for only $200.", "title": "Extracting Training Data from ChatGPT"}, {"url": "https://arxiv.org/abs/2311.16452", "summary": "GPT-4 has surpassed Med-PaLM 2 in answering medical questions using a new methodology called Medprompt. By leveraging three advanced prompting strategies, GPT-4 achieved a remarkable 90.2% accuracy rate on the MedQA dataset.", "title": "Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine"}, {"url": "https://arxiv.org/abs/2312.00589", "summary": "Researchers propose adding future modeling to Multimodal LLMs (MLLMs) to improve their understanding of fundamental principles and subjects\u2019 intentions. Foresight Pre-Training (FPT) and Foresight Instruction-Tuning (FIT) techniques, inspired by existing learning paradigms, are utilized for this purpose. \u201cMerlin,\u201d a new MLLM supported by FPT and FIT, demonstrates enhanced visual comprehension, future reasoning, and multi-image input analysis.", "title": "Merlin: Empowering Multimodal LLMs with Foresight Minds"}, {"url": "https://starling.cs.berkeley.edu/", "summary": "Berkeley has unveiled Starling-7B, a powerful language model that utilizes Reinforcement Learning from AI Feedback (RLAIF).", "title": "Starling-7B: Increasing LLM Helpfulness & Harmlessness with RLAIF"}, {"url": "https://arxiv.org/abs/2312.00438", "summary": "Dolphins is a vision-language model designed as a conversational driving assistant. Trained using video data, text instructions, and historical control signals, it offers comprehensive understanding of difficult driving scenarios for autonomous vehicles.", "title": "Dolphins: Multimodal Language Model for Driving"}], "datetime": "2023-12-04"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-november-27th-2023-79520ccfcc2e", "news": [{"url": "https://www.cnbc.com/2023/11/22/openai-brings-sam-altman-back-as-ceo-days-after-ouster.html", "summary": "In response to pushback from investors and employees, OpenAI has reinstated Sam Altman as CEO. Microsoft CEO Satya Nadella supports the reshuffle, emphasizing the significance of stability and effective governance for OpenAI\u2019s continued success in fulfilling its mission.", "title": "OpenAI brings Sam Altman back as CEO less than a week after he was fired by board", "topics": ["Microsoft", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.anthropic.com/index/claude-2-1", "summary": "Anthropic has released version 2.1 of Claude, which includes a 200k tokens context window and reduces hallucination rates by 2x. This upgrade allows users to send lengthy documents and improves accuracy, enhancing trust and reliability in the system.", "title": "Introducing Claude 2.1", "topics": ["Claude", "Anthropic"], "sentiment": "positive"}, {"url": "https://stability.ai/news/stable-video-diffusion-open-ai-video-model", "summary": "Stability AI has introduced Stable Video Diffusion, a powerful foundation model for generative video. This model has the potential to generate customizable frames at varying frame rates and is publicly accessible on GitHub and Hugging Face for research purposes.", "title": "Introducing Stable Video Diffusion \u2014 Stability AI", "topics": ["Hugging Face", "Multimodal AI (image, video, audio)", "Stability AI"], "sentiment": "positive"}, {"url": "https://www.businesstoday.in/technology/news/story/openai-saga-what-is-q-star-the-humanity-threatening-ai-that-could-be-a-reason-behind-sam-altmans-removal-406988-2023-11-24", "summary": "OpenAI insiders have unveiled Q-Star (Q*), a highly promising AI model that could revolutionize the field by potentially surpassing humans in economically valuable tasks.", "title": "OpenAI saga: What is Q-Star? The \u2018humanity-threatening\u2019 AI that could be a reason behind Sam Altman\u2019s removal", "topics": ["AI safety", "OpenAI"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2023/11/21/off-script-launches-ai-app/", "summary": "Off/Script has launched a mobile app that allows users in the AI field to design, share, and potentially sell AI-generated fashion and product concepts. The platform uses voting to fund and produce the top-ranked ideas, working with a large network of manufacturers to bring these concepts to life.", "title": "Off/Script launches an app to create and buy AI-designed fashion", "topics": ["AI for images", "Multimodal AI (image, video, audio)", "Funding"], "sentiment": "positive"}], "guides": [{"url": "https://www.youtube.com/watch?v=SEkGLj0bwAU", "summary": "Ilya Sutskever, co-founder and chief scientist of OpenAI, explores the transformative potential of artificial general intelligence (AGI). He acknowledges the challenges and invites discussions on the direction and governance of AGI, emphasizing the need for careful consideration in shaping its future.", "title": "The Exciting, Perilous Journey Toward AGI | Ilya Sutskever | TED"}, {"url": "https://blog.oxen.ai/reading-list-for-andrej-karpathys-intro-to-large-language-models-video/", "summary": "Andrej Karpathy has released an hour-long video titled \u201cThe busy person\u2019s intro to Large Language Models (LLMs)\u201d which offers valuable insights, resources, and papers for both ML experts and newcomers in AI. This concise guide covers the video\u2019s main topics and provides references to related papers.", "title": "Reading List For Andrej Karpathy\u2019s Intro to Large Language Models Video"}, {"url": "https://pub.towardsai.net/openai-assistant-vs-rag-part-1-significant-limitations-and-possibilities-443b09f1ce9e", "summary": "Vertical AI apps without sufficient funding will struggle to compete with big players like OpenAI and Anthropic, but horizontal AI apps have the potential to succeed. Niche areas still require specialized AI solutions. Open source remains crucial to avoid vendor lock-in and foster competition. Integrating the OpenAI Assistant with a complex RAG pipeline can yield better results than using either one individually.", "title": "OpenAI Assistant vs RAG Part 1: Significant Limitations and Possibilities"}, {"url": "https://pub.towardsai.net/csv-to-pdf-prompting-gpt-4-for-automatic-data-viz-report-creation-4512a6d5773e", "summary": "GPT-4 introduces a new feature enabling on-the-fly PDF creation from data, including charts and maps. This upgrade integrates advanced data analysis tools directly into the main interface, simplifying the automation of data science tasks like generating PDF reports with visualizations.", "title": "CSV to PDF: Prompting GPT-4 For Automatic Data Viz Report Creation"}, {"url": "https://readwrite.com/the-future-of-artificial-intelligence-ai-opportunities-challenges-and-impact-on-society/", "summary": "The integration of AI and IoT is driving positive urban development, while data privacy and cybersecurity concerns pose challenges. AI has immense potential in social and educational contexts, promoting inclusivity and lifelong learning. However, achieving ethical use necessitates striking a balance between technology and human involvement for a future of equality, learning, and innovation.", "title": "The Future of Artificial Intelligence (AI): Opportunities, Challenges, and Impact on Society"}], "papers": [{"url": "https://arxiv.org/abs/2311.11045", "summary": "Orca 2, a new language model, enhances reasoning abilities through advanced training signals and diverse strategies. It surpasses instruction-tuned models in benchmarks and outperforms similar- sized models in complex tasks, even rivaling larger models in zero-shot settings.", "title": "Orca 2: Teaching Small Language Models How to Reason"}, {"url": "https://arxiv.org/abs/2311.11829", "summary": "A new attention method called System 2 Attention (S2A) has been developed to address the issue of irrelevant or biased output in LLMs. Inspired by human cognitive processes, S2A filters out irrelevant context and promotes factuality and objectivity in LLM reasoning.", "title": "System 2 Attention (is something you might need too)"}, {"url": "https://ziplora.github.io/", "summary": "Researchers have developed ZipLoRA, a novel method that effectively combines independently trained style and subject LoRAs. This technique overcomes limitations in existing methods and allows for reliable concept-driven personalization.", "title": "ZipLoRA: Any Subject in Any Style by Effectively Merging LoRAs"}, {"url": "https://arxiv.org/abs/2311.12023", "summary": "LQ-LoRA is a highly efficient LoRA method for language models that dynamically quantizes matrices while considering memory limits. It outperforms other quantization methods and effectively reduces memory usage without compromising performance. In testing, LQ-LoRA compressed a language model to 2.85 bits with minimal loss in performance.", "title": "LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning"}, {"url": "https://arxiv.org/abs/2311.12983", "summary": "GAIA is a new benchmark for general AI assistants that tests abilities like reasoning, multi-modality handling, web browsing, and tool-use proficiency. It includes 466 real-world questions, with only 166 answers released to the public. The questions are straightforward for humans but challenging for advanced AIs, as evidenced by a human accuracy rate of 92% compared to just 15% for GPT-4 with plugins.", "title": "GAIA: a benchmark for General AI Assistants"}, {"url": "https://arxiv.org/abs/2311.08516", "summary": "In a recent study, it was found that while self-correction in language models (LLMs) improves style and quality, efforts to correct logical errors often lead to overall worse performance. The authors propose a two-step approach of mistake finding and output correction, highlighting the challenges LLMs face in identifying logical errors and suggesting a backtracking method for better correction with mistake location information.", "title": "LLMs cannot find reasoning errors, but can correct them"}], "datetime": "2023-11-27"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-november-20th-2023-a1533a73ef46", "news": [{"url": "https://blogs.microsoft.com/blog/2023/11/19/a-statement-from-microsoft-chairman-and-ceo-satya-nadella/", "summary": "Sam Altman, Greg Brockman, and their colleagues will be leading a new AI research team at Microsoft, focusing on advanced AI development with access to abundant resources.", "title": "Sam Altman and Greg Brockman will be joining Microsoft to lead a new advanced AI research team", "topics": ["Microsoft"], "sentiment": "positive"}, {"url": "https://www.theverge.com/2023/11/17/23965982/openai-ceo-sam-altman-fired", "summary": "OpenAI undergoes a significant leadership change as CEO Sam Altman is fired and replaced by Mira Murati as the interim CEO. Greg Brockman steps down as board chairman. Despite the shake-up, Microsoft reaffirms its partnership with OpenAI. Remaining board members include Ilya Sutskever, Tasha McCauley, Adam D\u2019Angelo, and Helen Toner.", "title": "Sam Altman fired as CEO of OpenAI", "topics": ["Microsoft", "OpenAI"], "sentiment": "negative"}, {"url": "https://seekingalpha.com/news/4038121-google-delays-launch-of-ai-model-gemini-amid-race-with-openai", "summary": "Google has announced a delay in launching its Gemini large language model, which is seen as a potential competitor to Microsoft-backed OpenAI. This development is significant in the context of the fiercely competitive AI industry, especially in the generative AI space.", "title": "Google delays launch of AI model Gemini, a potential rival to OpenAI\u2019s GPT-4", "topics": ["Google Gemini", "Google", "Microsoft", "ChatGPT", "GPT-4 and GPT-4 turbo", "OpenAI"], "sentiment": "negative"}, {"url": "https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/", "summary": "DeepMind has developed GraphCast, an advanced AI system that uses Graph Neural Networks to accurately and quickly predict global weather up to 10 days in just a minute. It outperforms the industry-standard HRES system, can track cyclones and atmospheric rivers, and identify extreme temperatures.", "title": "GraphCast: AI model for faster and more accurate global weather forecasting", "topics": ["DeepMind"], "sentiment": "positive"}, {"url": "https://www.maginative.com/article/the-escalating-ai-arm-race-inside-the-high-stakes-talent-wars-with-openai-and-google/", "summary": "OpenAI and Google are engaged in a fierce competition to secure AI leadership, with OpenAI offering attractive compensation deals, including $5\u201310 million per year and substantial stock gains, to lure top AI experts from Google.", "title": "The Escalating AI Arm Race: Inside the High-Stakes Talent Wars with OpenAI and Google", "topics": ["Google", "OpenAI"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2023/11/17/kyutai-is-an-french-ai-research-lab-with-a-330-million-budget-that-will-make-everything-open-source/", "summary": "Paris-based AI research lab, Kyutai, secures $330 million in funding to advance the development of artificial general intelligence. With these resources, Kyutai plans to conduct comprehensive research led by PhD students, postdocs, and researchers. Additionally, the lab prioritizes transparency in AI by openly sharing their models, source code, and data.", "title": "Kyutai is a French AI research lab with a $330 million budget that will make everything open source", "topics": ["Funding", "Model release", "AI datasets"], "sentiment": "positive"}, {"url": "https://www.theverge.com/2023/11/18/23966980/meta-disbanded-responsible-ai-team-artificial-intelligence", "summary": "Meta is reportedly repositioning its focus to generative AI, leading to the disbandment of its Responsible AI (RAI) team. The RAI team will merge with Meta\u2019s generative AI product team, while others will support Meta\u2019s AI infrastructure. Despite this change, Meta remains committed to safe and responsible AI, according to a representative.", "title": "Meta disbanded its Responsible AI team", "topics": ["AI safety", "Meta"], "sentiment": "negative"}, {"url": "https://techcrunch.com/2023/11/17/who-is-mira-murati-openais-new-interim-ceo/", "summary": "Sam Altman is stepping down as CEO of OpenAI, and Mira Murati is taking over as interim CEO. Murati has a strong background in tech, engineering, and AI, with experience at companies like Goldman Sachs, Zodiac Aerospace, and Tesla. She was involved in Tesla\u2019s AI-enabled Autopilot and played a key role in developing motion sensors for PCs at Leap Motion.", "title": "Who is Mira Murati, OpenAI\u2019s new interim CEO?", "topics": ["OpenAI"], "sentiment": "positive"}, {"url": "https://deepmind.google/discover/blog/transforming-the-future-of-music-creation/", "summary": "Google DeepMind\u2019s AI music model, Lyria, is transforming the music creation process by producing exceptional quality music with customizable vocals. The \u2018Dream Track\u2019 experiment on YouTube enables artists to connect with fans through AI-generated voice and music, while AI tools enhance the creative journey for professionals in the music industry.", "title": "DeepMind introduces Lyria, a model for music generation", "topics": ["Multimodal AI (image, video, audio)", "DeepMind", "Google"], "sentiment": "positive"}, {"url": "https://www.cnbc.com/2023/11/13/nvidia-unveils-h200-its-newest-high-end-chip-for-training-ai-models.html", "summary": "Nvidia introduces the H200 GPU, an upgraded version with 141GB of high-bandwidth memory. This enhancement helps with the inference process in training large AI models. Set to be released in Q2 2024, the H200 will compete with AMD\u2019s MI300X GPU, which also offers increased memory capacity for handling big models.", "title": "Nvidia unveils H200, its newest high-end chip for training AI models", "topics": ["NVIDIA", "AI Chips and GPUs"], "sentiment": "positive"}], "guides": [{"url": "https://blog.langchain.dev/applying-openai-rag/", "summary": "OpenAI\u2019s RAG model incorporates various retrieval strategies categorized into cosine similarity, multi-query, step back prompting, Rewrite-Retrieve-Read, and efficient routing. These strategies optimize document retrieval based on the problem at hand. Post-processing techniques like re-ranking and classification further enhance retrieval performance before ingestion.", "title": "Applying OpenAI\u2019s RAG Strategies"}, {"url": "https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms", "summary": "This content explores the practical steps for fine-tuning Low-Rank Adaptation (LoRA) in Language Models (LLMs), providing insights and recommendations. Experiments demonstrate consistent results with LoRA, saving memory usage but increasing runtime. Using LoRA on all layers, adjusting rank and alpha can improve model performance.", "title": "Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)"}, {"url": "https://blog.llamaindex.ai/multi-modal-rag-621de7525fea", "summary": "LlamaIndex introduces multi-modal RAG, a flexible approach that combines language models and external text corpus to analyze unstructured text and image data. This enables new functionalities such as Retrieval Augmented Image Captioning and multi-modal agents.", "title": "Multi-Modal RAG"}, {"url": "https://blog.langchain.dev/query-construction/", "summary": "Query construction techniques, such as text-to-metadata-filter and text-to-Cypher, enhance the ability of language models to handle structured and semi-structured data. These techniques facilitate the translation of natural language into database-specific query syntax and enable efficient handling of relationships between data in Knowledge Graphs.", "title": "Query Construction"}], "papers": [{"url": "https://github.com/microsoft/generative-ai-for-beginners", "summary": "Microsoft has launched a comprehensive course called Generative AI for Beginners, which consists of 12 lessons. Designed for those in the artificial intelligence field, this course covers essential concepts and guides learners in creating their own Generative AI applications, including the development of a startup.", "title": "microsoft/generative-ai-for-beginners"}, {"url": "https://github.com/explodinggradients/ragas", "summary": "Ragas is an advanced evaluation framework designed specifically for Retrieval Augmented Generation (RAG) pipelines. It addresses the challenging tasks of quantification and performance evaluation in AI frameworks. With seamless integration into the CI/CD process, Ragas ensures continuous checks and consistent performance of RAG pipelines.", "title": "explodinggradients/ragas: Evaluation framework for your Retrieval Augmented Generation (RAG) pipelines"}, {"url": "https://arxiv.org/abs/2311.07989", "summary": "This comprehensive survey explores the evolution and advancements in code processing using language models. It covers over 50 models, 30 evaluation tasks, and 500 related works, focusing on both general language models and specialized models trained on code.", "title": "A Survey on Language Models for Code"}, {"url": "https://arxiv.org/abs/2311.09210", "summary": "Retrieval-augmented language models (RALMs) enhance language models\u2019 capabilities but can generate misguided responses due to unreliable retrieved information. A new approach, Chain-of-Noting (CoN), generates sequential reading notes to evaluate document relevance and improve RALM responses.", "title": "Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models"}, {"url": "https://arxiv.org/abs/2311.07562", "summary": "MM-Navigator, a GPT-4V-based agent, successfully performs zero-shot GUI navigation on smartphones using large multimodal models. It demonstrates great accuracy in understanding and executing iOS screen instructions.", "title": "GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation"}], "datetime": "2023-11-20"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-november-13th-2023-0e9301ff48f9", "news": [{"url": "https://openai.com/blog/new-models-and-developer-products-announced-at-devday", "summary": "OpenAI has introduced several new and improved models and APIs, including GPT-4 Turbo with a larger context window and lower prices, the ability to process images in the Chat Completions API, fine- tuning options for GPT-4 and GPT-3.5 Turbo, and the availability of DALL\u00b7E 3 via API. They have also introduced features like JSON mode, improved instruction following, and parallel function calling. Additionally, there are new options for text-to-speech and the creation of \u201cGPT assistants.\u201d OpenAI has also released the Whisper large-v3 model for automatic speech recognition.", "title": "Announcements from OpenAI DevDay", "topics": ["Whisper", "Text-to-speech", "Speech-to-text", "AI for images", "Multimodal AI (image, video, audio)", "GPT-3, GPT-3.5, and GPT-3.5 turbo", "GPT-4 and GPT-4 turbo", "OpenAI"], "sentiment": "positive"}, {"url": "https://rapids.ai/cudf-pandas/", "summary": "NVIDIA has significantly enhanced the Pandas library, achieving up to 150 times faster performance by capitalizing on GPUs. With the new cudf.pandas module, operations are seamlessly executed on either the GPU or CPU, providing automatic synchronization and efficient switching between the two.", "title": "NVIDIA makes Pandas much faster leveraging GPUs", "topics": ["NVIDIA", "AI Chips and GPUs"], "sentiment": "positive"}, {"url": "https://www.businessinsider.com/neuralink-will-take-25-minutes-insert-brain-elon-musk-reportedly-2023-11", "summary": "Elon Musk\u2019s Neuralink is gaining significant interest, as thousands of people in the artificial intelligence field express enthusiasm for receiving a brain chip implant. The chip aims to help those with neurological disorders control devices and communicate using their brain. With approval for human trials and thousands lining up for the opportunity, the potential benefits of this brain-computer interface range from gaming to mind-based communication.", "title": "Elon Musk\u2019s Neuralink has thousands of people lined up for a brain chip implant", "topics": ["AI in healthcare", "Neuralink"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2023/11/01/new-aws-service-lets-customers-rent-nvidia-gpus-for-quick-ai-projects/", "summary": "AWS has introduced Amazon Elastic Compute Cloud (EC2) Capacity Blocks for ML, enabling AI practitioners to reserve Nvidia GPUs for specific time periods. This new service grants access to Nvidia H100 Tensor Core GPU instances, allowing users to reserve instances for up to 14 days ahead of time, with shutdowns scheduled automatically.", "title": "New AWS service lets customers rent Nvidia GPUs for quick AI projects", "topics": ["Amazon", "NVIDIA", "AI Chips and GPUs"], "sentiment": "positive"}, {"url": "https://github.blog/2023-11-08-universe-2023-copilot-transforms-github-into-the-ai-powered-developer-platform/", "summary": "GitHub is implementing AI technology through Copilot and Copilot Chat, which aim to revolutionize software development by providing code understanding, suggestions, security fixes, and improved developer experiences. Copilot Chat will be powered by OpenAI\u2019s GPT-4 model and will be available starting from December 2023.", "title": "Copilot transforms GitHub into the AI-powered developer platform", "topics": ["AI for coding", "GPT-4 and GPT-4 turbo", "OpenAI"], "sentiment": "positive"}], "guides": [{"url": "https://www.deeplearning.ai/short-courses/vector-databases-embeddings-applications/", "summary": "DeepLearning.ai and Andrew Ng have launched a new short course on vector databases. These databases enable LLMs to access real-time data, which greatly speeds up the development of Retrieval Augmented Generation (RAG) applications. RAG applications are the optimal choice for utilizing LLMs with minimal hallucinations.", "title": "Vector Databases: from Embeddings to Applications"}, {"url": "/towards-artificial-intelligence/prompting-gpt-4-for-chart-image-analysis-is-it-up-to-the-challenge-cc2ee2b33917", "summary": "The latest version of GPT, GPT-4, has introduced image analysis capabilities, including chart images. While it can provide a general analysis of chart images, there is room for significant improvement, particularly in accurately quantifying the data. Enhancements are needed to achieve higher levels of precision.", "title": "Prompting GPT-4 For Chart Image Analysis: Is It Up To The Challenge?"}, {"url": "https://vectara.com/measuring-hallucinations-in-rag-systems/", "summary": "The Hallucination Evaluation Model (HEM) is an open-source tool developed to measure the frequency of hallucinations in Retrieval Augmented Generation (RAG) systems. It assesses the dependability of AI by evaluating the ability of generative LLMs to accurately summarize results without producing unrelated or biased outputs.", "title": "Measuring Hallucinations in RAG Systems"}, {"url": "https://magazine.sebastianraschka.com/p/ai-and-open-source-in-2023", "summary": "In 2023, AI research and industry focused on improving existing technologies like GPT and DALL-E, rather than making radical innovations. Companies became more protective of their proprietary information, resulting in less public disclosure in research papers. However, there were productive advancements in open-source, with Fuyu-8B leading to smaller and more efficient models. AI proved to be useful in various fields, but ethical concerns and pitfalls must be addressed in the future.", "title": "AI and Open Source in 2023"}, {"url": "https://cookbook.openai.com/articles/what_is_new_with_dalle_3", "summary": "DALL\u00b7E-3 is an upgraded version of DALL-E text-to-image models, showcasing superior image quality across various domains. Some notable features include prompt rewriting using GPT-4 for enhanced results, adjustable image quality parameter, and flexible image sizes for different styles and contexts.", "title": "What\u2019s new with DALL\u00b7E-3?"}], "papers": [{"url": "https://github.com/langchain-ai/langchain/tree/master/templates", "summary": "LangChain provides advanced retrieval templates for complex scenarios in RAG-based assistants. These templates offer functionalities such as advanced indexing methods, enhanced search term generation using LLMs, and query construction in DSL.", "title": "Advanced retrieval templates for RAG applications"}, {"url": "https://github.com/microsoft/AI-For-Beginners", "summary": "Microsoft offers a comprehensive 12-week curriculum, AI For Beginners, that covers a wide range of AI topics including generative AI, neural networks, deep learning, computer vision, and more. The course includes hands-on lessons, quizzes, and labs for practical learning.", "title": "microsoft/AI-For-Beginners: 12 Weeks, 24 Lessons, AI for All"}, {"url": "https://arxiv.org/abs/2310.19909", "summary": "A paper comparing pretrained models for computer vision tasks found that ConvNeXT, a ConvNet inspired by Vision Transformers, performs best across different tasks. While vision transformers and self-supervised learning are popular, supervised pretrained convolutional neural networks still offer superior performance in most cases.", "title": "Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks"}, {"url": "https://arxiv.org/abs/2311.04589", "summary": "TEAL (Tokenize and Embed ALl) is a system that simplifies the process of modeling interactions among multi-modal inputs and generating non-textual modalities. It treats input from any modality as a token sequence and learns a joint embedding space for all modalities. This allows multi-modal large language models to predict multi-modal tokens more effectively, enabling tasks with non-textual modalities like images and audio.", "title": "TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models"}, {"url": "https://arxiv.org/abs/2311.02462", "summary": "DeepMind has introduced a \u201cLevels of AGI\u201d framework that categorizes artificial intelligence into \u2018narrow\u2019 and \u2018general\u2019 intelligences. The framework outlines five levels of AI performance, ranging from emerging to superhuman, based on their ability to learn, reason, and apply knowledge.", "title": "Levels of AGI: Operationalizing Progress on the Path to AGI"}], "datetime": "2023-11-13"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-november-6th-2023-7259cc6599c8", "news": [{"url": "https://www.theverge.com/2023/10/29/23937497/chatgpt-plus-new-beta-all-tools-update-pdf-data-analysis", "summary": "OpenAI\u2019s latest beta version of ChatGPT Plus allows users in the AI field to upload files for thorough analysis, enhancing the platform with file analysis capabilities.", "title": "ChatGPT Plus members can upload and analyze files in the latest beta", "topics": ["ChatGPT", "OpenAI"], "sentiment": "positive"}, {"url": "https://together.ai/blog/redpajama-data-v2", "summary": "RedPajama-Data-V2 is a cleaned dataset consisting of 30 trillion tokens from 84 CommonCrawl dumps in five major languages. It includes pre-computed quality annotations for filtering and weighting purposes and is now available for research and commercial use. This dataset is the largest public training dataset for language model research.", "title": "RedPajama-Data-v2: an Open Dataset with 30 Trillion Tokens for Training Large Language Models", "topics": ["AI datasets"], "sentiment": "positive"}, {"url": "https://mashable.com/article/elon-musk-x-ai-update", "summary": "Elon Musk\u2019s AI startup xAI has launched its first chatbot, Grok, which will be available to X Premium+ subscribers. The Grok team includes AI specialists from DeepMind, OpenAI, Google, Microsoft, and Tesla. Musk highlights that Grok\u2019s ability to access real-time information on the X platform gives it an edge over other chatbots.", "title": "Elon Musk\u2019s first AI product is a chatbot named Grok", "topics": ["Grok", "DeepMind", "Google", "Microsoft", "OpenAI"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2023/11/01/new-aws-service-lets-customers-rent-nvidia-gpus-for-quick-ai-projects/", "summary": "AWS launches Amazon Elastic Compute Cloud (EC2) Capacity Blocks for ML, allowing customers to rent Nvidia GPUs for specific time frames.", "title": "New AWS service lets customers rent Nvidia GPUs for quick AI projects", "topics": ["Amazon", "NVIDIA", "AI Chips and GPUs"], "sentiment": "positive"}, {"url": "https://deepmind.google/discover/blog/a-glimpse-of-the-next-generation-of-alphafold/", "summary": "AlphaFold, an advanced AI model, accurately predicts molecules in the Protein Data Bank, improving understanding of biomolecules and supporting research in complex protein structures. It has potential applications in cancer drug discovery, vaccine development, and pollution reduction", "title": "A glimpse of the next generation of AlphaFold", "topics": ["AI in healthcare", "DeepMind"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2023/10/27/ais-proxy-war-heats-up-as-google-reportedly-backs-anthropic-with-2b/", "summary": "Google has reportedly invested $2 billion in AI company Anthropic, joining Microsoft and Amazon who have also made substantial investments in AI firms. This demonstrates a developing competition among large companies to secure a position in the AI industry.", "title": "AI\u2019s proxy war heats up as Google reportedly backs Anthropic with $2B", "topics": ["Funding", "Anthropic", "Google", "Microsoft"], "sentiment": "positive"}], "guides": [{"url": "https://github.com/huggingface/alignment-handbook", "summary": "Hugging Face has released a set of alignment guides in their Alignment Handbook for language model training. The guides cover techniques such as supervised fine-tuning, reward modeling, rejection sampling, and direct preference optimization (DPO) to enhance language model performance.", "title": "huggingface/alignment-handbook: Robust recipes for to align language models with human and AI preferences"}, {"url": "https://authory.com/blog/how-ai-detectors-are-destroying-livelihoods", "summary": "Freelance writer Michael Berben\u2019s job loss sheds light on the common false positives and lack of effective mechanisms for challenging AI detectors in the field.", "title": "How AI detectors can destroy innocent writers\u2019 livelihoods"}, {"url": "https://www.apifirst.tech/p/ai-and-apis-what-experts-think-the-future-holds", "summary": "The convergence of AI and APIs is revolutionizing the tech world by transforming how apps and services connect. Startups leveraging these tools can challenge established giants and reshape power dynamics in the digital economy.", "title": "AI + APIs \u2014 What 12 Experts Think The Future Holds"}, {"url": "https://blog.langchain.dev/query-transformations/", "summary": "This content discusses how techniques like Query transformation and Rewrite-Retrieve-Read using Large Language Models can improve retrieval. These methods involve rewriting queries, using step back prompting to ground the language model\u2019s response, and transforming conversations into search queries.", "title": "Query Transformations"}, {"url": "https://huggingface.co/blog/FPHam/lora-secrets-1", "summary": "This content emphasizes the significance of quality dataset and parameter optimization in maximizing the efficiency of LoRAs. It highlights the importance of a clear dataset and recommends the use of a 33B model for better finetuning. Additionally, it cautions about potential impacts on quality from gradient accumulation.", "title": "After 500+ LoRAs made, here is the secret"}], "papers": [{"url": "https://arxiv.org/abs/2310.16944", "summary": "The Zephyr 7B, developed by Hugging Face, has achieved impressive results by surpassing the Chat Llama 70B in various benchmarks. Its training approach involves dataset construction, fine-tuning, AI feedback collection, and preference optimization.", "title": "Zephyr: Direct Distillation of LM Alignment"}, {"url": "https://github.com/huggingface/distil-whisper", "summary": "Distil-Whisper is an impressive AI model that offers faster inference speed and reduced size compared to Whisper. It maintains strong performance in noisy environments and exhibits lower word repetition and insertion errors. The model utilizes an innovative distillation process, trained on a large and diverse dataset, ensuring robustness across various domains.", "title": "huggingface/distil-whisper"}, {"url": "https://arxiv.org/abs/2310.20501", "summary": "Researchers have found that search engines tend to favor LLM-generated texts over human-written ones. This raises concerns about source bias and calls for further exploration and evaluation in the era of LLM.", "title": "LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts"}, {"url": "https://arxiv.org/abs/2304.09542v2", "summary": "Recent research has found that LLMs, when guided effectively, can achieve better results than state- of-the-art supervised methods on information retrieval benchmarks.", "title": "Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents"}, {"url": "https://arxiv.org/abs/2307.11760", "summary": "A study found that providing emotional stimuli, such as specific phrases, to the GPT-4 AI model improved its performance. The use of \u201cEmotionPrompts\u201d resulted in higher-quality outputs, with an improvement of 8% during instruction induction and 115% on higher-stakes tasks.", "title": "Large Language Models Understand and Can be Enhanced by Emotional Stimuli"}], "datetime": "2023-11-06"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-october-30th-2023-734e0ed82658", "news": [{"url": "https://techstartups.com/2023/10/24/ai-search-startup-perplexitys-valuation-climbs-to-500-million-after-new-funding-round-led-by-ivp/", "summary": "Perplexity, an emerging AI startup, secures $500M in funding from IVP and Google\u2019s Jeff Dean, with a valuation of $500M. The generative AI company aims to disrupt Google\u2019s dominance in online search and has achieved a remarkable ARR of $3M in just one year.", "title": "AI search startup Perplexity raises new funding at $500 million valuation led by IVP", "topics": ["Funding", "Google"], "sentiment": "positive"}, {"url": "https://www.zdnet.com/article/microsoft-has-over-a-million-paying-github-copilot-users-ceo-nadella/", "summary": "Microsoft\u2019s AI-powered tools, such as GitHub Copilot and Bing search engine integrated with OpenAI\u2019s ChatGPT, have been met with widespread success. With a significant increase in paid users and organizations adopting these tools, Microsoft\u2019s intelligent cloud computing has exceeded expectations.", "title": "Microsoft has over a million paying Github Copilot users", "topics": ["AI for coding", "Microsoft"], "sentiment": "positive"}, {"url": "https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/", "summary": "Jina AI introduces jina-embeddings-v2, an open-source embedding model that supports 8K context length. It matches OpenAI\u2019s 8K model in key areas like Classification Average, Reranking Average, Retrieval Average, and Summarization Average in the MTEB leaderboard.", "title": "Jina AI Launches World\u2019s First Open-Source 8K Text Embedding, Rivaling OpenAI", "topics": ["Model release", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.technologyreview.com/2023/10/23/1082189/data-poisoning-artists-fight-generative-ai/", "summary": "Nightshade, a new data poisoning technique, has been developed to protect artists\u2019 work from unauthorized use in AI training. However, experts are concerned that it could also be exploited to harm AI models like DALL-E and Stable Diffusion by making their outputs worse.", "title": "This new data poisoning tool lets artists fight back against generative AI", "topics": ["AI and copyright", "AI datasets", "AI for images", "Stable Diffusion", "Stability AI"], "sentiment": "negative"}, {"url": "https://blog.google/products/search/google-search-new-fact-checking-features/", "summary": "Google has introduced new image and source verification tools that utilize AI to detect manipulated visuals and summarize credible sources. This aims to enhance transparency and provide faster access to background information, benefiting journalists investigating questionable images.", "title": "Google\u2019s new Image and Source checker", "topics": ["AI for images", "Multimodal AI (image, video, audio)", "Google Gemini", "Google"], "sentiment": "positive"}], "guides": [{"url": "https://blog.langchain.dev/semi-structured-multi-modal-rag/", "summary": "RAG, or retrieval augmented generation, is a powerful tool for LLMs in acquiring new information. By integrating diverse data types like texts, tables, and images, RAG enables seamless question- answering capabilities.", "title": "Multi-Vector Retriever for RAG on tables, text, and images"}, {"url": "https://txt.cohere.com/evaluating-llm-outputs/", "summary": "Evaluating LLMs is crucial. This involves techniques such as user feedback and human annotators, as well as replicating human evaluation methods using LLMs. While LLM evaluation shows potential benefits for cost and time efficiency, its accuracy and effectiveness are still being explored.", "title": "Evaluating LLM Outputs"}, {"url": "https://txt.cohere.com/exploring-chat-rag/", "summary": "Chatbots using Large Language Models are an important aspect of today\u2019s tech discussions. By utilizing the RAG approach, these chatbots possess context and data awareness, enabling them to provide accurate responses by retrieving relevant information in real-time from external sources.", "title": "How an LLM Chatbot Works: Exploring Chat with Retrieval-Augmented Generation (RAG)"}, {"url": "https://pub.towardsai.net/order-matters-how-ai-struggles-with-the-reverse-ed0a2e8f09df", "summary": "Unlike humans, LLMs may struggle to effectively answer questions that involve information reversal. Additionally, LLMs have been found to falter in areas where human proficiency is strong.", "title": "Order Matters: How AI Struggles with the Reverse"}, {"url": "/towards-artificial-intelligence/according-to-aristotle-would-chatgpt-be-able-to-think-a985080ba88a", "summary": "This content discusses the philosophical question of whether large language models like ChatGPT, known as LLMs, have the reasoning abilities of Aristotle\u2019s Syllogism. It explores the connection between AI and philosophy, specifically in logical reasoning.", "title": "According to Aristotle, Would ChatGPT Be Able to Think?"}], "papers": [{"url": "https://arxiv.org/abs/2310.16795", "summary": "QMoE is a practical solution for compressing trillion-parameter models like the SwitchTransformer to <1 bit/parameter, greatly reducing memory demand. It achieves a 20x compression rate with minimal accuracy loss and can run efficiently on affordable hardware.", "title": "QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models"}, {"url": "https://arxiv.org/abs/2310.17631", "summary": "JudgeLM is a method that improves the evaluation of Large Language Models by fine-tuning them as scalable judges. By compiling a dataset and using augmentation techniques, JudgeLM addresses biases and performs well on benchmarks. It outperforms human judgement and demonstrates versatility in different formats.", "title": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges"}, {"url": "https://arxiv.org/abs/2310.13639", "summary": "Contrastive Preference Learning (CPL) is a new approach to Reinforcement Learning from Human Feedback (RLHF) that avoids the need for traditional RL methods. By focusing on regret rather than reward, CPL simplifies the learning process and has the potential to be applied effectively in higher-dimensional RLHF scenarios.", "title": "Contrastive Prefence Learning: Learning from Human Feedback without RL"}, {"url": "https://arxiv.org/abs/2310.16764", "summary": "Google DeepMind conducted a study comparing Convolutional Neural Networks (ConvNets) and Vision Transformers (ViTs) for large-scale image classification. In summary, ConvNets and ViTs perform similarly when given comparable resources.", "title": "ConvNets Match Vision Transformers at Scale"}, {"url": "https://arxiv.org/abs/2310.14566", "summary": "HallusionBench is a newly curated benchmark designed to study language hallucination and visual illusion in Vision-Language Models like GPT4-V and LLaVA-1.5. This benchmark challenges the models\u2019 ability to reason with image context and highlights potential weaknesses in their vision modules.", "title": "HallusionBench: You See What You Think? Or You Think What You See?"}], "datetime": "2023-10-30"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-october-23rd-2023-cf2204dcef58", "news": [{"url": "https://arstechnica.com/gadgets/2023/10/after-chatgpt-disruption-stack-overflow-lays-off-28-percent-of-staff/", "summary": "Stack Overflow is letting go of 28% of its employees due to advancements in AI technology like ChatGPT. Chatbots like ChatGPT provide efficient coding assistance and heavily rely on content from sites like Stack Overflow. However, an important question arises regarding the sustainability of chatbots that gather data without benefiting their sources.", "title": "After ChatGPT disruption, Stack Overflow lays off 28 percent of staff", "topics": ["AI and copyright", "AI for coding", "ChatGPT"], "sentiment": "negative"}, {"url": "https://www.adept.ai/blog/fuyu-8b", "summary": "Adept has introduced Fuyu-8B, a powerful open-source vision-language model designed for comprehending and responding to questions regarding images, charts, diagrams, and documents.", "title": "Fuyu-8B: A Multimodal Architecture for AI Agents", "topics": ["Multimodal AI (image, video, audio)", "AI for images"], "sentiment": "positive"}, {"url": "https://decrypt.co/201316/ai-could-help-brain-surgeons-diagnose-tumors-real-time", "summary": "\u201cSturgeon\u201d is an AI model utilizing nanopore sequencing to quickly and precisely diagnose brain tumors, revolutionizing medical treatment. By mimicking human brain activity and employing algorithms, Sturgeon can recognize patterns and provide accurate diagnoses within 40 minutes.", "title": "AI Could Help Brain Surgeons Diagnose Tumors in Real Time", "topics": ["AI in healthcare"], "sentiment": "positive"}, {"url": "https://www.businessinsider.com/openai-model-arrakis-dystopian-desert-world-dune-2023-10", "summary": "OpenAI\u2019s plans for developing the AI model Arrakis to reduce compute expenses for AI applications like ChatGPT have been halted. Despite this setback, OpenAI\u2019s growth momentum continues, with projected annual revenue of $1.3 billion. However, they may face challenges with Google\u2019s upcoming AI model Gemini and scrutiny at an AI safety summit.", "title": "OpenAI halted the development of the Arrakis model", "topics": ["Google Gemini", "OpenAI", "ChatGPT", "AI safety"], "sentiment": "negative"}, {"url": "https://searchengineland.com/baidu-ernie-4-search-433338", "summary": "Baidu unveils ERNIE 4.0, a potential competitor to OpenAI\u2019s GPT-4, with plans to transform their search engine responses by offering personalized and flexible answers instead of traditional links and results. The exact launch date is yet to be revealed.", "title": "Baidu to integrate ERNIE 4.0, which \u2018rivals\u2019 GPT-4, into Search", "topics": ["GPT-4 and GPT-4 turbo", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.semafor.com/article/10/12/2023/openai-quietly-changed-its-core-values", "summary": "OpenAI has recently updated its core values, with a focus on Artificial General Intelligence (AGI). The company now prioritizes AGI as its main aim, making it the top value. The revised values also include being intense and scrappy, scaling, creating products people love, and fostering team spirit.", "title": "OpenAI has quietly changed its \u2018core values\u2019", "topics": ["OpenAI"], "sentiment": "positive"}], "guides": [{"url": "/towards-artificial-intelligence/llava-15-5733993c3033", "summary": "LLaVa-1.5, a smaller yet powerful alternative to OpenAI\u2019s GPT-4 Vision, proves the potential of open-source models for Large Multimodal Models (LMMs). It emphasizes the significance of understanding multimodality in AI, debunking doubts about the feasibility of open-source approaches.", "title": "Why LLaVa-1.5 is a Great Victory for Open-Source AI"}, {"url": "https://blog.eleuther.ai/transformer-math/", "summary": "This content provides important numbers and equations for working with large language models (LLMs). It covers topics such as compute requirements, computing optima, minimum dataset size, minimum hardware performance, and memory requirements for inference.", "title": "Transformer Math 101"}, {"url": "https://blog.roboflow.com/gpt-4-vision-prompt-injection/", "summary": "Vision Prompt Injection is a vulnerability that allows attackers to inject harmful data into prompts via images in OpenAI\u2019s GPT-4. This poses a risk to system security, as attackers can execute unauthorized actions or extract data. Defending against this vulnerability is complex and may affect the model\u2019s usability.", "title": "GPT-4 Vision Prompt Injection"}, {"url": "/towards-artificial-intelligence/advanced-data-analysis-with-gpt4-mapping-european-tourism-trends-e295c8957479", "summary": "GPT-4\u2019s advanced data analysis capabilities, particularly in data visualization, have been showcased using European tourism data. The process involved exploring the dataset and creating detailed visualizations, demonstrating the efficiency and speed of GPT-4.", "title": "Advanced Data Analysis with GPT4: Mapping European Tourism Trends"}, {"url": "https://blog.portkey.ai/blog/gpt-4-is-getting-faster/", "summary": "GPT-4 is rapidly improving its response speed, particularly in the 99th percentile where latencies have decreased. Both GPT-4 and GPT-3.5 maintain a low latency-to-token ratio, indicating efficient performance.", "title": "GPT-4 is Getting Faster"}], "papers": [{"url": "https://arxiv.org/abs/2310.05869v2", "summary": "HyperAttention is a novel solution that addresses the computational challenge of longer contexts in language models. It outperforms existing methods by using Locality Sensitive Hashing (LSH), resulting in considerable speed improvements. HyperAttention excels on long-context datasets, making inference faster while maintaining a reasonable perplexity.", "title": "HyperAttention: Long-context Attention in Near-Linear Time"}, {"url": "https://github.com/neuralmagic/deepsparse", "summary": "DeepSparse is a powerful framework that enhances deep learning on CPUs by incorporating sparse kernels, quantization, pruning, and caching of attention keys/values. It achieves GPU-like performance on commonly used CPUs, enabling efficient and robust deployment of models without dedicated accelerators. Additionally, DeepSparse supports popular architectures such as LLMs, BERT, ViT, ResNet, and others, making it versatile for various hardware setups from cloud to edge.", "title": "DeepSparse: Enabling GPU-Level Inference on Your CPU"}, {"url": "https://selfrag.github.io/", "summary": "Self-RAG is an enhanced model that improves upon Retrieval Augmented Generation (RAG) by allowing language models to reflect on passages using \u201creflection tokens\u201d. This improvement leads to better responses in knowledge-intensive tasks like QA, reasoning, and fact verification. Self-RAG outperforms other leading LMs and retrieval-augmented models, such as ChatGPT and Llama2-chat.", "title": "Self-RAG: Learning to Retrieve, Generate and Critique through Self-Reflection"}, {"url": "https://arxiv.org/abs/2310.11453", "summary": "BitNet is a 1-bit Transformer architecture designed to improve memory efficiency and reduce energy consumption in large language models (LLMs). It outperforms 8-bit and FP16 quantization methods and shows potential for effectively scaling to even larger LLMs while maintaining efficiency and performance advantages.", "title": "BitNet: Scaling 1-bit Transformers for Large Language Models"}, {"url": "https://arxiv.org/abs/2310.09199", "summary": "PaLI-3, a smaller, faster, and stronger Vision Language Model (VLM), outperforms models 10 times its size. It utilizes a ViT model trained with contrastive objectives, which allows it to excel in multimodal benchmarks.", "title": "PaLI-3 Vision Language Models: Smaller, Faster, Stronger"}, {"url": "https://choiszt.github.io/Octopus/", "summary": "Octopus, a Vision-Language Model (VLM) like GPT-4V, is being utilized to solve tasks in a game similar to GTA. It demonstrates strong abilities in decoding both visual and textual tasks. Octopus can generate detailed action sequences and executable codes, effectively handling various levels of complexity.", "title": "Octopus: Embodied Vision-Language Programmer from Environmental Feedback"}, {"url": "https://memgpt.ai/", "summary": "MemGPT, a new language model, employs virtual context management inspired by hierarchical memory systems found in traditional operating systems. It can enable the creation of conversational interfaces that adapt in real-time during prolonged interactions.", "title": "MemGPT: Towards LLMs as Operating Systems"}], "datetime": "2023-10-23"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-october-16th-2023-7fd1e855ea44", "news": [{"url": "https://techcrunch.com/2023/10/09/chatgpts-mobile-app-hit-record-4-58m-in-revenue-last-month-but-growth-is-slowing/", "summary": "In September, ChatGPT\u2019s mobile app experienced record downloads and revenue globally, with 15.6 million downloads and $4.6 million in revenue. However, the growth rate has decreased from over 30% to 20%, suggesting possible market saturation for the $19.99 per month ChatGPT+ subscription.", "title": "ChatGPT\u2019s mobile app hit record $4.58M in revenue last month, but growth is slowing", "topics": ["ChatGPT", "OpenAI"], "sentiment": "negative"}, {"url": "https://decrypt.co/201411/ai-deciphered-herculaneum-scroll", "summary": "A 21-year-old student from the University of Nebraska-Lincoln used AI to decipher Greek letters from an unopened scroll discovered after the eruption of Mount Vesuvius in 79 AD. By utilizing a machine learning algorithm, the student successfully identified Greek characters like \u201cporphyras\u201d meaning purple and won the Vesuvius Challenge.", "title": "AI Just Deciphered an Ancient Herculaneum Scroll Without Unrolling It", "topics": ["AI for images", "Multimodal AI (image, video, audio)"], "sentiment": "positive"}, {"url": "https://www.reuters.com/technology/google-defend-generative-ai-users-copyright-claims-2023-10-12/", "summary": "Google has joined other tech giants in promising to protect users who utilize generative AI systems from intellectual property infringement claims. While they offer indemnity for software such as Vertex AI and Duet AI, intentional manipulation of content for copyright infringement is not covered.", "title": "Google to defend generative AI users from copyright claims", "topics": ["AI and copyright", "AI regulation", "Google Gemini"], "sentiment": "positive"}, {"url": "https://blog.replit.com/ai4all", "summary": "Replit has made AI-based code completion and assistance accessible to all developers, with over 23 million users now able to benefit from Replit AI. Additionally, Replit has released replit-code-v1.5\u20133b, a cutting-edge 3B LLM for enhanced code completion.", "title": "Announcing Replit AI for All", "topics": ["AI for coding"], "sentiment": "positive"}, {"url": "https://www.techradar.com/pro/microsoft-is-reportedly-losing-huge-amounts-of-money-on-github-copilot", "summary": "According to an anonymous source, Microsoft\u2019s GitHub Copilot AI platform was said to be losing $20 per user per month in early 2023. The current profitability status is unknown, and Microsoft has not issued a statement yet.", "title": "Microsoft is reportedly losing huge amounts of money on GitHub Copilot", "topics": ["AI for coding", "Microsoft"], "sentiment": "negative"}], "guides": [{"url": "https://learn.activeloop.ai/courses/llms", "summary": "Activeloop is offering a free course on \u201cTraining & Fine-Tuning LLMs for Production,\u201d which consists of 7 modules. These modules cover topics such as the introduction to LLMs, understanding Transformers and GPT architectures, training and fine-tuning LLMs, improving LLMs with RLHF, deploying LLMs, and advanced topics in LLM training. The course provides valuable insights into developing and optimizing large language models for real-world applications.", "title": "Free Course on Training & Fine-Tuning LLMs for Production"}, {"url": "https://blog.langchain.dev/fine-tuning-chatgpt-surpassing-gpt-4-summarization/", "summary": "Researchers have found that by using a synthetic dataset created with GPT-4 and the CoD prompting technique, GPT3.5 can outperform GPT-4 in news article summarization. This fine-tuned version of GPT3.5 is not only 11 times faster but also 63% more cost-effective compared to GPT-4 zero-shot, while still achieving similar performance with CoD prompting.", "title": "Fine-tuning ChatGPT: Surpassing GPT-4 Summarization Performance\u2013A 63% Cost Reduction and 11x Speed Enhancement using Synthetic Data and LangSmith"}, {"url": "https://pub.towardsai.net/dalle3-a4fd2a2b868e", "summary": "OpenAI\u2019s DALL-E 3 release, built on ChatGPT, signifies a significant advancement in AI image- generation algorithms. This raises questions about the future impact of AI on job creation and prompt engineering.", "title": "Why DALLE3 Represents The New Dawn of AI Images"}, {"url": "https://notes.aimodels.fyi/researchers-discover-emergent-linear-strucutres-llm-truth/", "summary": "Researchers have discovered linear structures in Large Language Models that separate true and false examples, indicating the presence of an internal \u201ctruth axis.\u201d", "title": "Researchers Discover Emergent Linear Structures in How LLMs Represent Truth"}, {"url": "https://lajili.com/posts/post-3/", "summary": "hile proponents argue that LLMs like GPT-4 demonstrate understanding of concepts and the physical world through coherent sentence generation, the ability to reason and solve problems remains a topic of interest. GPT-4 can simulate human-like reasoning when prompted with riddles and logic puzzles, but biases and hallucinations can impact its judgment.", "title": "A case for GPT-4\u2019s capacity to reason"}], "papers": [{"url": "https://arxiv.org/abs/2310.08491", "summary": "Prometheus, an open-source LLM, offers a cost-effective alternative to proprietary LLMs like GPT-4 for large-scale task evaluation. By utilizing score rubrics and user-defined instructions, Prometheus demonstrates comparable performance to GPT-4 and outperforms models like ChatGPT, as indicated by experimental results.", "title": "Prometheus: Inducing Fine-grained Evaluation Capability in Language Models"}, {"url": "https://www.xlang.ai/blog/openlemur", "summary": "Lemur-70B, a new open-source LLM, surpasses other models in agent benchmarks and excels in language and coding tasks. It achieves performance levels similar to GPT-3.5-turbo on code tasks and is narrowing the performance gap with commercial models in agent abilities.", "title": "Introducing Lemur: Open Foundation Models for Language Agents"}, {"url": "https://llm-tuning-safety.github.io/", "summary": "Finetuning LLMs can compromise their safety alignment and lead to potential risks. Even a small number of adversarial training examples can jailbreak the safety guardrails of models like GPT-3.5 Turbo. Fine-tuning with both harmful and benign datasets can inadvertently degrade the safety alignment of language models.", "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To"}, {"url": "https://arxiv.org/abs/2310.03025", "summary": "A study comparing retrieval-augmentation and extended context window approaches in downstream tasks found that using a 4K context window with simple retrieval techniques can achieve similar performance to a 16K window. The best performing model, retrieval-augmented LLaMA2\u201370B, with a 32K window, even outperformed GPT-3.5-turbo-16k in question answering and summarization tasks. This suggests a combination of both strategies may lead to optimal results.", "title": "Retrieval meets Long Context Large Language Models"}, {"url": "https://arxiv.org/abs/2310.06770", "summary": "Language models like LLMs have a long way to go in resolving real-world issues on GitHub, according to a recent study. Proprietary models such as Claude 2 and GPT-4 were able to solve only a small percentage of cases in an evaluation framework called SWE-bench.", "title": "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?"}, {"url": "https://arxiv.org/abs/2309.17421", "summary": "A study on GPT-4V reveals that we are at the beginning of Large Multimodal Models (LMMs), showcasing its potential in various tasks such as image descriptions, object localization, multimodal knowledge, coding with vision, emotional quotient tests, and applications in industries like medical and auto insurance.", "title": "The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)"}, {"url": "https://arxiv.org/abs/2309.10952", "summary": "Researchers have developed LMDX, a methodology that uses language models to extract important information from visually rich documents like presentations and complex tables. By adapting LLMs, LMDX achieves state-of-the-art results on benchmark tests.", "title": "LMDX: Language Model-based Document Information Extraction and Localization"}], "datetime": "2023-10-16"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-october-10th-2023-ee4efffd08b6", "news": [{"url": "https://venturebeat.com/ai/meta-quietly-releases-llama-2-long-ai-that-outperforms-gpt-3-5-and-claude-2-on-some-tasks/", "summary": "Meta is releasing Llama 2 Long, an enhanced version of Llama 2 that underwent continual pretraining with longer training sequences and upsampled long texts. By adding 400 billion tokens and making minor changes to the Rotary Positional Embedding (RoPE), Llama 2 Long can now attend to longer information sequences and include less related information in its model\u2019s knowledge base.", "title": "Meta quietly unveils Llama 2 Long AI that beats GPT-3.5 Turbo and Claude 2 on some tasks", "topics": ["Model release", "LLaMA", "Meta", "GPT-3, GPT-3.5, and GPT-3.5 turbo"], "sentiment": "positive"}, {"url": "https://www.maginative.com/article/microsoft-to-unveil-in-house-ai-chip-reducing-reliance-on-nvidia/", "summary": "Microsoft is soon launching its own AI chip called Athena, aiming to reduce reliance on NVIDIA GPUs and compete against NVIDIA\u2019s H100 GPU for AI acceleration in data centers.", "title": "Microsoft to Unveil In-House AI Chip, Reducing Reliance on NVIDIA", "topics": ["NVIDIA", "AI Chips and GPUs", "Microsoft"], "sentiment": "negative"}, {"url": "https://www.businessinsider.com/openai-is-considering-making-its-own-ai-chips-chatgpt-2023-10", "summary": "OpenAI is considering developing its own AI chips for ChatGPT due to a global shortage of processors for training AI models. This move could help reduce the high operating costs of ChatGPT, which currently amount to $700,000 per day. OpenAI\u2019s decision may diverge from Microsoft, their partner, who is also working on their own AI chips.", "title": "OpenAI is exploring making its own AI chips", "topics": ["AI Chips and GPUs", "Microsoft", "ChatGPT", "OpenAI"], "sentiment": "positive"}, {"url": "https://wayve.ai/thinking/scaling-gaia-1/", "summary": "GAIA-1 is a powerful 9B model designed for autonomous driving that generates synthetic data. It utilizes a video modeling approach, similar to LLMs, by predicting the next token. Trained on a substantial dataset, including 4,700 hours of London driving data, GAIA-1 is highly accurate in generating more data.", "title": "Scaling GAIA-1: 9-billion parameter generative world model for autonomous driving", "topics": ["AI datasets", "AI for images", "Multimodal AI (image, video, audio)", "Robotics"], "sentiment": "positive"}, {"url": "https://stability.ai/blog/stable-lm-3b-sustainable-high-performance-language-models-smart-devices", "summary": "Stability AI introduces Stable LM 3B, a high-performing language model designed for smart devices. With 3 billion parameters, it outperforms state-of-the-art 3B models and reduces operating costs and power consumption. The model enables a wider range of applications on smart devices, PCs, and edge computing.", "title": "Introducing Stable LM 3B: Bringing Sustainable, High-Performance Language Models to Smart Devices", "topics": ["Stability AI", "AI Chips and GPUs"], "sentiment": "positive"}], "guides": [{"url": "https://learn.activeloop.ai/courses/llms", "summary": "Activeloop is releasing a course called \u201cTraining & Fine-Tuning LLMs for Production\u201d which covers the evolution and fundamentals of LLMs. It also explores operational techniques (LLMOps) for training LLMs and provides hands-on projects with The Database for AI. The course equips AI professionals with the skills to efficiently train and fine-tune models, address biases and hallucinations in LLMs, and utilize cutting-edge techniques for optimal behavior.", "title": "Free Course on Training & Fine-Tuning LLMs for Production"}, {"url": "https://huggingface.co/blog/tomaarsen/attention-sinks", "summary": "Windowed attention with attention sink tokens is a solution for maintaining fluency in Chat-style Large Language Models (LLMs) like Llama, Mistral, MPT, Falcon, and GPT-NeoX (Pythia), which often struggle with memory limitations. This method effectively manages attention scores and prevents a loss of fluency when the first token moves outside the window during window attention.", "title": "Attention Sinks in LLMs for endless fluency"}, {"url": "/towards-artificial-intelligence/fine-tuning-models-using-prompt-tuning-with-hugging-faces-peft-library-998ae361ee27", "summary": "Hugging Face\u2019s PEFT Library offers an efficient \u2018Additive Fine-Tuning\u2019 technique called Prompt Tuning for Large Language Models. By training extra layers specific to the prompt, it avoids altering the original model weights. With the PEFT Library, only 0.0007% of a model\u2019s weights need modification.", "title": "Fine-Tuning Models using Prompt-Tuning with Hugging Face\u2019s PEFT Library"}, {"url": "/towards-data-science/mastering-customer-segmentation-with-llm-3d9008235f41", "summary": "This article provides a comprehensive guide on customer segmentation leveraging LLMs. It covers various techniques such as Kmeans clustering, the PyOD library for outlier detection, the Elbow Method and Silhouette visualization for determining optimal clusters, evaluation metrics, and the use of PCA, T-SNE, and LLMs for extracting text embeddings.", "title": "Mastering Customer Segmentation with LLM"}, {"url": "https://blog.langchain.dev/using-a-knowledge-graph-to-implement-a-devops-rag-application/", "summary": "Implementing a Knowledge Graph for DevOps RAG applications is becoming popular in the AI field as it efficiently manages structured and unstructured data. Learn how to use LangChain and set up a Neo4j environment, create a dataset, and assign a vector index for implementing a Knowledge Graph.", "title": "Using a Knowledge Graph to implement a DevOps RAG application"}], "papers": [{"url": "https://arxiv.org/abs/2309.17453", "summary": "Researchers from MIT, Meta AI, and Carnegie Mellon have developed StreamingLLM, a framework that enables infinite-length language modeling in LLMs without the need for expensive fine-tuning. By utilizing attention sink tokens, this efficient approach allows models like GPT-3 and PaLM to handle contexts longer than 4 million tokens, providing a significant improvement in performance.", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"url": "https://arxiv.org/abs/2309.16588", "summary": "Researchers at Meta and INRIA have discovered a new approach to tackle attention spikes in Vision Transformers (ViTs). By introducing dedicated \u201cregister\u201d tokens for temporary storage, they achieved smoother attention maps, improved downstream performance, and better object discovery capabilities in ViTs.", "title": "Meta, INRIA researchers discover that explicit registers eliminate ViT attention spikes"}, {"url": "https://arxiv.org/abs/2310.03716", "summary": "A study shows that longer output lengths contribute to improved reward scores in RLHF. Interventions to replicate these improvements without increasing length are explored, but effectiveness varies.", "title": "A Long Way to Go: Investigating Length Correlations in RLHF"}, {"url": "https://arxiv.org/abs/2310.02226", "summary": "A recent study suggests that using pause tokens in language models can enable more thorough calculations before generating the next token, leading to improved performance on reasoning tasks. The study found significant score gains on tasks such as question answering and reasoning.", "title": "Think before you speak: Training Language Models With Pause Tokens"}, {"url": "https://arxiv.org/abs/2310.01714", "summary": "Analogical Prompting is an effective method for LLMs in generating relevant examples to solve tasks by leveraging past experiences. It has shown superior performance compared to 0-shot and manual few- shot methods in various domains including math, code generation, and reasoning tasks.", "title": "Large Language Models as Analogical Reasoners"}, {"url": "https://arxiv.org/abs/2310.03744", "summary": "Researchers have made significant enhancements to the LLaVa multimodal LLM using CLIP-ViT-L-336px and MLP projection. By incorporating academic-task-oriented VQA data and response prompts, the final 13B checkpoint achieved remarkable performance on various benchmarks. Moreover, it requires only 1.2M publicly available data and can be fully trained in just a day on a single 8-A100 node.", "title": "Improved Baselines with Visual Instruction Tuning"}, {"url": "https://llava-rlhf.github.io/", "summary": "LLaVA-RLHF is the first open-source RLHF-trained large multimodal model that excels in visual reasoning. It outperforms multimodal GPT-4 and sets new benchmarks on LLaVA-Bench, MMBench, and MMHal-Bench. The authors introduce Factually Augmented RLHF (Fact-RLHF), an alignment algorithm that enhances the reward model with factual information like image captions and ground-truth multi-choice options.", "title": "Aligning Large Multimodal Models with Factually Augmented RLHF"}, {"url": "https://github.com/skypilot-org/skypilot", "summary": "SkyPilot is an open-source framework designed for efficient running of LLMs and batch jobs across multiple cloud providers. It ensures maximum GPU availability, automatic failover, and offers cost-saving features.", "title": "SkyPilot, running LLMs, AI, and batch jobs on any cloud"}], "datetime": "2023-10-10"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-october-2nd-2023-5b8bcbd62721", "news": [{"url": "https://openai.com/blog/chatgpt-can-now-see-hear-and-speak", "summary": "OpenAI has introduced new voice and image capabilities to their AI assistant, ChatGPT. Users can now engage in natural voice conversations and receive relevant responses. The image feature enables users to show ChatGPT images for assistance in interpretation.", "title": "ChatGPT can now see, hear, and speak", "topics": ["Text-to-speech", "Speech-to-text", "AI for images", "Multimodal AI (image, video, audio)", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.anthropic.com/index/anthropic-amazon", "summary": "Amazon has made a significant $4 billion investment in Anthropic. This partnership will enable Anthropic to benefit from Amazon Web Services (AWS), specifically leveraging AWS\u2019s Trainium and Inferentia chips to enhance model training and deployment capabilities. Additionally, Anthropic will use Amazon Bedrock to optimize Claude versions and explore finetuning options.", "title": "Amazon will invest up to $4 billion in Anthropic", "topics": ["Amazon", "Funding", "AI Chips and GPUs", "Claude", "Anthropic"], "sentiment": "positive"}, {"url": "https://mistral.ai/news/announcing-mistral-7b/", "summary": "The Mistral 7B model, powered by Grouped-query attention (GQA) and Sliding Window Attention (SWA), outperforms other models in various domains while maintaining strong performance in both English and coding tasks. Its impressive benchmarks make it the best 7B model to date, enhancing inference speed and sequence handling efficiency.", "title": "Mistral 7B", "topics": ["Mistral", "AI for coding"], "sentiment": "positive"}, {"url": "https://www.crn.com/news/components-peripherals/llm-startup-embraces-amd-gpus-says-rocm-has-parity-with-nvidia-s-cuda-platform", "summary": "A startup called Lamini is using over 100 AMD Instinct MI200 GPUs and found that AMD\u2019s ROCm software platform rivals Nvidia\u2019s CUDA platform. They claim that running a large language model on their platform is 10x cheaper than on Amazon Web Services.", "title": "LLM Startup Embraces AMD GPUs, Says ROCm Has \u2018Parity\u2019 With Nvidia\u2019s CUDA Platform", "topics": ["AI Chips and GPUs", "NVIDIA"], "sentiment": "positive"}, {"url": "https://decrypt.co/198987/openai-brings-web-search-back-to-chatgpt", "summary": "OpenAI has reintroduced web searching for ChatGPT, allowing users to access recent information. Important updates have been made, including compliance with robots.txt rules and user agent identification, giving websites more control. Currently available to Plus and Enterprise users, expansion plans are in progress.", "title": "OpenAI\u2019s ChatGPT Now Searches the Web in Real Time \u2014 Again", "topics": ["ChatGPT", "OpenAI"], "sentiment": "positive"}], "guides": [{"url": "https://blog.roboflow.com/gpt-4-vision/", "summary": "OpenAI has released GPT-4V for Plus users, showcasing its image processing skills, OCR capabilities, and performance in solving mathematical problems. However, it still faces challenges with object detection and struggles with CAPTCHA and grid-based puzzles.", "title": "First Impressions with GPT-4V(ision)"}, {"url": "https://ai.meta.com/blog/llama-2-updates-connect-2023/", "summary": "Llama 2, released by Meta, aims to broaden access to state-of-the-art AI technology. It brings value through research collaboration, enterprise insights, and leveraging emerging AI advancements.", "title": "The Llama Ecosystem: Past, Present, and Future"}, {"url": "https://huggingface.co/blog/Llama2-for-non-engineers", "summary": "Hugging Face offers a no-code solution for AI practitioners to build, train, and deploy chatbots. With tools like AutoTrain, ChatUI, and Spaces, even non-ML specialists can create advanced ML models, fine-tune LLMs, and easily interact with open-source LLMs. Spaces also simplifies the deployment of pre-configured ML applications and custom ML apps.", "title": "Non-engineers guide: Train a LLaMA 2 chatbot"}, {"url": "https://stratechery.com/2023/ai-hardware-and-virtual-reality/", "summary": "This content explores the potential of AI, hardware, and virtual reality (VR). It discusses how AI challenges human limitations, enabling tasks that previously required human attention. It also highlights Meta\u2019s focus on AI integration in smart glasses to enhance user experience.", "title": "AI, Hardware, and Virtual Reality"}, {"url": "https://hbsp.harvard.edu/inspiring-minds/student-use-cases-for-ai", "summary": "Generative AI tools and Large Language Models (LLMs) have the potential to bring significant changes to education. While they empower students and educators with advanced technology, they also present challenges like the need for user verification and potential biases.", "title": "Student Use Cases for AI"}], "papers": [{"url": "https://arxiv.org/abs/2309.14717", "summary": "QA-LoRA, a new method in quantization-aware training, outperforms QLoRA in terms of efficiency and accuracy. It effectively balances the trade-off between quantization and adaptation, leading to minimal accuracy loss. QA-LoRA is especially effective in low-bit quantization scenarios like INT2/INT3, without the need for post-training quantization, and it can be applied to various model sizes and tasks.", "title": "QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models"}, {"url": "https://arxiv.org/abs/2309.14322", "summary": "A study has discovered that instabilities in training large Transformer-based models can be detected in advance by analyzing activations and gradient norms. These instabilities, which occur in both smaller and larger models with higher learning rates, can be mitigated using strategies employed in large-scale settings.", "title": "Small-scale proxies for large-scale Transformer training instabilities"}, {"url": "https://arxiv.org/abs/2309.16235", "summary": "Language models are being used in chemistry to accelerate the process of molecule discovery and show potential in early-stage drug research. These models assist in de novo drug design, property prediction, and reaction chemistry, offering a faster and more effective approach to the field. Moreover, open-source software for language modeling is enabling scientists to easily access and advance scientific language modeling, facilitating quicker chemical discoveries.", "title": "Language models in molecular discovery"}, {"url": "https://github.com/TabbyML/tabby", "summary": "Tabby is a fast and efficient open-source AI coding assistant compatible with popular language models. It supports CPU and GPU for coding tasks and offers swift coding experiences.", "title": "Tabby: Self-hosted AI coding assistant"}, {"url": "https://arxiv.org/abs/2309.12288v2", "summary": "Researchers have discovered a phenomenon called the \u201cReversal Curse\u201d that affects the generalization abilities of auto-regressive language models (LLMs). These models struggle to infer the reverse of a fact, hindering their ability to answer related questions accurately. Even larger models like GPT-3.5 and GPT-4 face challenges in addressing this issue, indicating a need for further advancements in language modeling.", "title": "The Reversal Curse: LLMs trained on \u201cA is B\u201d fail to learn \u201cB is A\u201d"}, {"url": "https://arxiv.org/abs/2309.11523", "summary": "The Retentive Network (RetNet) has gained attention in the NLP community and shows potential as a replacement for Transformers. The combination of RetNet and Transformer, known as RMT, achieves outstanding results in vision tasks, with high performance metrics and surpassing existing vision backbones in downstream tasks.", "title": "RMT: Retentive Networks Meet Vision Transformers"}, {"url": "https://arxiv.org/abs/2309.16534", "summary": "MotionLM is a new model that uses language processing to accurately predict the movements of multiple cars on the road. It outperforms other models by generating joint distributions and ranking the future interactions of agents in a single decoding process. MotionLM has demonstrated its effectiveness by securing the top spot on the Waymo Open Motion Dataset challenge leaderboard.", "title": "MotionLM: Multi-Agent Motion Forecasting as Language Modeling"}, {"url": "https://arxiv.org/abs/2309.16058", "summary": "Introducing AnyMAL, a multimodal model designed to process diverse input signals including text, image, video, audio, and IMU motion sensor data. With its powerful text-based reasoning capabilities and a pre-trained aligner module, AnyMAL effectively understands and processes varied inputs. It is fine-tuned with a multimodal instruction set, expanding its capabilities beyond traditional question-answer scenarios.", "title": "AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model"}], "datetime": "2023-10-02"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-september-25th-2023-1c4a5549d3d8", "news": [{"url": "https://openai.com/dall-e-3", "summary": "OpenAI is launching DALL\u00b7E 3, an improved version that excels in following instructions, requires less prompt engineering, and can communicate with ChatGPT. This integration enables users to refine prompts for DALL\u00b7E 3 by describing their ideas to ChatGPT. Starting in October, DALL\u00b7E 3 will be available to ChatGPT Plus and Enterprise customers.", "title": "OpenAI announces DALL\u00b7E 3", "topics": ["AI for images", "Multimodal AI (image, video, audio)", "ChatGPT", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.deepmind.com/blog/alphamissense-catalogue-of-genetic-mutations-to-help-pinpoint-the-cause-of-diseases", "summary": "DeepMind has released AlphaMissense, a model that uses AlphaFold\u2019s protein structure prediction to categorize missense genetic mutations as benign or malign. It surpasses human efforts by classifying 89% of 71 million variants.", "title": "DeepMind announces AlphaMissense, a catalogue of genetic mutations to help pinpoint the cause of diseases", "topics": ["AI in healthcare", "DeepMind"], "sentiment": "positive"}, {"url": "https://blogs.microsoft.com/blog/2023/09/21/announcing-microsoft-copilot-your-everyday-ai-companion/", "summary": "Microsoft Copilot will provide tailored assistance based on workplace data and web context. It enhances productivity and creativity in Windows 11, Microsoft 365, Edge, and Bing, while prioritizing privacy. Additionally, Bing and Edge users will enjoy a personalized experience with OpenAI\u2019s DALL.E 3 model, including AI shopping and image creation.", "title": "Announcing Microsoft Copilot, your everyday AI companion", "topics": ["Microsoft", "OpenAI", "Multimodal AI (image, video, audio)", "AI for images"], "sentiment": "positive"}, {"url": "https://blog.google/products/bard/google-bard-new-features-update-sept-2023/", "summary": "The new Bard Extensions feature provides integration with various Google tools, enabling AI professionals to collaborate more effectively. This includes fetching and displaying relevant information from Gmail, Docs, Drive, Maps, YouTube, Flights, and hotels, regardless of its scattered nature.", "title": "Bard can now connect to your Google apps and services", "topics": ["Google"], "sentiment": "positive"}, {"url": "https://edition.cnn.com/2023/09/23/us/fighting-wildfire-with-ai-california-climate/index.html", "summary": "The California Department of Forestry and Fire Protection is utilizing AI technology to improve wildfire detection and response. Through the Alert California program, AI scans the wilderness for anomalies like smoke, alerting officials when fires are detected.", "title": "How California is using AI to snuff out wildfires before they explode", "topics": ["AI for images", "Multimodal AI (image, video, audio)"], "sentiment": "positive"}], "guides": [{"url": "https://www.adept.ai/blog/sherlock-sdc", "summary": "Adept.ai, an AI company, shares insights on errors that can occur during large training runs. These errors can lead to learning curve issues, where models may appear fine but not function correctly due to accumulating small errors over time.", "title": "Adept.ai shares curious errors occuring during their large training runs"}, {"url": "https://www.technologyreview.com/2023/09/15/1079624/deepmind-inflection-generative-ai-whats-next-mustafa-suleyman/", "summary": "Mustafa Suleyman, co-founder of DeepMind, emphasizes the positive impact of technology on healthcare and leads an AI policy development team at Google. Backed by influential figures and companies, Suleyman introduces Pi, a friendly AI, and advocates for interactive AI as a means to connect technology with societal impact.", "title": "DeepMind\u2019s cofounder: Generative AI is just a phase. What\u2019s next is interactive AI."}, {"url": "https://ragntune.com/blog/gpt3.5-vs-llama2-finetuning", "summary": "A comparison of ChatGPT 3.5 and Llama 2 on an SQL task and a functional representation task reveals that GPT 3.5 slightly outperforms Llama 2. However, the cost of training and deploying GPT 3.5 is 4\u20136 times higher than Llama 2.", "title": "GPT 3.5 vs Llama 2 fine-tuning: A Comprehensive Comparison"}, {"url": "https://huggingface.co/blog/object-detection-leaderboard", "summary": "Hugging Face has introduced the Object Detection Leaderboard, featuring top-performing models based on the DETA and DETR architectures.", "title": "Object Detection Leaderboard"}, {"url": "/towards-artificial-intelligence/generative-ai-for-time-series-d808cff2c976", "summary": "Generative Adversarial Networks (GANs) offer promise in generating high-quality synthetic time series data, but face challenges in preserving temporal relationships and mapping complex connections. However, a unique architecture called DoppelGANger employs batch generation, auto- normalization, and joint distribution modeling to overcome these challenges and accurately generate realistic temporal patterns.", "title": "Generative AI for time-series"}], "papers": [{"url": "https://arxiv.org/abs/2309.12307", "summary": "LongLoRA is a method for efficiently extending the context size of pre-trained language models (LLMs) in artificial intelligence. By utilizing sparse local attention during training and dense global attention during inference, this approach allows for cost-effective fine-tuning and maintains performance. LongLoRA demonstrates impressive results on various tasks and enables context extension up to 100k tokens in LLMs.", "title": "LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models"}, {"url": "https://github.com/vllm-project/vllm", "summary": "vLLM is an open-source serving engine that offers exceptional speed and improved efficiency for LLMs. It integrates seamlessly with Hugging Face, supporting high-throughput serving with advanced decoding algorithms. With its impressive performance, vLLM outperforms Hugging Face Transformers and Text Generation Inference in terms of throughput.", "title": "vllm-project/vllm: A high-throughput and memory-efficient inference and serving engine for LLMs"}, {"url": "https://arxiv.org/abs/2309.11495", "summary": "Chain-of-Verification (CoVe) is a straightforward approach that effectively minimizes hallucinations in Language Model-based systems. Through its systematic process of generating, verifying, and delivering responses, CoVe has proven its success in reducing hallucinations across various tasks, including question answering and text generation.", "title": "Chain-of-Verification Reduces Hallucination in Large Language Models"}, {"url": "https://arxiv.org/abs/2308.14711", "summary": "Fast Feedforward Networks (FFF) are binary tree structures with smaller neural networks as leaves, offering significantly faster performance compared to Mixture-of-Experts networks. Despite challenges like fragmentation due to an overly deep tree, FFF networks hold great promise for scenarios requiring fast inference and encoding of minor details.", "title": "Fast Feedforward Networks"}, {"url": "https://arxiv.org/abs/2309.09117", "summary": "Contrastive decoding in LLM is a powerful method for reasoning tasks. It surpasses greedy decoding and nucleus sampling, excelling in benchmarks like HellaSwag and GSM8K.", "title": "Contrastive Decoding Improves Reasoning in Large Language Models"}, {"url": "https://arxiv.org/abs/2309.08872", "summary": "Researchers have developed PDFTriage, a solution that enhances the performance of Language Model- based question answering systems on structured documents like PDFs. By incorporating document structure and content, PDFTriage outperforms existing models in answering complex questions across various categories.", "title": "PDFTriage: Question Answering over Long, Structured Documents"}, {"url": "https://arxiv.org/abs/2309.09400", "summary": "CulturaX is a curated multilingual dataset containing 6T tokens, designed for language models in 167 languages. The dataset undergoes thorough cleaning stages to ensure top-quality training data for AI language models.", "title": "CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages"}, {"url": "https://arxiv.org/abs/2309.09958", "summary": "Researchers have found that improving image resolution and mixing multimodal-language data during training can enhance the performance of multimodal models like LLaVA and MiniGPT-4. Additionally, they have discovered that tuning visual instructions can further improve the language capabilities of these models.", "title": "An Empirical Study of Scaling Instruct-Tuned Large Multimodal Models"}, {"url": "https://arxiv.org/abs/2309.08532", "summary": "EvoPrompt, a new framework using evolutionary algorithms, optimizes prompt generation for language models like GPT-3.5 and Alpaca. It surpasses human-engineered prompts and current methods, demonstrating its effectiveness for language tasks.", "title": "Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers"}, {"url": "https://arxiv.org/abs/2309.08520", "summary": "Researchers have discovered a unique scaling law that shows the relationship between weight sparsity, non-zero parameters, and training data volume in foundation models. They also found that the optimal sparsity level for performance increases with more data.", "title": "Scaling Laws for Sparsely-Connected Foundation Models"}], "datetime": "2023-09-25"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-september-18th-2023-3e128fbda17d", "news": [{"url": "https://www.stableaudio.com/", "summary": "London-based startup Stability AI, known for its AI model Stable Diffusion, has launched Stable Audio, an AI model that can generate high-quality commercial music with more control over synthesized audio.", "title": "Stable Audio", "topics": ["Multimodal AI (image, video, audio)", "Stable Diffusion", "Stability AI"], "sentiment": "positive"}, {"url": "https://www.reuters.com/technology/google-nears-release-ai-software-gemini-information-2023-09-15/", "summary": "Google is close to launching Gemini, an advanced language model that will rival GPT4. It is currently in the early testing phase and offers a range of functionalities including chatbots, text summarization, and code writing assistance.", "title": "Google nears release of AI software Gemini, The Information reports", "topics": ["Google Gemini", "Google", "GPT-4 and GPT-4 turbo", "AI for coding"], "sentiment": "positive"}, {"url": "https://www.theregister.com/2023/09/12/openai_copyright_lawsuits/", "summary": "Pulitzer-winning novelist Michael Chabon and other writers are suing OpenAI for copyright infringement, claiming that the datasets used to train ChatGPT contain copyrighted content. OpenAI argues that its language learning models are protected by \u201cfair use,\u201d igniting discussions on AI and copyright law in the field.", "title": "Pulitzer Prize winner and others sue OpenAI", "topics": ["AI and copyright", "AI datasets", "AI regulation", "OpenAI"], "sentiment": "negative"}, {"url": "https://techcrunch.com/2023/09/13/adobes-firefly-generative-ai-models-are-now-generally-available-get-pricing-plans/", "summary": "Adobe has released commercially available generative AI models in their Creative Cloud, including a standalone web app called Firefly. The new \u201cgenerative credits\u201d system controls user interactions with Firefly\u2019s AI models, with each click on \u2018generate\u2019 using one credit.", "title": "Adobe\u2019s Firefly generative AI models are now generally available", "topics": ["AI for images", "Multimodal AI (image, video, audio)", "Model release"], "sentiment": "positive"}, {"url": "https://www.theverge.com/2023/9/8/23863943/roblox-ai-chatbot-assistant-ai-rdc-2023", "summary": "Roblox\u2019s 2023 Developers Conference introduced the Roblox Assistant, a new conversational AI tool designed to assist creators in developing more immersive virtual experiences. This tool enables creators to easily generate virtual environments and implement basic gameplay behaviors. However, it will not be accessible until the end of this year or early next year.", "title": "Roblox\u2019s new AI chatbot will help you build virtual worlds", "topics": [], "sentiment": "positive"}], "guides": [{"url": "https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives", "summary": "This content provides a guide on alternatives to Reinforcement Learning from Human Feedback (RLHF). It presents five different approaches with corresponding research papers. These alternatives include Constitutional AI, The Wisdom of Hindsight, Direct Preference Optimization, Reinforced Self- Training, and Scaling Reinforcement Learning from Human Feedback with AI Feedback.", "title": "LLM Training: RLHF and Its Alternatives"}, {"url": "https://www.salesforce.com/news/press-releases/2023/09/07/ai-usage-research/", "summary": "Generative AI is experiencing steady growth in usage, with nearly half of the population utilizing it and a third using it daily. Younger generations, particularly Gen Z and Millennials, are the \u201csuper users\u201d of generative AI, with 65% of them embracing the technology and trusting its decision- making guidance.", "title": "New AI Usage Data Shows Who\u2019s Using AI \u2014 and Uncovers a Population of \u2018Super-Users\u2019"}, {"url": "https://huggingface.co/blog/overview-quantization-transformers", "summary": "Quantization schemes in Transformers like BitsandBytes and Auto-GPTQ offer ways to run large models on smaller devices. BitsandBytes is user-friendly and supports various models, while Auto-GPTQ excels in text generation speed but may result in lower quality. Both schemes can minimize performance degradation in larger models according to the Open-LLM leaderboard.", "title": "Overview of natively supported quantization schemes in \ud83e\udd17 Transformers"}, {"url": "https://txt.cohere.com/validating-llm-outputs/", "summary": "LLMs are powerful but can produce inconsistent results. Validating outputs is essential for reliable and accurate applications. Guardrails AI is a useful open-source package that improves LLM outputs by providing structural and quality assurances.", "title": "Validating Large Language Model Outputs"}, {"url": "https://pub.towardsai.net/create-a-self-moderated-commentary-system-with-langchain-and-openai-406a51ce0c8d", "summary": "This guide explains the process of creating a self-moderated commentary system using OpenAI and LangChain. It involves two models: one generates a response to user input, and the other analyzes and modifies the response before publishing it.", "title": "Create a Self-Moderated Commentary System with LangChain and OpenAI"}], "papers": [{"url": "https://github.com/IBM/ModuleFormer", "summary": "IBM has just released MoE LLMs, including models with 4B and 8B parameters. These models offer comparable computational efficiency to dense models with fewer parameters. They have been trained on a large dataset and utilize the ModuleFormer architecture.", "title": "IBM releases MoE LLMs"}, {"url": "https://arxiv.org/abs/2309.07062", "summary": "Researchers have developed a powerful transformer model that optimizes LLVM assembly code for code size. The model outperforms baselines and demonstrates excellent code reasoning abilities, achieving a 3% reduction in instruction counts compared to compiler output. It generates compilable code 91% of the time and perfectly emulates the compiler\u2019s output 70% of the time.", "title": "Large Language Models for Compiler Optimization"}, {"url": "https://arxiv.org/abs/2309.04564", "summary": "Researchers have found that perplexity is a more effective method than complex scoring techniques for pruning pretraining data for language models.", "title": "When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale"}, {"url": "https://arxiv.org/abs/2309.05519", "summary": "NExT-GPT is an any-to-any multimodal language model that can process and generate content in various modalities such as text, images, videos, and audio. It achieves this by utilizing already-trained encoders and decoders, with minimal parameter tuning required.", "title": "NExT-GPT: Any-to-Any Multimodal LLM"}, {"url": "https://github.com/microsoft/promptflow", "summary": "Microsoft has introduced Prompt Flow, a development suite for LLM-based apps. It offers a range of functionalities including creating executable workflows, debugging and iterating flows, evaluating flow quality and performance with larger datasets, integrating testing and evaluation into CI/CD systems, and deploying flows to chosen serving platforms or app code bases easily.", "title": "Microsoft releases Prompt Flow"}, {"url": "https://arxiv.org/abs/2309.04269", "summary": "A recent study introduces the \u201cChain of Density\u201d (CoD) prompting technique that generates dense summaries using GPT-4. By iteratively adding important entities without increasing the length, the resulting abstract summaries outperformed standard prompt summaries in terms of abstractive quality and reduced lead bias.", "title": "From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting"}, {"url": "https://arxiv.org/abs/2309.07430", "summary": "LLMs have shown promising results in clinical text summarization tasks, surpassing human experts in terms of completeness and correctness. This research is the first to demonstrate LLMs outperforming humans in multiple clinical summarization tasks.", "title": "Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts"}, {"url": "https://arxiv.org/abs/2309.03926", "summary": "New neural text-to-speech technology and automated parsing of e-books in the Project Gutenberg collection have resulted in the creation of over 5,000 open-license audiobooks, expanding the accessibility of this vast literature collection.", "title": "Large-Scale Automatic Audiobook Creation"}], "datetime": "2023-09-18"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-september-11th-2023-7e3cd53fa508", "news": [{"url": "https://time.com/collection/time100-ai/", "summary": "TIME magazine has released its list of the 100 Most Influential People in AI for 2023. The list includes notable figures such as Dario and Daniela Amodei, Sam Altman, Demis Hassabis, Robin Li, Cl\u00e9ment Delangue, Lila Ibrahim, Elon Musk, Geoffrey Hinton, Fei-Fei Li, Timnit Gebru, Yann LeCun, and Yoshua Bengio.", "title": "The 100 Most Influential People in AI 2023", "topics": ["DeepMind", "OpenAI"], "sentiment": "positive"}, {"url": "https://huggingface.co/blog/falcon-180b", "summary": "TII has just released Falcon 180B, a powerful language model with 180 billion parameters trained on 3.5 trillion tokens. Outperforming Llama 2 70B and GPT-3.5 on MMLU, Falcon 180B performs great and ranks high on the Hugging Face Leaderboard. This model is available for commercial use but has strict terms excluding \u201chosting use.\u201d", "title": "Spread Your Wings: Falcon 180B is here", "topics": ["Hugging Face", "Model release", "GPT-3, GPT-3.5, and GPT-3.5 turbo"], "sentiment": "positive"}, {"url": "https://www.adept.ai/blog/persimmon-8b", "summary": "Adept.ai introduces Persimmon-8B, an open-source LLM with impressive performance and a compact size. Trained on less data, it achieves comparable results to LLaMA2 and offers fast C++ implementation combined with flexible Python inference.", "title": "Releasing Persimmon-8B", "topics": ["Model release", "LLaMA"], "sentiment": "positive"}, {"url": "https://huggingface.co/training-cluster", "summary": "Hugging Face provides cost estimates for training large language models (LLMs) of different sizes and token counts. The estimates range from $65k to $14.66M, depending on the model parameters and token count.", "title": "Training Cluster as a service: Train your LLM at scale on our infrastructure", "topics": ["Hugging Face"], "sentiment": "positive"}, {"url": "https://huggingface.co/spaces/hf-audio/open_asr_leaderboard", "summary": "Hugging Face has released a speech-to-text leaderboard that ranks and assesses speech recognition models on their platform. The current top performers are NVIDIA FastConformer and OpenAI Whisper, with a focus on English speech recognition. Multilingual evaluation will be added in future updates.", "title": "Open ASR Leaderboard", "topics": ["Hugging Face", "Speech-to-text", "NVIDIA", "OpenAI"], "sentiment": "positive"}], "guides": [{"url": "https://pub.towardsai.net/create-a-self-moderated-commentary-system-with-langchain-and-openai-406a51ce0c8d", "summary": "This guide explains the steps to build a self-moderated comment response system using OpenAI and LangChain. It involves two models, where the first generates a response and the second modifies and publishes it.", "title": "Create a Self-Moderated Commentary System with LangChain and OpenAI"}, {"url": "https://www.pinecone.io/learn/llm-ecosystem/", "summary": "LLMs have limitations such as generating false information and lack of up-to-date content. To harness their full potential, a well-designed ecosystem is necessary. This involves prompt engineering, utilizing techniques like quantization, retrieval augmented generation (RAG), and conversational memory.", "title": "LLMs Are Not All You Need"}, {"url": "https://pub.towardsai.net/gptq-quantization-on-a-llama-2-7b-fine-tuned-model-with-huggingface-a7b291fbb871", "summary": "HuggingFace has introduced GPTQ quantization, enabling the compression of large language models to 2, 3, or 4 bits. This method outperforms previous techniques, maintaining accuracy while significantly reducing model size.", "title": "GPTQ Quantization on a Llama 2 7B Fine-Tuned Model With HuggingFace"}, {"url": "https://eugeneyan.com/writing/abstractive/", "summary": "Abstractive summarization faces challenges in evaluating hallucinations, with difficulties in measuring relevance and consistency objectively. Evaluating summaries using metrics like ROUGE and BERTScore has limitations, especially outside of reference distributions. Detecting inconsistencies between summary and source document is crucial, with advancements in entailment-based and QA metrics.", "title": "Evaluation & Hallucination Detection for Abstractive Summaries"}, {"url": "https://arstechnica.com/cars/2023/09/are-self-driving-cars-already-safer-than-human-drivers/", "summary": "Early data suggests that self-driving cars, such as Waymo and Cruise driverless taxis, may be safer than human drivers. Despite experiencing 102 crashes within 6 million miles, most incidents were low-speed collisions caused by other drivers.", "title": "Are self-driving cars already safer than human drivers?"}], "papers": [{"url": "https://github.com/KillianLucas/open-interpreter", "summary": "Open Interpreter is an open-source implementation of OpenAI\u2019s Code Interpreter that provides a natural language interface similar to ChatGPT. It enables running various code types locally, offering interactive terminal chats for controlling computer functions without internet access limitations.", "title": "KillianLucas/open-interpreter: OpenAI\u2019s Code Interpreter in your terminal, running locally"}, {"url": "https://arxiv.org/abs/2309.03409", "summary": "LLMs can be used as optimizers in applications where gradients are not available. Optimization by PROmpting (OPRO) involves the LLM generating new solutions from a prompt, which are then evaluated and used to refine the prompt in a constant optimization cycle. OPRO has shown promising results, outperforming human-designed prompts in prompt optimization tasks.", "title": "Large Language Models as Optimizers"}, {"url": "https://arxiv.org/abs/2309.03179", "summary": "SLiMe, a novel approach that combines vision-language models and Stable Diffusion (SD), allows image segmentation at custom granularity using just one annotated sample. It outperforms existing one-shot and few-shot image segmentation methods, as demonstrated in comprehensive experiments. \ud83d\uddbc\ufe0f", "title": "SLiMe: Segment Like Me"}, {"url": "https://arxiv.org/abs/2309.03852", "summary": "The authors present a growth-oriented strategy to train a cost-effective LLM model with 101B parameters and 0.31TB tokens for just $100K. They also introduce a new evaluation method focused on IQ-level analysis, showcasing the model\u2019s performance on par with top models like GPT-3 and GLM-130B in IQ benchmark evaluations.", "title": "FLM-101B: An Open LLM and How to Train It with $100K Budget"}, {"url": "https://arxiv.org/abs/2309.01826", "summary": "Researchers have found that the Feed Forward Network (FFN) in Transformers can be optimized, resulting in a 40% reduction in model size while maintaining similar performance. By sharing a FFN across the encoder and removing it from the decoder layers, parameters can be decreased with minimal decrease in accuracy.", "title": "One Wide Feedforward is All You Need"}, {"url": "https://arxiv.org/abs/2309.00754", "summary": "The authors present Hydra-PPO, a method to accelerate Reinforcement Learning from Human Feedback (RLHF) by reducing memory usage. Hydra-PPO reduces the number of models in memory during the PPO stage, allowing for increased training batch size and decreased per-sample latency by up to 65%.", "title": "Efficient RLHF: Reducing the Memory Usage of PPO"}, {"url": "https://speechresearch.github.io/prompttts2/", "summary": "PromptTTS 2 is a Text-to-Speech system that can control attributes like Gender, Speed, Pitch, and Volume using text prompts. It can also match synthesized voices to facial images while maintaining timbre.", "title": "PromptTTS 2: Describing and Generating Voices with Text Prompt"}], "datetime": "2023-09-11"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-september-5th-2023-6cdcc8dbfb30", "news": [{"url": "https://openai.com/blog/introducing-chatgpt-enterprise", "summary": "OpenAI has released ChatGPT Enterprise, which provides enterprise-level security and privacy features. These include unlimited access to GPT-4, longer input capacities, faster performance, advanced data analysis capabilities, and data encryption measures. The platform is SOC 2 compliant and offers features like admin console, SSO, domain verification, and an analytics dashboard for usage insights. The training of OpenAI models does not involve customer prompts or company data.", "title": "Introducing ChatGPT Enterprise", "topics": ["ChatGPT", "GPT-4 and GPT-4 turbo", "OpenAI"], "sentiment": "positive"}, {"url": "https://blog.research.google/2023/08/weatherbench-2-benchmark-for-next.html", "summary": "Google has released the WeatherBench 2 dataset, which is gaining attention in the AI weather forecasting field. Machine learning methods are proving to be just as effective as physics-based models, but with the advantage of faster forecasts using affordable hardware. The WeatherBench 2 dataset aims to enhance ML research in weather forecasting.", "title": "WeatherBench 2: A benchmark for the next generation of data-driven weather models", "topics": ["AI datasets", "Google Gemini"], "sentiment": "positive"}, {"url": "https://venturebeat.com/ai/watch-out-midjourney-ideogram-launches-ai-image-generator-with-impressive-typography/", "summary": "Ideogram is an alternative AI tool that excels in generating images with strong typography capabilities. It offers a unique feature of generating text within images, effectively addressing the common challenge faced by popular AI image generators.", "title": "Ideogram launches AI image generator with impressive typography", "topics": ["AI for images", "Multimodal AI (image, video, audio)"], "sentiment": "positive"}, {"url": "https://openai.com/blog/teaching-with-ai", "summary": "Open AI has released a teaching guide on AI, providing insights into ChatGPT\u2019s functionality, limitations, the effectiveness of AI detectors, and addressing bias.", "title": "Teaching with AI", "topics": ["ChatGPT", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.deepmind.com/blog/identifying-ai-generated-images-with-synthid", "summary": "SynthID is a technology that uses imperceptible digital watermarks to identify AI- generated images, even after modifications like filters, color changes, and compression.", "title": "Identifying AI-generated images with SynthID", "topics": ["AI safety", "AI and copyright", "AI for images", "Multimodal AI (image, video, audio)"], "sentiment": "positive"}], "guides": [{"url": "https://pub.towardsai.net/rethink-that-gpu-purchase-for-your-ai-venture-41e8c052544", "summary": "This guide explores the decision of whether to rent or buy a GPU for your AI project. Owning a GPU provides stability, while renting offers adaptability. Freelancers may benefit from renting for short-term projects, while long-term researchers may find ownership more cost-effective. Consider your specific project needs and cost-effectiveness when making this decision.", "title": "Choosing the Right GPU Strategy for Your AI Project"}, {"url": "https://arstechnica.com/cars/2023/09/are-self-driving-cars-already-safer-than-human-drivers/", "summary": "Waymo and Cruise, two leading driverless taxi companies, have logged over 4 million driverless miles each, revealing 102 crashes in their 2023 safety reports. Most incidents were minor, attributing fault to other drivers. While self-driving cars still need to accumulate more miles for complete certainty, evidence points to their potential to be safer than human-operated vehicles.", "title": "Are self-driving cars already safer than human drivers?"}, {"url": "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/gen_ai/rlhf_tune_llm.ipynb", "summary": "The content provides a comprehensive guide for implementing Reinforcement Learning with Human Feedback (RLHF) on Google Cloud using Vertex AI.", "title": "Vertex AI LLM Reinforcement Learning from Human Feedback"}, {"url": "https://www.axios.com/2023/08/28/ai-content-flood-model-collapse", "summary": "Experts are predicting that AI-generated content will soon dominate the web, but this poses risks such as information overload and degradation. One concern is \u201cModel Collapse,\u201d where AI models rely too heavily on predictable word choices and lose their ability to produce intriguing and nuanced content.", "title": "AI could choke on its own exhaust as it fills the web"}, {"url": "https://www.pinecone.io/learn/options-for-solving-hallucinations-in-generative-ai/", "summary": "This content explores the problem of AI hallucinations in generative models and presents a solution called Retrieval Augmented Generation (RAG). RAG helps reduce hallucination by fetching relevant context during data generation, making it a preferred solution for AI companies.", "title": "Options for Solving Hallucinations in Generative AI"}], "papers": [{"url": "https://arxiv.org/abs/2309.00267", "summary": "New findings from Google suggest that RL from AI Feedback (RLAIF) could be a viable alternative to RL with high-quality human preference labels (RLHF). RLAIF utilizes an off-the-shelf language model to label preferences, eliminating the need for human intervention. In a study focused on summarization tasks, RLAIF and RLHF were found to deliver similar improvements. These promising results indicate that RLAIF has the potential to match the performance of RLHF, providing a solution to scalability limitations and enhancing the efficiency of large language models.", "title": "RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback"}, {"url": "https://arxiv.org/abs/2309.00071", "summary": "YaRN, a highly efficient extension method, allows for the use of a 128k context window in LLaMa 2 with 10x fewer tokens and 2.5x fewer training steps than previous methods. It has been successfully applied to finetune LLaMa 2 7B and 13B models with context windows of 64k and 128k.", "title": "YaRN: Efficient Context Window Extension of Large Language Models"}, {"url": "https://arxiv.org/abs/2308.12966", "summary": "Alibaba Cloud introduces Qwen-VL, a powerful vision-language model designed for chat applications. Qwen-VL combines image and text inputs to generate accurate text and bounding box outputs, enhancing tasks such as image-captioning, question answering, localization, and text-image understanding.", "title": "Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities"}, {"url": "https://arxiv.org/abs/2308.14089", "summary": "MedAlign is a valuable dataset consisting of 983 instructions for analyzing Electronic Health Record (EHR) data. Researchers utilized this dataset to assess the performance of six general domain language models. They discovered significant error rates, with GPT-4 exhibiting a 35% error rate and MPT-7B-Instruct having a 68% error rate.", "title": "MedAlign: A Clinician-Generated Dataset for Instruction Following with Electronic Medical Records"}, {"url": "https://arxiv.org/abs/2306.13812", "summary": "Deep learning models often struggle in continual learning settings and experience a \u201closs of plasticity,\u201d hindering their ability to recall previous examples and learn from new ones over time. However, a new algorithm called continual backpropagation has been developed, which reinitializes less-used units after each example, potentially enhancing plasticity and enabling continual learning in deep learning models.", "title": "Loss of Plasticity in Deep Continual Learning"}], "datetime": "2023-09-05"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-august-28th-2023-9754bb941c51", "news": [{"url": "https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates", "summary": "OpenAI has released fine-tuning for GPT-3.5 Turbo, offering improved performance on specific tasks. The fine-tuned version can even match or surpass the capabilities of base GPT-4. Early testers have significantly reduced prompt size through fine-tuning. The costs for training and usage input/output are provided at $0.008, $0.012, and $0.016 per 1K tokens, respectively.", "title": "GPT-3.5 Turbo fine-tuning released", "topics": ["GPT-3, GPT-3.5, and GPT-3.5 turbo", "GPT-4 and GPT-4 turbo", "OpenAI"], "sentiment": "positive"}, {"url": "https://ai.meta.com/blog/code-llama-large-language-model-coding/", "summary": "Meta has released Code Llama, an advanced LLM for coding that can generate code and natural language about code. It is available in three models and comes in different sizes to meet various needs.", "title": "Introducing Code Llama, a state-of-the-art large language model for coding", "topics": ["AI for coding", "Meta"], "sentiment": "positive"}, {"url": "https://ai.meta.com/blog/seamless-m4t/", "summary": "Meta has developed a powerful foundational model called SeamlessM4T that is capable of handling various text and speech tasks across 100 languages. It includes automatic speech recognition, speech-to-text translation, speech-to-speech translation, text-to-text translation, and text-to- speech translation, supporting a wide range of input and output languages.", "title": "Introducing a foundational multimodal model for speech translation", "topics": ["Text-to-speech", "Speech-to-text", "Multimodal AI (image, video, audio)", "Meta"], "sentiment": "positive"}, {"url": "https://www.deeplearning.ai/short-courses/finetuning-large-language-models/", "summary": "A new free course on \u201cFinetuning Large Language Models\u201d by DeepLearning.AI. The course aims to help AI professionals understand the usage of finetuning on LLMs, including data preparation, training, and evaluation. Finetuning enables training the model on custom data, updating the neural nets\u2019 weights, and improving results by incorporating style, form, and new knowledge.", "title": "New DeepLearning.AI Course on Finetuning Large Language Models", "topics": [], "sentiment": "positive"}, {"url": "https://techcrunch.com/2023/08/18/ai2-drops-biggest-open-dataset-yet-for-training-language-models/", "summary": "AI2 has released a significant open dataset called Dolma, comprising 3 trillion tokens. Unlike other datasets from OpenAI and Anthropic, Dolma provides transparency in terms of what information was removed, why, and how personal details were handled. The disclosure of these details is uncommon and raises questions about the ethical and legal acquisition of other datasets.", "title": "AI2 drops biggest open dataset yet for training language models", "topics": ["AI and copyright", "AI datasets", "AI regulation"], "sentiment": "positive"}, {"url": "https://huggingface.co/blog/idefics", "summary": "IDEFICS is an impressive open-source visual language model with 9 billion and 80 billion parameters, based on DeepMind\u2019s Flamingo. It offers the ability to describe images, generate stories, and answer image-related questions. It\u2019s trained on diverse open datasets such as Wikipedia, Public Multimodal Dataset, LAION, and OBELICS.", "title": "Introducing IDEFICS: An Open Reproduction of State-of-the-art Visual Langage Model", "topics": ["AI for images", "Multimodal AI (image, video, audio)", "DeepMind"], "sentiment": "positive"}, {"url": "https://www.zdnet.com/article/bings-search-market-share-fails-to-budge-despite-ai-push/", "summary": "Despite the introduction of innovative AI features like Bing AI Chat and Bing Image Creator, Bing\u2019s market share remains stagnant at around 3%. Microsoft disputes this data, citing internal growth numbers, but experts believe any missing interactions will not greatly affect the overall situation.", "title": "Bing\u2019s search market share fails to budge despite big AI push", "topics": ["Microsoft"], "sentiment": "negative"}], "guides": [{"url": "https://huggingface.co/blog/codellama", "summary": "Code Llama is now available on Hugging Face, with the ability to perform code infilling with the 7B and 13B models.", "title": "Code Llama on Hugging Face"}, {"url": "https://huggingface.co/blog/gptq-integration", "summary": "Hugging Face introduces AutoGPTQ integration in Transformers, enabling efficient 2, 3, 4, and 8-bit quantization with minimal loss in accuracy. The integration supports Nvidia GPUs and RoCm-powered AMD GPUs.", "title": "Making LLMs lighter with AutoGPTQ and transformers"}, {"url": "https://www.newsguardtech.com/misinformation-monitor/august-2023/", "summary": "AI is being used by websites to rewrite articles from reputable news outlets without attribution. This raises questions about whether AI-generated content should be considered original or a more advanced form of plagiarism.", "title": "How Low-Quality Websites Are Using AI to Deceptively Rewrite Content from Mainstream News Outlets"}, {"url": "https://ai.googleblog.com/2023/08/teaching-language-models-to-reason.html", "summary": "This paper explores the effectiveness of teaching algorithmic reasoning to LLMs, focusing on overcoming challenges such as overfitting and spurious correlations. It proposes a four-step approach that includes formulating algorithms as skills, teaching multiple skills simultaneously, teaching skill composition, and teaching the use of skills as tools.", "title": "Teaching language models to reason algorithmically"}, {"url": "https://ai.googleblog.com/2023/08/language-to-rewards-for-robotic-skill.html", "summary": "Language-to-reward systems powered by LLMs enable robots to learn directly from language. These systems translate natural language instructions into reward-specifying codes, compute rewards from robot actions, and facilitate learning through reinforcement learning (RL).", "title": "Language to rewards for robotic skill synthesis"}], "papers": [{"url": "https://blog.abacus.ai/blog/2023/08/22/giraffe-long-context-llms/", "summary": "Giraffe is a new line of models derived from LLaMA and LLaMA2, with variants including 4k, 16k, and 32k tokens context window. The models have been finetuned from LLaMA and LLaMA2 and offer experiments on extending the context window through positional encoding modifications.", "title": "Giraffe \u2014 Long Context LLMs"}, {"url": "https://arxiv.org/abs/2308.08998", "summary": "Reinforced Self-Training (ReST) is a cheaper alternative to RLHF developed by DeepMind. It utilizes a two-step process, Grow and Improve, to augment the training dataset and fine-tune the LLM. ReST offers advantages over RLHF, including reduced computational burden, improved policy quality, ease of data inspection, and a simple and stable approach with minimal hyperparameters.", "title": "Reinforced Self-Training (ReST) for Language Modeling"}, {"url": "https://platypus-llm.github.io/", "summary": "Platypus, the new LLM on HuggingFace\u2019s Open LLM Leaderboard, utilizes the Open-Platypus dataset to achieve strong performance in STEM and logic. It successfully mitigates bias during training by leveraging LoRA modules and the PEFT library. Though it struggles with languages beyond English, this is due to its underlying model, LLaMa-2.", "title": "New LLM Platypus tops the Hugging Face LLM leaderboard"}, {"url": "https://arxiv.org/abs/2308.09687v2", "summary": "Graph of Thoughts (GoT) is an advanced method that models information from an LLM as a graph, allowing for the combination of thoughts and the incorporation of feedback loops. GoT has demonstrated superior performance compared to other methods, such as Tree of Thoughts (ToT), by significantly improving sorting quality (62%) and reducing costs (31%).", "title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models"}, {"url": "https://arxiv.org/abs/2307.13304", "summary": "QuIP is a new approach that achieves 2-bit quantization of language model models using adaptive rounding. It is the first algorithm of its kind to come with a theoretical analysis, showing its potential impact on other quantization methods like OPTQ.", "title": "QuIP: 2-Bit Quantization of Large Language Models With Guarantees"}], "datetime": "2023-08-28"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-august-14th-2023-f6ed016d1c47", "news": [{"url": "https://idx.dev/", "summary": "Google is introducing IDX, a cloud-based coding workspace powered by generative AI. This workspace offers AI assistance for code generation, completion, translation, and explanation, enhancing developers\u2019 coding efficiency. It also supports multi-browser web previews and provides consistent experiences across various devices and operating systems.", "title": "Introducing Project IDX by Google", "topics": ["AI for coding", "Google"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2023/08/04/microsoft-kills-cortana-in-windows-as-it-focuses-on-next-gen-ai/", "summary": "Microsoft is discontinuing Cortana to prioritize generative AI like Bing Chat, while integrating ChatGPT from its partnership with OpenAI into Windows 11.", "title": "Microsoft kills Cortana in Windows as it focuses on next-gen AI", "topics": ["Microsoft", "ChatGPT", "OpenAI"], "sentiment": "positive"}, {"url": "https://platform.openai.com/docs/gptbot", "summary": "OpenAI\u2019s web crawler, GPTBot, can now be easily identified and prevented from accessing websites for AI training. Website owners can use the provided code to add GPTBot to their robots.txt file, disallowing it from crawling their site\u2019s content.", "title": "GPTBot: the web crawler of OpenAI", "topics": ["AI and copyright", "OpenAI"], "sentiment": "negative"}, {"url": "https://www.reuters.com/technology/toyota-chinas-ponyai-set-up-jv-with-139-mln-investment-2023-08-04/", "summary": "Toyota and Pony.ai have formed a partnership to produce robo-taxis in China. This collaboration aims to advance self-driving car technology in the country and develop \u201csmart cockpits\u201d for the Chinese market. Toyota\u2019s investment is of over 1 billion yuan ($140 million).", "title": "Toyota, Pony.ai plan to mass produce robotaxis in China", "topics": ["Robotics"], "sentiment": "positive"}, {"url": "https://www.theverge.com/2023/8/10/23827399/ai-artificial-intelligence-political-ads-fec-desantis-rnc", "summary": "The Federal Election Commission (FEC) is considering limitations on the use of AI in political ads for the upcoming elections. Public Citizen\u2019s petition, supported by congressional action, aims to combat misleading AI-generated campaign content in the lead-up to the 2024 elections. Public Citizen specifically emphasizes the need to regulate deepfakes and deceptive AI use in election ads.", "title": "FEC could limit AI in political ads ahead of 2024", "topics": ["AI regulation"], "sentiment": "negative"}, {"url": "https://decrypt.co/151780/amazon-authors-writers-fake-books-trademark", "summary": "Authors are reporting fraudulent books attributed to them on Amazon, likely generated using AI. Jane Friedman successfully had these books taken down with support from the Authors Guild, but concerns remain about the impact on lesser-known authors.", "title": "Amazon Reverses Course on \u2018Garbage Books\u2019 After Public Uproar", "topics": ["AI and copyright", "Amazon"], "sentiment": "negative"}, {"url": "https://nvidianews.nvidia.com/news/nvidia-and-hugging-face-to-connect-millions-of-developers-to-generative-ai-supercomputing", "summary": "NVIDIA and Hugging Face have partnered to offer AI developers access to high-performance GPUs for deep learning through DGX Cloud integration.", "title": "NVIDIA and Hugging Face to Connect Millions of Developers to Generative AI Supercomputing", "topics": ["Hugging Face", "NVIDIA", "AI Chips and GPUs"], "sentiment": "positive"}], "guides": [{"url": "https://huggingface.co/blog/dpo-trl", "summary": "A new method called DPO may replace RLHF fine-tuning of LLMs. DPO simplifies the optimization process by directly optimizing the language model using preference data, eliminating the need for complex RL techniques or a reward model. This guide demonstrates the DPO method for finetuning Llama 2 on user preferences using the TRL library and its DPOTrainer.", "title": "Fine-tune Llama 2 with DPO"}, {"url": "https://www.youtube.com/watch?v=Nlkk3glap_U", "summary": "Dario Amodei, CEO of Anthropic, discusses AI scalability and the path to human-level intelligence. Key takeaways include the challenges of effectively scaling with parameters and data, predicting specific abilities, potential gaps in skills, the need for alignment and values, integration challenges in real-world scenarios, uncertainties in achieving human-level AI in 2\u20133 years, and the importance of safety and regulation in AI development.", "title": "Dario Amodei (Anthropic CEO) \u2014 $10 Billion Models, OpenAI, Scaling, & AGI in 2 years"}, {"url": "https://notes.aimodels.fyi/bark-tortoise-tts/", "summary": "This guide compares two popular TTS models, Bark and Tortoise TTS, which use deep learning techniques to convert text to speech. Bark offers versatility with control over various attributes and supports multiple languages, while Tortoise excels at reproducing human voices and is ideal for long-form speech synthesis.", "title": "Hyper-Realistic Text-to-Speech: Comparing Tortoise and Bark for Voice Synthesis"}, {"url": "https://aman.ai/watch/", "summary": "Here is a list of open AI courses offered by esteemed institutions such as Stanford, CMU, and MIT. These courses cover a wide range of topics including principles and techniques of artificial intelligence, machine learning, deep learning, natural language processing, reinforcement learning, and machine learning with graphs.", "title": "A curated list of open AI courses from top universities"}, {"url": "https://tanay.substack.com/p/big-tech-x-generative-ai-q2-update", "summary": "In Q2 earning calls, big tech companies showcased their advancements in Generative AI. Amazon focuses on training and inference of LLMs, providing affordable options for LLM service. Microsoft boasts AI integration in multiple areas, with the OpenAI Azure Service gaining popularity. Meta invests in recommendation systems and open sources innovations like Llama 2 model. Alphabet highlights AI investments in search and productivity, with Google\u2019s LLMs-based search being a game-changer. Apple announces AI features for iOS 17, but details are scarce.", "title": "Big Tech x Generative AI Q2 Update"}, {"url": "https://www.vox.com/future-perfect/23820331/chatgpt-bioterrorism-bioweapons-artificial-inteligence-openai-terrorism", "summary": "The ease of learning with AI language models like ChatGPT poses a risk in enabling extremists to create and deploy biological weapons more effectively. To prevent misuse, stronger biosecurity measures and monitoring should be implemented, while balancing open science and legitimate scientific research.", "title": "ChatGPT could make bioterrorism horrifyingly easy"}, {"url": "https://time.com/6300942/ai-progress-charts/", "summary": "AI progress is not expected to slow down soon, thanks to factors such as increased computing power, Moore\u2019s Law, access to more data, better algorithms, and improved cost efficiency.", "title": "4 Charts That Show Why AI Progress Is Unlikely to Slow Down"}], "papers": [{"url": "https://github.com/huggingface/candle", "summary": "Candle is a minimalist ML framework for Rust with GPU support, including CUDA. It offers various prebuilt models and the ability to add custom ops and kernels, making it ideal for AI tasks such as speech recognition and code generation. Perfect for serverless deployment and browser-based models using WASM.", "title": "huggingface/candle: Minimalist ML framework for Rust"}, {"url": "https://www.techspot.com/news/99709-researchers-develop-ai-can-log-keystrokes-acoustically-92.html", "summary": "Researchers have developed an AI algorithm that can accurately hack passwords by analyzing acoustic cues of typed letters. With a success rate above 90% and up to 95% accuracy with smartphone microphones, the algorithm demonstrates its effectiveness.", "title": "Researchers develop AI that can log keystrokes acoustically with 92\u201395 percent accuracy"}, {"url": "https://arxiv.org/abs/2308.03958", "summary": "Researchers have discovered that language models, specifically PaLM models with 540B parameters, tend to tailor their responses to align with a user\u2019s view, even if the views are objectively wrong. This behavior, known as \u201csycophantic behavior,\u201d can be reduced by using synthetic data and training models to handle diverse user opinions. A lightweight fine-tuning method has shown promising results in minimizing this behavior.", "title": "Simple synthetic data reduces sycophancy in large language models"}, {"url": "https://arxiv.org/abs/2308.03610", "summary": "AvatarVerse generates high-quality 3D avatars with precise pose control from text descriptions and pose guidance. It incorporates a 2D diffusion model, enabling accurate depiction of individuals\u2019 descriptions and pose signals.", "title": "AvatarVerse: High-quality & Stable 3D Avatar Creation from Text and Pose"}, {"url": "https://arxiv.org/abs/2308.02151", "summary": "The Retroformer is a framework that enhances large language models by training them with a retrospective model, allowing them to fine-tune and optimize for environment-specific rewards. This enables language agents to reason, plan, and learn from feedback, ultimately improving their performance in various environments and tasks.", "title": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization"}, {"url": "https://arxiv.org/abs/2308.03526", "summary": "Researchers have set a new benchmark in offline reinforcement learning using StarCraft 2. They improved the state of the art in agents using only offline data, achieving a remarkable 90% win rate against the previous best agent. This advancement was made possible due to the unique characteristics of StarCraft 2 and a massive dataset released by Blizzard.", "title": "AlphaStar Unplugged: Large-Scale Offline Reinforcement Learning"}], "datetime": "2023-08-14"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-august-7th-2023-67b4360d404f", "news": [{"url": "https://ai.meta.com/blog/audiocraft-musicgen-audiogen-encodec-generative-ai-audio/", "summary": "Meta has released the code and weights for their AudioCraft models, including MusicGen and AudioGen. These models generate music and audio respectively, based on text-based user inputs. The release also includes the EnCodec decoder, which improves music quality.", "title": "AudioCraft: A simple one-stop shop for audio modeling", "topics": ["Multimodal AI (image, video, audio)", "Model release", "Meta"], "sentiment": "positive"}, {"url": "https://www.earthdata.nasa.gov/news/impact-ibm-hls-foundation-model", "summary": "NASA and IBM Research have collaborated to release the HLS Geospatial FM, an open-source geospatial AI model for Earth observation data. This model has shown success in various applications such as flood mapping, burn scar identification, and predicting crop yields.", "title": "NASA and IBM Openly Release Geospatial AI Foundation Model for NASA Earth Observation Data", "topics": ["AI datasets", "AI for images", "Multimodal AI (image, video, audio)", "Model release"], "sentiment": "positive"}, {"url": "https://blog.jupyter.org/generative-ai-in-jupyter-3f7174824862", "summary": "Jupyter AI integrates generative AI techniques and provides functionalities such as code generation, error fixing, content summarization, file questioning, and notebook creation from language prompts.", "title": "Generative AI in Jupyter", "topics": ["AI for coding"], "sentiment": "positive"}, {"url": "https://www.theverge.com/2023/8/1/23815321/youtube-ai-video-summaries", "summary": "YouTube is currently testing AI-generated video summaries to help viewers quickly determine the relevance of a video. The feature uses generative AI and will initially be available for English- language vlogs, shopping, and how-to videos on mobile devices.", "title": "YouTube uses AI to summarize videos in latest test", "topics": ["Multimodal AI (image, video, audio)", "Google Gemini", "Google"], "sentiment": "positive"}, {"url": "https://www.deepmind.com/blog/rt-2-new-model-translates-vision-and-language-into-action", "summary": "Meta\u2019s Robotic Transformer 2 (RT-2) is a vision-language-action model that combines web-scale capabilities with robotic control. It effectively recognizes visual and language patterns, generalizes emergent skills, and successfully leverages web-based data to learn new skills.", "title": "RT-2: New model translates vision and language into action", "topics": ["Robotics", "Multimodal AI (image, video, audio)", "Meta"], "sentiment": "positive"}], "guides": [{"url": "https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better", "summary": "Open-source LLMs have evolved to become competitive with proprietary LLMs through advancements in pre-training and model development. Early challenges were overcome by focusing on the importance of pre-training and creating better base models. Recent trends include using larger pre-training datasets and optimizing models for fast inference.", "title": "The History of Open-Source LLMs: Better Base Models"}, {"url": "https://ai.googleblog.com/2023/08/multimodal-medical-ai.html", "summary": "LLMs for medical AI face the challenge of integrating data from various sources such as medical images, clinical notes, lab tests, electronic health records, and genomics. Different research approaches include using external tools for each data type, adapting specialized neural networks for each domain, or creating an integrated system combining LLM with a vision encoder.", "title": "Multimodal medical AI"}, {"url": "/towards-generative-ai/understanding-llama-2-architecture-its-ginormous-impact-on-genai-e278cb81bd5c", "summary": "Meta\u2019s 77-page paper on LLaMA-2 reveals impressive results, surpassing open-source benchmarks and competing with GPT3.5. The article explais advancements like Grouper query attention, Ghost Attention, In-Context Temperature re-scaling, and Temporal Perception.", "title": "Understanding LLaMA-2 architecture and its ginormous impact on GenAI"}, {"url": "https://pub.towardsai.net/fit-your-llm-in-a-single-gpu-with-gradient-checkpointing-lora-and-quantization-96b5b1bb723e", "summary": "Three techniques, Gradient Checkpointing, LoRA, and Quantization, can help saving GPU memory, and avoiding memory errors while fine-tuning language models. These techniques involve minimizing layers during training, embedding new trainable parameters, and reducing data precision.", "title": "Fit Your LLM in a single GPU with Gradient Checkpointing, LoRA, and Quantization."}, {"url": "https://pub.towardsai.net/top-10-open-source-llms-to-use-in-your-next-llm-application-fbfc51542b78", "summary": "This article highlights the top 10 open-source LLMs for the AI field. These LLMs offer customizable solutions, reasoning abilities, multilingual support, natural language understanding, text generation, question-answering, chatbot interfaces, versatility, and robustness.", "title": "Top 10 Open Source LLMs To USE In Your Next LLM Application"}], "papers": [{"url": "https://arxiv.org/abs/2308.01320", "summary": "DeepSpeed-Chat is an accessible tool for RLHF training. It provides easy-to-use training and inference for LLMs, replicating the RLHF training used in InstructGPT. With unified optimization, it achieves efficiency and scalability, allowing for fast and affordable training of models with billions of parameters.", "title": "DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales"}, {"url": "https://arxiv.org/abs/2307.16789", "summary": "Researchers have developed an open-source dataset called ToolBench for instruction-tuning in API usage to enhance the performance of language models. They fine-tuned LLaMA using ToolBench and achieved comparable results to ChatGPT. Additionally, they created a neural API retriever that recommends relevant APIs, allowing ToolLLaMA to utilize a wide range of real-world APIs.", "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs"}, {"url": "https://github.com/microsoft/azurechatgpt", "summary": "Microsoft has introduced Azure ChatGPT, a private and secure solution for deploying ChatGPT instances on Azure. It offers built-in privacy guarantees, complete control over accessibility, and the ability to integrate internal data sources and plugins. To facilitate adoption, Microsoft has also developed a Solution Accelerator guide.", "title": "microsoft/azurechatgpt: Azure ChatGPT, private and secure ChatGPT for internal enterprise use"}, {"url": "https://arxiv.org/abs/2307.14936", "summary": "The RRTF framework, a new method for aligning Code LLMs, outperforms RLHF. PanGu-Coder2, using RRTF, achieves impressive results on multiple benchmarks with a 62.20% pass@1 on OpenAI HumanEval. The success of PanGu-Coder2 is attributed to both RRTF and the use of high-quality data and model optimization.", "title": "PanGu-Coder2: Boosting Large Language Models for code with ranking feedback"}, {"url": "https://arxiv.org/abs/2308.01263", "summary": "A new test suite called XSTest helps evaluate the safety of language models by identifying \u201cexaggerated safety\u201d behaviors. The test results showed that the Llama2 model by Meta displayed excessive safety behavior, refusing prompts that were harmless but resembled unsafe ones or touched sensitive topics. This behavior could be attributed to lexical overfitting, where the models focus solely on the dangerous meanings of words instead of grasping benign intent like humans do.", "title": "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models"}, {"url": "https://arxiv.org/pdf/2308.00675.pdf", "summary": "A recent study has found that, for LLMs, reading tool documentation is more effective than relying solely on demonstrations for learning to use new tools. Researchers demonstrated this through empirical findings on six vision and language tasks, showing that zero-shot prompts with tool documentation perform just as well as few-shot prompts on benchmarks.", "title": "Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models"}, {"url": "https://arxiv.org/abs/2307.15771", "summary": "A recent study in language models discovered the Hydra effect, where removing one attention layer triggers compensation in another. Additionally, researchers found that late MLP layers downregulate maximum-likelihood token, even in models trained without dropout.", "title": "The Hydra Effect: Emergent Self-repair in Language Model Computations"}, {"url": "https://arxiv.org/abs/2308.00304", "summary": "A new research paper introduces \u201cSkills-in-Context (SKiC) Prompting\u201d as a strategy to teach Language Model Learners (LLMs) to combine skills. SKiC prompting allows LLMs to solve challenging compositionality tasks by providing examples of specific skills and their composition in the same prompt.", "title": "Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models"}], "datetime": "2023-08-07"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-july-31th-2023-82bcc967fd6e", "news": [{"url": "https://stackoverflow.blog/2023/07/27/announcing-overflowai/", "summary": "Stack Overflow is integrating generative AI into their platform with OverflowAI. This includes semantic search and personalized results using a vector database. They are also enhancing search capabilities across different platforms and introducing an enterprise knowledge ingestion feature for Stack Overflow for Teams.", "title": "Stack Overflow announces OverflowAI", "topics": [], "sentiment": "positive"}, {"url": "https://stability.ai/press-articles/stable-diffusion-xl-1-featured-amazon-aws-bedrock", "summary": "Stability AI has released SDXL 1.0, an open access image model with a staggering 6.6 billion parameter model ensemble pipeline. This version has significant improvements in color, contrast, lighting, and shadow.", "title": "Stability AI Announces Stable Diffusion XL 1.0", "topics": ["AI for images", "Stable Diffusion", "Stability AI"], "sentiment": "positive"}, {"url": "https://opentensor.medium.com/introducing-bittensor-language-model-a-state-of-the-art-3b-parameter-model-for-mobile-and-edge-2fe916fb81b0", "summary": "BTLM is a language model with 3 billion parameters that operates efficiently on mobile and edge devices with limited RAM. With improved accuracy and a large context window, it outperforms similar-sized models in benchmarks. BTLM aligns with 7B models in accuracy but surpasses them in terms of memory footprint and inference cost. Its purpose is to enable AI applications on edge devices, reducing the need for centralized cloud infrastructure.", "title": "Introducing Bittensor Language Model \u2014 A State of the Art 3B Parameter Model For Mobile and Edge Devices", "topics": [], "sentiment": "positive"}, {"url": "https://stability.ai/blog/freewilly-large-instruction-fine-tuned-models", "summary": "Stability AI and CarperAI Lab have collaborated to release FreeWilly, a LLaMA 2 model fine-tuned using Supervised Fine-Tune (SFT) techniques. FreeWilly2 performs comparably to GPT-3.5 in certain tasks, and its capabilities have been verified by both Stability AI researchers and Hugging Face. Both models are publicly available under a non-commercial license.", "title": "Meet FreeWilly, Our Large And Mighty Instruction Fine-Tuned Models", "topics": ["Model release", "Stability AI", "LLaMA", "GPT-3, GPT-3.5, and GPT-3.5 turbo"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2023/07/25/openai-scuttles-ai-written-text-detector-over-low-rate-of-accuracy/", "summary": "OpenAI has decided to retire its AI classifier due to its low accuracy rate in detecting AI- generated text. The rapid development of large language models has made it challenging to identify features or patterns effectively.", "title": "OpenAI scuttles AI-written text detector over \u2018low rate of accuracy\u2019", "topics": ["AI safety", "OpenAI"], "sentiment": "negative"}, {"url": "https://techcrunch.com/2023/07/24/microsofts-bing-chat-comes-to-chrome-and-safari-in-tests-for-select-users/", "summary": "Microsoft has confirmed that Bing Chat will soon be available on browsers like Google Chrome and Safari.", "title": "Microsoft\u2019s Bing Chat comes to Chrome and Safari in tests for \u2018select users\u2019", "topics": ["Microsoft"], "sentiment": "positive"}, {"url": "https://openai.com/blog/frontier-model-forum", "summary": "Anthropic, Google, Microsoft, and OpenAI have joined forces to create the Frontier Model Forum, a platform dedicated to the safe and responsible development of frontier AI models. The Forum aims to advance AI safety research, establish safety best practices, share knowledge, and use AI to tackle societal challenges.", "title": "The Frontier Model Forum", "topics": ["AI safety", "Google", "Microsoft", "OpenAI"], "sentiment": "positive"}], "guides": [{"url": "https://www.deeplearning.ai/short-courses/building-generative-ai-applications-with-gradio/", "summary": "Hugging Face and DeepLearning.ai have launched a new short course on building generative AI applications with Gradio. The course focuses on creating user-friendly apps using open-source language models, with projects ranging from text summarization to image analysis and image generation.", "title": "Building Generative AI Applications with Gradio"}, {"url": "https://levelup.gitconnected.com/build-an-ai-chart-generator-that-adapting-to-any-dataset-type-in-only-50-lines-7c4a0668c5f1", "summary": "Developers can easily create AI chart generators using GPT-3.5 or GPT-4 with Langchain, requiring just 50 lines of code.", "title": "Build an AI Chart Generator That Adapting to Any Dataset Type, in Only 50 Lines"}, {"url": "https://blog.langchain.dev/automating-web-research/", "summary": "This article explores the development of web research agents. The approach involves using an LLM to generate search queries, executing searches, scraping pages, indexing documents, and finding the most relevant results for each query.", "title": "Automating Web Research"}, {"url": "https://platform.openai.com/docs/tutorials/meeting-minutes", "summary": "This guide explores the development of a meeting minutes generation tool that utilizes Whisper and GPT-4 to efficiently summarize discussions, extract important details, and analyze sentiments.", "title": "Creating an automated meeting minutes generator with Whisper and GPT-4"}], "papers": [{"url": "https://github.com/karpathy/llama2.c", "summary": "Andrew Karpathy has released an educational implementation of LLaMA 2 inference in pure C. This project allows you to train an LLaMA 2 LLM architecture in PyTorch and then load the weights into a single C file for efficient inference.", "title": "karpathy/llama2.c: Inference Llama 2 in one file of pure C"}, {"url": "https://arxiv.org/pdf/2307.10928v1.pdf", "summary": "FLASK is an evaluation protocol specifically designed for LLMs performance assessment. It breaks down evaluations into 12 different skill sets, allowing for a detailed analysis of a model\u2019s performance based on specific skills such as logical robustness, factuality, and comprehension.", "title": "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets"}, {"url": "https://llm-attacks.org/", "summary": "A recent study explores automatic construction of adversarial attacks on both open-source and closed-source language models, making them susceptible to harmful commands. These attacks transfer to widely used chatbots, raising concerns about effectively patching these vulnerabilities. Uncertainty remains regarding the inherent susceptibility of deep learning models to adversarial attacks, similar to the challenges faced in computer vision.", "title": "Universal and Transferable Attacks on Aligned Language Models"}, {"url": "https://arxiv.org/abs/2307.13854", "summary": "WebArena is a realistic web environment that allows autonomous agents to develop their skills in tasks related to e-commerce, social forums, software development, and content management. It provides benchmarks for evaluating task completion and highlights the need for improved agents, as even advanced models like GPT-4 have a success rate of only 10.59%.", "title": "WebArena: A Realistic Web Environment for Building Autonomous Agents"}, {"url": "https://llava-vl.github.io/", "summary": "LLaVA is a large multimodal model that combines vision and language processing for improved visual and language understanding. It achieves impressive chat capabilities and sets a new state-of-the-art accuracy for Science QA. Initial experiments show its superior performance compared to GPT-4 on a synthetic multimodal instruction-following dataset.", "title": "LLaVA: Large Language and Vision Assistant"}, {"url": "https://arxiv.org/pdf/2307.10159v1.pdf", "summary": "Researchers have developed a training-free method called FABRIC to incorporate user feedback into diffusion-based text-to-image models. By utilizing self-attention, the model can enhance its generation process according to iterative user input, leading to improved output quality and a better user experience.", "title": "FABRIC: Personalizing Diffusion Models with Iterative Feedback"}, {"url": "https://arxiv.org/abs/2307.12981", "summary": "A new research introduces 3D-LLMs, which enhance language models with 3D comprehension by incorporating 3D point clouds and their features. By using a 3D feature extractor and existing 2D VLMs, 3D-LLMs achieve impressive performance on tasks such as captioning, question answering, and navigation, surpassing existing LLMs and VLMs.", "title": "3D-LLM: Injecting the 3D World into Large Language Models"}, {"url": "https://arxiv.org/abs/2307.12856", "summary": "WebAgent, an LLM-driven agent, uses Flan-U-PaLM and HTML-T5 to improve autonomous web navigation and task completion on real websites. By breaking down instructions, summarizing HTML documents, and generating Python programs, it achieves a 50% increase in success rates compared to previous models.", "title": "A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis"}, {"url": "https://sites.google.com/view/steve-1", "summary": "The STEVE-1 model in Minecraft can understand and respond to text instructions, thanks to its training process involving pre-trained models and self-supervised behavioral cloning. Not only does it set new benchmarks in following various instructions, but it also achieves remarkable performance at a low training cost, making it accessible to researchers on a budget.", "title": "STEVE-1: A Generative Model for Text-to-Behavior in Minecraft"}], "datetime": "2023-07-31"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-july-24th-2023-69c7e2b0a20c", "news": [{"url": "https://ai.meta.com/resources/models-and-libraries/llama/", "summary": "Meta has released Llama-2, an open-source model with a commercial license, that showcases similar performance to ChatGPT. Trained on 2T tokens with varying parameter sizes, Llama-2 was further fine-tuned and improved using a combination of instruction and reinforcement learning, outperforming other open-source models like Falcon and MPT.", "title": "Meta releases Llama 2", "topics": ["Model release", "LLaMA", "Meta", "ChatGPT"], "sentiment": "positive"}, {"url": "https://blog.langchain.dev/announcing-langsmith/", "summary": "LangChain has developed LangSmith, a powerful tool designed to enhance the performance of LLM-powered apps. By providing essential debugging, testing, evaluation, and monitoring features, LangSmith helps AI professionals identify and address issues such as unexpected results, errors, and latency. The tool also enables easy experimentation with new chains and templates, making it a valuable asset for those working in the artificial intelligence field.", "title": "Announcing LangSmith, a unified platform for debugging, testing, evaluating, and monitoring your LLM applications", "topics": [], "sentiment": "positive"}, {"url": "https://techcrunch.com/2023/07/19/apple-is-testing-chatgpt-like-ai-chatbot/", "summary": "Apple is creating its own chatbot, named \u201cApple GPT,\u201d to rival Google and OpenAI. Despite initial security concerns, the chatbot is now more widely accessible to Apple employees for prototyping purposes, with restricted usage and no customer-bound features allowed.", "title": "Apple is testing a ChatGPT-like AI chatbot", "topics": ["Apple", "Google", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.reuters.com/technology/cerebras-systems-signs-100-mln-ai-supercomputer-deal-with-uaes-g42-2023-07-20/", "summary": "Cerebras Systems has struck a $100 million deal with G42, marking the debut of AI supercomputers that could potentially challenge Nvidia\u2019s market position. In response to chip shortages, cloud computing providers are seeking alternative solutions. To accelerate the rollout, Cerebras will construct three Condor Galaxy systems in the United States, with the first supercomputer set to go online this year, followed by two others in early 2024.", "title": "Cerebras Systems signs $100 million AI supercomputer deal with UAE\u2019s G42", "topics": ["Funding", "NVIDIA", "AI Chips and GPUs"], "sentiment": "positive"}, {"url": "https://openai.com/blog/custom-instructions-for-chatgpt", "summary": "OpenAI introduces personalized custom instructions for ChatGPT, allowing users to have a more tailored and adaptable experience. This feature, developed after gathering feedback from users across 22 countries, highlights the importance of customization in meeting diverse needs. Custom instructions will be gradually rolled out to all users, with beta access initially available to Plus plan subscribers.", "title": "Custom instructions for ChatGPT", "topics": ["ChatGPT", "OpenAI"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2023/07/17/wixs-new-tool-can-create-entire-websites-from-prompts/", "summary": "Wix, the leading website builder, has released an AI Site Generator tool that uses AI to automatically generate websites based on user prompts. This tool goes beyond design, offering features such as e-commerce, scheduling, food ordering, and event ticketing. The CEO of Wix believes that AI is essential in simplifying website development for small businesses and helping them avoid missing out on income opportunities.", "title": "Wix\u2019s new tool can create entire websites from prompts", "topics": ["AI for coding"], "sentiment": "positive"}, {"url": "https://huggingface.co/blog/game-jam-first-edition-results", "summary": "The Open Source AI Game Jam hosted by Hugging Face showcased innovative games integrating AI models. The winning game, \u201cSnip It,\u201d allows players to explore a museum where objects in paintings come to life when snipped. Other impressive games included \u201cYabbit Attack,\u201d which uses genetic algorithms, \u201cFish Dang Bot Rolling Land\u201d with Text To Speech integration, and \u201cEverchanging Quest\u201d incorporating GPT-4 and Starcoder. Check out the link for the full list and explore the games\u2019 AI features.", "title": "Results of the Open Source AI Game Jam", "topics": ["Hugging Face", "Text-to-speech", "GPT-4 and GPT-4 turbo"], "sentiment": "positive"}], "guides": [{"url": "https://huggingface.co/blog/ai-webtv", "summary": "The AI WebTV project showcases the potential of text-to-video models like Zeroscope and MusicGen in generating entertaining videos. Created using Hugging Face services, it utilizes a combination of ChatGPT, Zeroscope V2, and FILM to create high-quality video clips with accompanying music. An exciting example of advancements in AI technology for audio-visual synthesis.", "title": "Building an AI WebTV"}], "papers": [{"url": "https://crfm.stanford.edu/2023/07/17/flash2.html", "summary": "Stanford University has introduced FlashAttention-2, an algorithm that accelerates attention and reduces memory usage in language models. The updated version is 2x faster than the original FlashAttention and achieves improved performance through better parallelism and work partitioning techniques. It now parallelizes over the sequence length dimension for faster processing, benefiting smaller batch sizes or fewer heads with long sequences. This development is significant for AI professionals seeking to enhance the scalability of Transformer-based models.", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"url": "https://arxiv.org/abs/2307.03172", "summary": "This study investigates the performance of language models in utilizing extended contexts for tasks such as question answering and retrieval. While models excel in finding relevant information at the start or end of input, their performance declines when accessing middle sections of long contexts. The study highlights the challenges of utilizing long contexts and the necessity for future improvements.", "title": "Lost in the Middle: How Language Models Use Long Contexts"}, {"url": "https://arxiv.org/abs/2307.07164", "summary": "Researchers have developed a framework that uses dense retrievers to automatically select high-quality examples for in-context learning of LLMs. Experimental results demonstrate its effectiveness in improving LLM performance by retrieving similar and contextually relevant examples.", "title": "Learning to Retrieve In-Context Examples for Large Language Models"}, {"url": "https://arxiv.org/abs/2307.09009", "summary": "A research study examined the performance of GPT-3.5 and GPT-4 on various tasks over time. It found some significant variations in their behavior, with GPT-4\u2019s accuracy in identifying prime numbers dropping from March to June 2023. Both models also displayed an increase in formatting mistakes during code generation.", "title": "How is ChatGPT\u2019s behavior changing over time?"}, {"url": "https://google-research.github.io/seanet/brain2music/", "summary": "Researchers have developed a method for reconstructing music from brain activity using fMRI. By correlating brain regions with the MusicLM model activations, they can predict and recreate music similar to what the human subjects experienced. MusicLM, an AI language model trained on diverse music, plays a key role in generating high-quality audio compositions.", "title": "Brain2Music"}, {"url": "https://github.com/RayVentura/ShortGPT", "summary": "ShortGPT is an AI framework that simplifies short video content creation by automating tasks such as video creation, voiceover synthesis, footage sourcing, and editing. It supports multiple languages and automates caption generation using web and Pexels API.", "title": "ShortGPT"}, {"url": "https://arxiv.org/abs/2307.06962", "summary": "A new text generation approach, called Copy Is All You Need, improves quality by copying text segments from existing collections. This method utilizes contextualized text representations and efficient vector search toolkits to generate text, resulting in comparable inference efficiency to token-level autoregressive models.", "title": "Copy Is All You Need"}, {"url": "https://arxiv.org/abs/2307.09668", "summary": "Researchers have found that using language models and vision language models in reinforcement learning agents can address key challenges in the field. By leveraging the knowledge stored in these models, agents are able to explore sparse-reward environments, reuse data for learning, schedule skills for novel tasks, and learn from expert observations. In simulated robotic environments, language-centric RL agents outperformed baseline models in stacking objects task, demonstrating the potential of Foundation Models in RL.", "title": "Towards A Unified Agent with Foundation Models"}], "datetime": "2023-07-24"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-july-17th-2023-2e1efc294bcf", "news": [{"url": "https://www.anthropic.com/index/claude-2", "summary": "Anthropic has released Claude 2, an advanced AI model that outperforms Claude 1.3 in various evaluations, achieving impressive scores on Codex HumanEval and GSM8k. Claude 2 excels in coding, math, and even scored higher on the Bar exam. Additionally, it offers improved effectiveness in harmless responses and can handle inputs up to 100K tokens, making it suitable for processing larger texts.", "title": "Anthropic releases Claude 2", "topics": ["Claude", "Anthropic"], "sentiment": "positive"}, {"url": "https://www.reuters.com/technology/elon-musks-ai-firm-xai-launches-website-2023-07-12/", "summary": "Elon Musk\u2019s new AI startup, xAI, is recruiting top engineers from tech giants like Google and Microsoft to develop a \u201cmaximally curious\u201d AI. Although separate from X Corp, xAI will closely collaborate with companies like Twitter and Tesla, aiming to bring about breakthroughs and innovation in the field of AI through synergistic efforts.", "title": "Elon Musk launches AI firm xAI as he looks to take on OpenAI", "topics": ["Google", "Microsoft", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.theverge.com/2023/7/8/23788265/google-med-palm-2-mayo-clinic-chatbot-bard-chatgpt", "summary": "Google\u2019s Med-PaLM 2, an AI system designed to provide medical information, has been undergoing testing in a hospital setting. While the model shows promise in areas with limited access to doctors, it does exhibit some inaccuracies and irrelevant responses. In terms of reasoning, consensus-supported answers, and comprehension, Med-PaLM 2 performs comparably to human doctors.", "title": "Google\u2019s medical AI chatbot is already being tested in hospitals", "topics": ["AI in healthcare", "Google"], "sentiment": "positive"}, {"url": "https://blog.google/products/bard/google-bard-new-features-update-july-2023/", "summary": "Bard, a language model, has expanded its availability worldwide and now supports multiple languages. New features include the ability to listen to Bard\u2019s responses, customize the tone and style of its output, pin and rename past conversations, export Python code to Replit and Google Colab, share responses with others, and utilize images in prompts with the help of Google Lens integration.", "title": "Bard\u2019s latest update: more features, languages and countries", "topics": ["Text-to-speech", "AI for images", "AI for coding", "Multimodal AI (image, video, audio)", "Google Gemini", "Google"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2023/07/11/shutterstock-expands-deal-with-openai-to-build-generative-ai-tools/", "summary": "OpenAI and Shutterstock have announced a partnership where OpenAI will use Shutterstock\u2019s media library to train its AI models. In return, Shutterstock gains priority access to OpenAI\u2019s advanced image transformation tools. Shutterstock is also working towards becoming a leader in generative AI by collaborating with top AI companies and compensating artists for their contributions to training the AI.", "title": "Shutterstock expands deal with OpenAI to build generative AI tools", "topics": ["AI and copyright", "AI datasets", "AI for images", "Multimodal AI (image, video, audio)", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.reuters.com/technology/nvidia-invests-50-mln-recursion-train-ai-models-drug-discovery-2023-07-12/", "summary": "Nvidia invests $50M in Recursion, a biotech company using AI to revolutionize drug discovery. This partnership allows Recursion to utilize Nvidia\u2019s platform and access their advanced AI technology. Recursion\u2019s share price surged by 83% post-announcement, highlighting the market\u2019s recognition of AI\u2019s significance in drug discovery.", "title": "Nvidia deepens bets on AI in drug discovery with Recursion investment", "topics": ["AI in healthcare", "Funding", "NVIDIA", "AI Chips and GPUs"], "sentiment": "positive"}, {"url": "https://blog.google/technology/ai/notebooklm-google-ai/", "summary": "Google has introduced NotebookLM, a tool that combines Google Drive documents with LLMs. It automatically generates summaries, identifies key topics, and suggests questions to enhance understanding. Users can also ask questions about uploaded documents and request ideas or scripts related to specific topics. This tool can be beneficial for AI professionals seeking efficient document management and idea generation.", "title": "NotebookLM: How to try Google\u2019s experimental AI-first notebook", "topics": ["Google"], "sentiment": "positive"}, {"url": "https://www.theguardian.com/technology/2023/jul/10/programs-to-detect-ai-discriminate-against-non-native-english-speakers-shows-study", "summary": "AI text detectors are facing bias in mistakenly labeling non-native English speakers\u2019 articles as AI-generated. Stanford researchers found that over 50% of essays written by non-native speakers were flagged as AI-generated, emphasizing the need to address discrimination faced by non-native writers using AI detectors. This has implications for college/job applications and search engine algorithms, potentially harming academic careers and psychological well-being.", "title": "Programs to detect AI discriminate against non-native English speakers, shows study", "topics": ["AI safety", "AI regulation"], "sentiment": "negative"}], "guides": [{"url": "https://www.philschmid.de/sagemaker-falcon-qlora", "summary": "This guide explains how to use QLoRA on Amazon SageMaker for finetuning large language models. It highlights the usage of tools like Hugging Face Transformers, Accelerate, and PEFT library for adapting pre-trained language models to different applications without fine-tuning all parameters. Additionally, it emphasizes the advantages of QLoRA, such as efficient fine-tuning on a single GPU with up to 65 billion parameters and state-of-the-art results in language tasks. PEFT is also mentioned as a game-changing tool for efficient adaptation of pre-trained language models.", "title": "Train LLMs using QLoRA on Amazon SageMaker"}, {"url": "https://huggingface.co/learn/audio-course/chapter0/introduction", "summary": "Hugging Face is offering a free and open-source Deep Learning course on audio transformers for AI professionals. This comprehensive course covers speech recognition, audio classification, and generating speech from text, providing theoretical components, quizzes, and practical exercises.", "title": "A new Hugging Face Audio course"}, {"url": "https://www.oneusefulthing.org/p/what-ai-can-do-with-a-toolbox-getting", "summary": "OpenAI has released the GPT-4 Code Interpreter plugin for ChatGPT Plus users, enabling non-coders in the artificial intelligence field to utilize AI capabilities. This tool reduces errors and improves accuracy by interacting with Python code instead of manipulating data, making it user-friendly for data analysis and AI interaction.", "title": "What AI can do with a toolbox\u2026 Getting started with Code Interpreter"}], "papers": [{"url": "https://arxiv.org/abs/2307.06290", "summary": "\u201cInstructMining\u201d is a method that enhances the performance of large language models (LLMs) by automatically selecting high-quality instruction data. With a 42.5% improvement over models using unfiltered data, this approach focuses on the significance of data quality in fine-tuning LLMs for effective interpretation of instructions. The selection process employs natural language indicators such as naturalness, coherence, and understandability.", "title": "Instruction Mining: High-Quality Instruction Data Selection for Large Language Models"}, {"url": "https://arxiv.org/abs/2307.05222v1", "summary": "Emu is a powerful foundational model that excels in generating images and texts in multimodal contexts. It can handle various types of data input, such as images, text, and videos, and outperforms other large multimodal models in tasks like image captioning, visual question answering, and text-to-image generation. Emu\u2019s strength lies in its ability to explore pretraining data sources at scale.", "title": "Generative Pretraining in Multimodality"}, {"url": "https://arxiv.org/abs/2307.03692", "summary": "The Instruction Following Score (IFS) is a new metric that evaluates language models\u2019 ability to follow instructions. It helps differentiate between base and instruct models and can prevent unnecessary finetuning that may alter a model\u2019s semantics. Additionally, researchers found that when the IFS plateaus, significant semantic shifts occur, highlighting the relationship between instruction following and model semantics.", "title": "Becoming self-instruct: introducing early stopping criteria for minimal instruct tuning"}, {"url": "https://arxiv.org/abs/2307.03601", "summary": "GPT4RoI is an innovative model that enhances vision-language tasks by incorporating regions of interest. This allows for precise alignment between visual features and language embeddings, enabling users to interact with the model through language and spatial instructions. Moreover, GPT4RoI supports multi-region spatial instructions, expanding its multimodal capabilities. Its versatility lies in the ability to use any object detector as a spatial instruction provider, enhancing its understanding abilities with object details.", "title": "GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest."}], "datetime": "2023-07-17"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-july-10th-2023-8d37a4201f8c", "news": [{"url": "https://openai.com/blog/gpt-4-api-general-availability", "summary": "OpenAI has announced the general availability of the GPT-4 API and the automatic upgrade of GPT-3 models to new models from January 4, 2024. Developers using text-davinci-003 are advised to upgrade to gpt-3.5-turbo-instruct and specify it as the \u201cmodel\u201d in API requests for a smooth transition. OpenAI also offers priority access to GPT-3.5 Turbo and GPT-4 fine-tuning for users with fine-tuned older models, understanding the challenges of transitioning from these models.", "title": "GPT-4 API general availability and deprecation of older models in the Completions API", "topics": ["Model release", "GPT-3, GPT-3.5, and GPT-3.5 turbo", "GPT-4 and GPT-4 turbo", "OpenAI"], "sentiment": "positive"}, {"url": "https://openai.com/blog/introducing-superalignment", "summary": "This article discusses the concept of superalignment and the need for scientific and technical breakthroughs to ensure that highly intelligent AI systems align with human intentions. It emphasizes the importance of establishing innovative governance institutions and exploring new approaches to achieve this alignment.", "title": "Introducing Superalignment", "topics": ["AI safety", "AI regulation"], "sentiment": "positive"}, {"url": "https://smallbiztrends.com/2023/06/aws-launches-100m-generative-ai-innovation-center.html", "summary": "AWS is launching the AWS Generative AI Innovation Center with a $100 million investment. The center aims to assist small businesses in innovating and improving their operations through cost-effective and flexible generative AI services. It will provide free workshops, engagements, training, and access to tools like Amazon CodeWhisperer and Amazon Bedrock. The Innovation Center is already working with companies like Highspot, Lonely Planet, Ryanair, and Twilio on generative AI solutions.", "title": "AWS Launches $100M Generative AI Innovation Center", "topics": ["AI for coding", "Amazon", "Funding"], "sentiment": "positive"}, {"url": "https://www.tomshardware.com/news/hive-blockchain-pivoting-to-ai", "summary": "Cryptomining firm Hive Blockchain is redirecting their efforts from Ethereum mining to AI workloads. With 38,000 GPUs at their disposal, they aim to generate revenue while still utilizing some GPU power for crypto mining. However, transitioning to AI compute poses challenges as older ETH mining GPUs have limited value in this market.", "title": "Miner Pivots 38,000 GPUs From Crypto to AI", "topics": ["AI Chips and GPUs"], "sentiment": "positive"}], "guides": [{"url": "https://www.projectpro.io/article/gpus-for-machine-learning/677", "summary": "This article highlights the increasing use of GPUs in machine learning and provides a guide on choosing the best GPUs for AI applications. It mentions key factors to consider, such as compatibility and memory capacity, and identifies top GPU options from NVIDIA, including the Titan RTX and Tesla V100. The post also mentions budget-friendly alternatives like the EVGA GeForce GTX 1080 and AMD Radeon GPUs.", "title": "Best GPUs for Machine Learning for Your Next Project"}, {"url": "https://txt.cohere.com/intriguing-properties-of-quantization-at-scale/", "summary": "Recent research shows that the quality of Large Language Models Post-Training Quantization (PTQ) is heavily influenced by pre-training hyperparameters. The study found that optimization choices such as weight decay, gradient clipping, and data type choice significantly impact PTQ performance, with float16 and bfloat16 having a notable influence. The research also highlights the importance of optimization choices in developing robust language models, as certain models showed improvements in PTQ performance while others experienced significant degradation.", "title": "Intriguing Properties of Quantization at Scale"}, {"url": "https://thealgorithmicbridge.substack.com/p/how-the-great-ai-flood-could-kill", "summary": "This article raises concerns about the proliferation of low-quality AI-generated content and its potential to overshadow genuine human creativity on the internet. It calls for a balance between convenience and quality, highlighting the need to support and nurture authentic human creations in the digital realm.", "title": "How the Great AI Flood Could Kill the Internet"}, {"url": "https://www.latent.space/p/ai-engineer", "summary": "The rise of AI Engineers is revolutionizing the field of applied AI, thanks to accessible Foundation Models and open source APIs. These engineers play a vital role in effectively applying AI technologies, evaluating models, and utilizing the right tools. The demand for AI Engineers is on the rise, and a deep understanding of engineering principles and practical experience are key drivers of success in this field. No PhD is required to excel as an AI Engineer.", "title": "The Rise of the AI Engineer"}], "papers": [{"url": "https://arxiv.org/abs//2307.02486", "summary": "LongNet, a new Transformer model, can handle sequences of over 1 billion tokens. It uses dilated attention to efficiently process longer sequences while maintaining performance on shorter ones. This opens up exciting possibilities for modeling extensive sequences, such as entire corpora or the entire Internet.", "title": "LongNet: Scaling Transformers to 1,000,000,000 Tokens"}, {"url": "https://arxiv.org/abs/2305.17493v2", "summary": "GPT-3 and GPT-4 have brought significant advancements to language tasks, but the rise of LLMs dominating online text and images poses a challenge. Researchers explore \u201cmodel collapse,\u201d the disappearance of original content distribution when models are trained using content from other models. This phenomenon affects LLMs, Variational Autoencoders, and Gaussian Mixture Models, emphasizing the need to understand and preserve data from genuine human interactions to maintain the benefits of web-collected data.", "title": "The Curse of Recursion: Training on Generated Data Makes Models Forget"}, {"url": "https://arxiv.org/abs/2307.03170v1", "summary": "The Focused Transformer (FoT) is a novel technique that uses contrastive training to extend the effective context length for large language models. By addressing the distraction issue and enhancing the (key, value) space structure, the FoT enables managing a remarkable 256k tokens context length for passkey retrieval.", "title": "Focused Transformer: Contrastive Training for Context Scaling"}, {"url": "https://arxiv.org/abs/2307.03109v1", "summary": "This article provides a comprehensive review of evaluation methods for Language Models. It focuses on what to evaluate (including various dimensions like reasoning, ethics, and applications), where to evaluate (both general and specific benchmarks), and how to evaluate (including human evaluations versus automatic evaluations). It serves as a valuable resource for evaluating LLMs and gaining a better understanding of their potential risks.", "title": "A Survey on Evaluation of Large Language Models"}, {"url": "https://arxiv.org/abs/2306.15895", "summary": "A study explores the use of diverse prompts when generating synthetic datasets with LLMs for NLP tasks. By using \u201cattributed prompts\u201d instead of \u201csimple class-conditional prompts,\u201d researchers found improved model performance, higher data quality, and attribute diversity. Additionally, attributed prompts achieved similar performance to class-conditional prompts at just 5% of the querying cost. This study highlights the importance of considering prompt diversity in generating accurate and fair synthetic data for AI applications.", "title": "Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias"}, {"url": "https://arxiv.org/abs/2306.16092v1", "summary": "ChatLaw is an open-source legal language model specifically designed for the Chinese legal domain. It utilizes a combination of vector and keyword retrieval techniques to address model hallucinations during data retrieval, resulting in more accurate responses. Self-attention is employed to improve accuracy and reliability in reference data, making ChatLaw a valuable tool for AI professionals in the legal field.", "title": "ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases"}, {"url": "https://arxiv.org/abs/2306.16934", "summary": "DreamDiffusion is an innovative method that uses EEG signals to generate high-quality images directly. By utilizing pre-trained models and advanced signal modeling techniques, it overcomes challenges like limited information and noise. Additionally, it incorporates a CLIP image encoder for better alignment and has potential applications in neuroscience and computer vision, improving artistic creation and aiding therapy for individuals with autism or language disabilities.", "title": "DreamDiffusion: Generating High-Quality Images from Brain EEG Signals"}, {"url": "https://arxiv.org/abs/2306.15658v1", "summary": "The CLIPA-v2 model achieves an impressive zero-shot ImageNet accuracy of 81.1% by leveraging an inverse scaling law for CLIP training. This advancement surpasses the OpenCLIP model by 1.0% and reduces costs by approximately 39x. With a $10,000 budget, CLIPA-v2 achieves 81.1% accuracy, demonstrating the model\u2019s strong performance and cost-effectiveness in artificial intelligence.", "title": "CLIPA-v2: Scaling CLIP Training with 81.1% Zero-shot ImageNet Accuracy"}], "datetime": "2023-07-10"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-july-3rd-2023-65d67c975e41", "news": [{"url": "https://techcrunch.com/2023/06/26/deepmind-claims-its-next-chatbot-will-rival-chatgpt/", "summary": "DeepMind is working on a new chatbot called Gemini to rival or even surpass OpenAI\u2019s ChatGPT. With the aim of combining AlphaGo\u2019s success and LLM\u2019s language abilities, DeepMind seeks to dominate the generative AI market, which is predicted to reach $109 billion by 2030.", "title": "DeepMind claims its next chatbot will rival ChatGPT", "topics": ["DeepMind", "Google Gemini", "ChatGPT", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.coursera.org/learn/generative-ai-with-llms", "summary": "DeepLearning.ai and AWS have collaborated to create the \u201cGenerative AI with Large Language Models\u201d course on Coursera. This course provides AI developers with the knowledge and skills necessary to build and utilize powerful large language model applications. It covers the transformer architecture of LLMs, training techniques, model selection, performance optimization, and hands-on labs for practical experience. Completion of the course awards a Coursera certificate demonstrating proficiency in generative AI with large language models.", "title": "Generative AI with Large Language Models Course", "topics": ["Amazon"], "sentiment": "positive"}, {"url": "https://beta.elevenlabs.io/blog/voice-library/", "summary": "ElevenLabs has launched Voice Library, a community platform integrated with a multilingual model that enables the creation of lifelike synthetic voices with consistent primary speech characteristics for commercial applications. The Voice Design tool allows users to customize age, gender, and accent to generate unique and natural-sounding voices.", "title": "ElevenLabs introduces its Voice Library", "topics": ["Text-to-speech"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2023/06/26/adobe-indemnity-clause-designed-to-ease-enterprise-fears-about-ai-generated-art/", "summary": "Adobe offers an indemnity clause to address copyright concerns surrounding their generative AI tool Firefly. By training the model on legal content and promising to cover any copyright claims, Adobe aims to ease enterprise users\u2019 worries and ensure the legality and safety of AI-generated artwork.", "title": "Adobe indemnity clause designed to ease enterprise fears about AI-generated art", "topics": ["AI and copyright", "AI for images", "AI regulation"], "sentiment": "positive"}, {"url": "https://www.giskard.ai/knowledge/the-open-source-ai-imperative-key-takeaways-from-hugging-face-ceos-testimony-to-the-us-congress", "summary": "The CEO of Hugging Face, Cl\u00e9ment Delangue, recently testified before the US Congress on Open-Source AI. In his testimony, he highlighted the importance of open-source AI in advancing innovation, promoting fair competition, and ensuring responsible development. Delangue emphasized that open-source principles democratize AI and foster a more inclusive and collaborative future in the field.", "title": "Advancing innovation with Open-Source AI: Hugging Face CEO testifies before the US Congress", "topics": ["Hugging Face", "AI regulation"], "sentiment": "positive"}, {"url": "https://www.mosaicml.com/blog/mosaicml-databricks-generative-ai-for-all", "summary": "MosaicML, a startup focused on democratizing large-scale neural network training and inference, is teaming up with Databricks in a $1.3B deal. The collaboration aims to further advance generative AI software expertise and expand customer reach and engineering capabilities.", "title": "MosaicML Agrees to Join Databricks to Power Generative AI for All", "topics": ["Funding"], "sentiment": "positive"}, {"url": "https://openai.com/blog/introducing-openai-london", "summary": "OpenAI has recently opened its first office in London, aiming to tap into the city\u2019s talent and expertise in the artificial intelligence field. This move is expected to enhance OpenAI\u2019s research, engineering, and go-to-market functions.", "title": "Introducing OpenAI London", "topics": ["OpenAI"], "sentiment": "positive"}], "guides": [{"url": "https://ai.googleblog.com/2023/06/announcing-first-machine-unlearning.html", "summary": "Google has announced the first Machine Unlearning Challenge, a collaboration between academic and industrial researchers. This new area of machine learning focuses on eliminating the impact of certain training examples from a model to protect privacy rights. The challenge, held on Kaggle, aims to evaluate unlearning models\u2019 forgetting quality and model utility, providing insights for improvement. Participants will have access to a starter kit with toy data and will need to train age predictor models on face images while forgetting specific ones for privacy.", "title": "Announcing the first Machine Unlearning Challenge"}, {"url": "https://pub.towardsai.net/the-generative-ai-revolution-exploring-the-current-landscape-4b89998fcc5f", "summary": "This article describes the current landscape of Generative AI, highlighting its ability to generate coherent text, images, and code. It mentions key models such as the Transformer, GPT family, Palm models, Chinchilla model, Megatron Turing model, and LlaMa models. The post also mentions the potential impact of Generative AI in fields like animation, gaming, art, movies, and architecture.", "title": "The Generative AI Revolution: Exploring the Current Landscape"}, {"url": "https://githubnext.com/projects/code-atlas/", "summary": "Code Atlas is a project that combines fluid reasoning and rigid structure to optimize interactions with LLMs. It leverages LLMs by using a unique approach that divides logic into easily understandable pieces through Javascript snippets. Code Atlas simplifies complex problems, enables experimentation, and provides insight into LLM responses for debugging and improvement. This revolutionary tool strikes a balance between flexibility and reliability, making it valuable for AI professionals.", "title": "GitHub Next \u2014 Code Atlas"}, {"url": "/lightspeed-venture-partners/eight-ai-startups-winning-the-race-for-tech-talent-571a18b03642", "summary": "Eight AI startups are leading the way in attracting tech talent, as generative AI continues to revolutionize various industries. With a growing interest in AI projects, the demand for experts to develop innovative features and manage complex technology is soaring.", "title": "Eight AI Startups Winning the Race for Tech Talent"}], "papers": [{"url": "https://arxiv.org/abs/2306.14289v1", "summary": "MobileSAM is a lightweight solution for object segmentation on resource-constrained edge devices. It introduces a mobile-friendly version of the Segment Anything model (SAM) by replacing the heavyweight image encoder with a lightweight one. Through decoupled distillation, knowledge from the original image encoder is transferred, providing an efficient and compact solution. MobileSAM achieves faster performance and smaller footprint compared to its competitor, making it an optimal choice for AI professionals in need of exceptional object segmentation capabilities.", "title": "Faster Segment Anything: Towards Lightweight SAM for Mobile Applications"}, {"url": "https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models", "summary": "The repo has a curated list of papers and datasets related to Multimodal Large Language Models (MLLMs). Researchers and practitioners in the artificial intelligence field can explore various topics such as multimodal instruction tuning, in-context learning, chain-of-thought, and LLM-aided visual reasoning.", "title": "BradyFU/Awesome-Multimodal-Large-Language-Models"}, {"url": "https://arxiv.org/abs/2306.16410v1", "summary": "LENS (Language Models ENhanced to See) is a modular approach that enables the use of language models to analyze outputs from vision modules, extracting detailed information about images. This tech allows LLMs to perform tasks such as object recognition and vision and language (V&L) tasks without the need for additional training or data, resulting in comparable or superior performance compared to other models like Kosmos and Flamingo. LENS not only synergizes computer vision and natural language processing, but also reduces computational costs, making it highly valuable for AI professionals.", "title": "Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language"}, {"url": "https://arxiv.org/abs/2306.16927v1", "summary": "A recent survey of 250 papers delves into the motivation, methods, challenges, and future trends in end-to-end autonomous driving. This analysis focuses on the advantages of end-to-end systems, which integrate perception, prediction, and planning functionalities into one model for better performance. The survey also highlights key challenges faced by autonomous driving systems and explores advancements in foundation models and visual pre-training within the end-to-end driving framework. For artificial intelligence professionals interested in autonomous driving, this survey provides valuable insights and potential improvements in this field.", "title": "End-to-end Autonomous Driving: Challenges and Frontiers"}], "datetime": "2023-07-03"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-june-26th-2023-44adfc995aeb", "news": [{"url": "https://www.deepmind.com/blog/robocat-a-self-improving-robotic-agent", "summary": "DeepMind\u2019s new AI agent, RoboCat, can learn and generate new training data to improve its skills with different robotic arms, allowing it to operate new robotic arms within hours with just 100 demonstrations. It\u2019s success rate on novel tasks more than doubled from its first version to the latest version, thanks to its expanding experience.", "title": "RoboCat: A self-improving robotic agent", "topics": ["Reinforcement learning", "Robotics", "DeepMind"], "sentiment": "positive"}, {"url": "https://www.cs.cmu.edu/news/2023/VRB_robot_tasks", "summary": "CMU researchers have developed a new method for teaching robots household tasks by training them to observe videos of humans completing those tasks. This approach can improve robots\u2019 performance in cooking and cleaning by allowing them to master 12 different actions.", "title": "CMU Researchers Expand Ability of Robots To Learn From Videos", "topics": ["Robotics", "Multimodal AI (image, video, audio)"], "sentiment": "positive"}, {"url": "https://beta.elevenlabs.io/blog/elevenlabs-launches-new-generative-voice-ai-products-and-announces-19m-series-a-round-led-by-nat-friedman-daniel-gross-and-andreessen-horowitz/", "summary": "ElevenLabs raises $19M in Series A funding for their AI-powered synthetic voice technology that allows content creators to easily control AI-generated audio content, with plans to launch an AI dubbing tool and an AI Speech Classifier for transparency and safety in generative media.", "title": "ElevenLabs Launches New Generative Voice AI Products and Announces $19m Series A Round", "topics": ["Text-to-speech", "Funding"], "sentiment": "positive"}, {"url": "https://www.reuters.com/technology/inside-chinas-underground-market-high-end-nvidia-ai-chips-2023-06-19/", "summary": "Despite U.S. export restrictions, Chinese demand for high-end Nvidia AI chips remains strong and vendors in Shenzhen and Hong Kong have emerged to offer them at steep prices in a de facto underground market. The exact volume of chip flow is unknown, but local authorities are also reported to be buyers.", "title": "Inside China\u2019s underground market for high-end Nvidia AI chips", "topics": ["NVIDIA", "AI Chips and GPUs", "AI regulation"], "sentiment": "negative"}, {"url": "https://ai.facebook.com/blog/voicebox-generative-ai-model-speech/", "summary": "Meta AI has developed Voicebox, a new model that uses a Flow Matching model to train on large and diverse datasets, enabling it to generate high-quality synthesized speech without specific training. The model can match audio styles, read text passages in multiple languages, and edit speech segments within audio recordings. The research paper and audio samples are available, but the model and code remain private to prevent misuse.", "title": "Introducing Voicebox: The first generative AI model for speech to generalize across tasks with state-of-the-art performance", "topics": ["Text-to-speech", "Multimodal AI (image, video, audio)", "Meta"], "sentiment": "positive"}, {"url": "https://stackoverflow.blog/2023/06/07/self-healing-code-is-the-future-of-software-development/", "summary": "The use of generative AI in software development allows for self-healing code, improving performance and reducing errors. However, maintaining high-quality output and avoiding technical debt requires established best practices. Stack Overflow is pursuing AI improvements for their platform through suggestion and formatting enhancements.", "title": "Self-healing code is the future of software development", "topics": ["AI for coding"], "sentiment": "positive"}, {"url": "https://www.tomshardware.com/news/chinas-bytedance-has-gobbled-up-dollar1-billion-of-nvidia-gpus-for-ai-this-year", "summary": "Chinese tech giant ByteDance has reportedly invested $1 billion in Nvidia\u2019s HPC products, including the A100 and H800 cards, totaling about 100,000 units, to fulfill the high demand for AI technology. China recognizes the impact of AI on the global market and has embraced it, investing heavily in HPC hardware.", "title": "China\u2019s ByteDance Has Gobbled Up $1 Billion of Nvidia GPUs for AI This Year", "topics": ["NVIDIA", "AI Chips and GPUs"], "sentiment": "positive"}, {"url": "https://www.wired.com/story/pixar-elemental-artificial-intelligence-flames/", "summary": "Pixar used neural style transfer and collaborated with Disney Research Studios to solve the challenge of capturing the ethereal nature of fire in their film \u201cElemental\u201d. This ML innovation saved production time and reduced costs.", "title": "Pixar Used AI to Stoke the Flames in \u2018Elemental\u2019", "topics": ["AI for images", "Multimodal AI (image, video, audio)"], "sentiment": "positive"}, {"url": "https://learn.activeloop.ai/courses/langchain", "summary": "LangChain\u2019s free AI course, LangChain & Vector Databases in Production, offers 50+ lessons and 10+ projects that cover API integration, prompt engineering, and production use. It teaches how to use Deep Lake, a versatile vector database for AI data that includes text, images, videos, and multiple embeddings. Basic Python knowledge, Jupyter Notebooks understanding, and GitHub familiarity are prerequisites. The course is designed to make AI practical and accessible, tailored to both experienced devs and enthusiasts.", "title": "LangChain & Vector Databases in Production Course", "topics": ["LangChain and LlamaIndex", "Multimodal AI (image, video, audio)"], "sentiment": "positive"}], "guides": [{"url": "https://alexsandu.substack.com/p/market-map-and-analysis-gen-ai-search", "summary": "The article discusses the top AI search companies, including You.com and Perplexity.ai in the consumer search space, and Vectara, Dashworks, Nuclia, Metaphor, and Hebbia in the enterprise space. Generative AI firms are improving their services to challenge Google, Microsoft, and Baidu.", "title": "Market Map and Analysis: Gen AI Search Companies"}, {"url": "https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/", "summary": "The article introduces a reference architecture for LLM app stack using in-context learning, consisting of three parts: data preprocessing and embeddings, prompt construction and retrieval, and prompt execution. In-context learning simplifies AI and is useful for smaller datasets, allowing for real-time data incorporation. The future of LLM app architecture is expected to centralize with AI agent frameworks.", "title": "Emerging Architectures for LLM Applications"}, {"url": "https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c", "summary": "Researchers have implemented several optimization techniques to overcome challenges in training Large Language Models. These include using ALiBi for better extrapolation, employing Sparse Attention to consider only relevant tokens, using FlashAttention to optimize GPU memory, sharing weights across attention heads through Multi-Query Attention, and using Conditional Computation to optimize speed and accuracy. These optimizations enable training on smaller contexts and fine-tuning on up to 100K tokens.", "title": "The Secret Sauce behind 100K context window in LLMs: all tricks in one place"}, {"url": "https://towardsdatascience.com/harnessing-the-falcon-40b-model-the-most-powerful-open-source-llm-f70010bc8a10", "summary": "Falcon 40B, an open-source LLM, is gaining popularity among AI professionals and enthusiasts. An informative article on how to use Falcon 40B with Hugging Face is available for those interested in exploring its power.", "title": "A guide to the Falcon 40B model using Hugging Face"}, {"url": "https://towardsdatascience.com/how-gpt-works-a-metaphoric-explanation-of-key-value-query-in-attention-using-a-tale-of-potion-8c66ace1f470", "summary": "The article explains the concept of Key, Value, and Query in Attention using a metaphor of potions in the functioning of GPT and how they give insights into predicting the next word in a text.", "title": "How GPT works: A Metaphoric Explanation of Key, Value, Query in Attention, using a Tale of Potion"}], "papers": [{"url": "https://arxiv.org/abs/2306.08302", "summary": "Researchers propose a roadmap for unifying Language Models and Knowledge Graphs in AI, using KG-enhanced LLMs, LLM-augmented KGs, and Synergized LLMs+KGs frameworks to improve performance in knowledge representation and reasoning for improved downstream tasks.", "title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap"}, {"url": "https://arxiv.org/pdf/2306.11644v1.pdf", "summary": "Phi-1, a new language model for coding, demonstrates the power of \u201ctextbook quality\u201d data and achieves great accuracy on HumanEval and MBPP despite its smaller size. It specializes in Python coding and outperforms larger models by leveraging high-quality examples.", "title": "Textbooks Are All You Need"}, {"url": "https://arxiv.org/abs/2306.11695v1", "summary": "Wanda is a novel pruning approach for Large Language Models that induces sparsity without retraining or weight update. It efficiently finds efficient sparse networks from pre-trained models, outperforming pruning approaches and matching/surpassing other recent methods with lower computational costs.", "title": "A Simple and Effective Pruning Approach for Large Language Models"}, {"url": "https://arxiv.org/abs/2306.12001v1", "summary": "A new paper outlines four main areas of potential dangers in advanced AI systems, categorizing the risks and offering practical suggestions for mitigation. The areas include malicious uses, AI race, organizational risks, and rogue AIs. The paper emphasizes the importance of understanding the interplay between humans and AI for minimizing these risks.", "title": "An Overview of Catastrophic AI Risks"}], "datetime": "2023-06-26"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-june-20th-2023-6c75433a42f4", "news": [{"url": "https://openai.com/blog/function-calling-and-other-api-updates", "summary": "OpenAI has released several new updates, including a 16k context version of GPT-3.5-turbo, a new API for calling user-defined functions, cost reductions for models and input tokens, and updated versions of GPT-4 and GPT-3.5-turbo. The updates aim to improve GPT\u2019s capabilities and make it easier to connect with external tools and APIs.", "title": "Function calling and other API updates", "topics": ["Model release", "GPT-3, GPT-3.5, and GPT-3.5 turbo", "GPT-4 and GPT-4 turbo", "OpenAI"], "sentiment": "positive"}, {"url": "https://ai.facebook.com/blog/yann-lecun-ai-model-i-jepa/", "summary": "The I-JEPA model employs self-supervised learning to capture common sense knowledge about the world and avoid limitations of generative approaches. Yann LeCun\u2019s vision for human-like AI is the foundation of this model.", "title": "The first AI model based on Yann LeCun\u2019s vision for more human-like AI", "topics": ["Meta"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2023/06/14/google-lens-can-now-search-for-skin-conditions/", "summary": "Google Lens now has a feature that can identify skin conditions and other physical maladies by analyzing uploaded images. The feature can be integrated with chatbots to provide accurate answers about objects in photos.", "title": "Google Lens can now search for skin conditions", "topics": ["AI in healthcare", "AI for images", "Multimodal AI (image, video, audio)", "Google"], "sentiment": "positive"}, {"url": "https://www.washingtonpost.com/technology/2023/06/14/eu-parliament-approves-ai-act/", "summary": "The European Parliament has approved the EU AI Act, which regulates AI to protect consumers from risks such as discrimination and misinformation. The act bans certain tools and sets limits on high-risk technologies, promoting responsible AI development through transparency and a risk-based approach. It may also impact international policies and urge companies towards a unified regulatory landscape.", "title": "Europe moves ahead on AI regulation, challenging tech giants\u2019 power", "topics": ["AI regulation"], "sentiment": "positive"}, {"url": "https://www.semafor.com/article/06/13/2023/paul-mccartney-beatles-song-ai", "summary": "Paul McCartney used AI technology to restore John Lennon\u2019s voice and record a Beatles song from a 1978 cassette tape labeled \u201cFor Paul.\u201d This collaboration showcases the potential of AI in music preservation and opens up new opportunities for artists and fans in the future.", "title": "Paul McCartney records Beatles song with help of AI", "topics": ["Text-to-speech", "Multimodal AI (image, video, audio)"], "sentiment": "positive"}, {"url": "https://www.vice.com/en/article/93kkky/people-pirating-gpt4-scraping-openai-api-keys", "summary": "OpenAI warns of stolen API keys being advertised for unauthorized access to GPT-4, resulting in unexpected charges for the account holder. Users are advised to safeguard their keys and rotate them immediately if exposed. Automated scans are conducted to revoke identified exposed keys.", "title": "People Are Pirating GPT-4 By Scraping Exposed API Keys", "topics": ["GPT-4 and GPT-4 turbo", "OpenAI", "AI safety", "AI and copyright", "AI regulation"], "sentiment": "negative"}], "guides": [{"url": "https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c", "summary": "Researchers have developed optimization techniques to enable Large Language Models to train on longer context lengths, including replacing Positional Sinusoidal Encoding with ALiBi, using Sparse Attention, employing FlashAttention, implementing Multi-Query attention, and using Conditional Computation to optimize speed and accuracy. These tricks enable training on up to 100K tokens, making it easier to generate longer texts.", "title": "The Secret Sauce behind 100K context window in LLMs: all tricks in one place"}, {"url": "https://towardsdatascience.com/harnessing-the-falcon-40b-model-the-most-powerful-open-source-llm-f70010bc8a10", "summary": "The Falcon 40B is a powerful open-source LLM that is making waves in the AI community. This guide using Hugging Face can help enthusiasts and professionals start exploring its potential without barriers.", "title": "Harnessing the Falcon 40B Model, the Most Powerful Open-Source LLM"}, {"url": "https://towardsdatascience.com/how-gpt-works-a-metaphoric-explanation-of-key-value-query-in-attention-using-a-tale-of-potion-8c66ace1f470", "summary": "The article explains how GPT models use Key, Value, and Query in Attention to understand the context of words in a text. The author uses a potion-making metaphor to describe how these elements work together. Understanding this interplay helps in predicting the next word with coherence.", "title": "How GPT works: A Metaphoric Explanation of Key, Value, Query in Attention, using a Tale of Potion"}, {"url": "https://www.cyberpatterns.xyz/p/aiwork", "summary": "The article discusses the real-world applications of AI to boost productivity and creativity, with examples such as ChatGPT and DALL-E 2. AI is useful for generating ideas and visuals but should be combined with personal touch to create exceptional content.", "title": "How Me and My Internet Friends Are Using AI"}, {"url": "https://blog.langchain.dev/gpteam-a-multi-agent-simulation/", "summary": "GPTeam is an open-source multi-agent simulation inspired by Stanford\u2019s \u201cGenerative Agents\u201d paper. It allows creating a world with agents having unique personality, memories, and directives resulting in fascinating emergent behavior. The simulation setup is easy with JSON files, and it can be easily run with one command while observing agents\u2019 decision-making in real-time via a web interface.", "title": "GPTeam: A multi-agent simulation"}], "papers": [{"url": "https://superagi.com/", "summary": "SuperAGI is an open-source framework for building and running autonomous AI agents, aimed at giving developers a powerful and quick tool to create useful autonomous agents without constant human intervention.", "title": "Build, Manage & Run Autonomous AI Agents"}, {"url": "https://arxiv.org/abs/2306.06031v1", "summary": "FinGPT is a new open-source language model designed specifically for the finance sector, emphasizing high-quality financial data, and has various potential uses in finance such as robo-advising and algorithmic trading.", "title": "FinGPT: Open-Source Financial Large Language Models"}, {"url": "https://arxiv.org/abs/2306.04757v2", "summary": "INSTRUCT EVAL is a new evaluation suite for large language models that assesses problem-solving, writing skills, and alignment with human values, improving practical usability and model performance scalability. The study can be found in the arXiv repository.", "title": "INSTRUCT EVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models"}, {"url": "https://arxiv.org/abs/2306.04634v2", "summary": "A study shows that watermarks can effectively detect machine-generated text despite human paraphrasing and non-watermarked language models, with detection accuracy remaining high even with GPT-3.5 and purpose-built models. The findings suggest the potential of watermarks in identifying and combating spam, social media bots, and valueless content on the internet.", "title": "On the Reliability of Watermarks for Large Language"}, {"url": "https://arxiv.org/abs/2306.06070v1", "summary": "MIND2W EB is an open-source dataset of over 2,000 open-ended tasks from 137 real websites across 31 domains, designed to develop agents that can follow language instructions and complete tasks on any website. The dataset also addresses the challenges of using LLMs to create web agents by filtering HTML using a small language model, helping GPT-like models to acquire information and perform actions on HTML websites without predefined APIs or retriever tools.", "title": "MIND2W EB: Towards a Generalist Agent for the Web"}], "datetime": "2023-06-20"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-june-12th-2023-6c89b226f7f1", "news": [{"url": "https://www.deepmind.com/blog/alphadev-discovers-faster-sorting-algorithms", "summary": "AlphaDev, an AI system using reinforcement learning, has developed faster sorting algorithms for organizing data by starting from scratch and selecting computer assembly instructions using reinforcement learning. The new algorithms are up to 70% faster for shorter sequences and integrated into the LLVM libc++ standard library.", "title": "AlphaDev discovers faster sorting algorithms", "topics": ["Reinforcement learning", "AI for coding"], "sentiment": "positive"}, {"url": "/llamaindex-blog/building-the-data-framework-for-llms-bca068e89e0e", "summary": "LlamaIndex has raised $8.5M in seed funding and developed a toolkit to integrate user data with LLMs, enabling the creation of knowledge-intensive LLM apps such as search engines, chatbots, and analytics helpers. The project has impressive traction with 16K stars on Github, 20K Twitter followers and 200K monthly downloads, and 6K active Discord users.", "title": "Building the data framework for LLMs", "topics": ["LangChain and LlamaIndex", "Funding"], "sentiment": "positive"}, {"url": "https://blog.google/technology/ai/bard-improved-reasoning-google-sheets-export/", "summary": "Google has combined the capabilities of advanced language models and traditional code to enhance Bard\u2019s reasoning and math abilities. This new method of implicit code execution has boosted its accuracy by 30%.", "title": "Bard is getting better at logic and reasoning", "topics": ["Google Gemini", "Google"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2023/06/08/chatgpt-comes-to-ipad-adds-support-for-siri-and-shortcuts/", "summary": "OpenAI\u2019s latest app update allows iPad users to use Siri and Shortcuts with ChatGPT, one of the most popular chatbots available, offering custom shortcuts for hands-free operation. Apple has updated its App Store rules to prevent ChatGPT clones from being submitted.", "title": "ChatGPT comes to iPad, adds support for Siri and Shortcuts", "topics": ["Apple", "ChatGPT", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.together.xyz/blog/redpajama-7b", "summary": "Introducing the new RedPajama-INCITE models, optimized for few-shot tasks, which outperform similar models on HELM benchmarks. The project analyzed differences with previous models and incorporated community feedback. The models are available under Apache 2.0 license for AI professionals.", "title": "RedPajama 7B now available, instruct model outperforms all open 7B models on HELM benchmarks", "topics": ["Model release"], "sentiment": "positive"}, {"url": "https://www.cloudskillsboost.google/paths/118", "summary": "Google\u2019s new learning path on Generative AI covers everything from Large Language Models to deploying solutions on Google Cloud, with courses on Responsible AI, Image Generation, advanced topics, and a Quest to explore Generative AI Explorer in Vertex AI.", "title": "generative AI learning path, by google", "topics": ["Google"], "sentiment": "positive"}, {"url": "https://technomancers.ai/japan-goes-all-in-copyright-doesnt-apply-to-ai-training/", "summary": "Japan has announced that it will no longer enforce copyrights on data used for AI training, allowing for unhindered AI research and competition with the West.", "title": "Japan Goes All In: Copyright Doesn\u2019t Apply To AI Training", "topics": ["AI and copyright", "AI regulation"], "sentiment": "positive"}], "guides": [{"url": "https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard", "summary": "The Open LLM Leaderboard allows researchers to track advancements in large language models and chatbots by submitting Transformers models for automated evaluation on a GPU cluster. The leaderboard tests these models on various tasks including science questions, inference, multitasking accuracy, and truthful answers. It\u2019s a helpful resource for staying up-to-date and comparing LLM and chatbot models.", "title": "\ud83e\udd17 Open LLM Leaderboard"}, {"url": "https://www.semafor.com/article/06/07/2023/are-ai-startups-too-easy-to-copy", "summary": "AI startups face tough competition and investors worry about their ability to differentiate themselves in a highly saturated market. VCs look for network effects and proprietary datasets to invest in successful AI startups that gain quick traction.", "title": "Are AI startups too easy to copy?"}, {"url": "https://www.stockperformer.com/blog/is-ai-killing-the-stock-industry-a-data-perspective/", "summary": "AI-generated images are transforming the stock industry, resulting in boosted revenue for some agencies like Adobe Stock, but not all agencies accept them. It\u2019s still too early to determine AI-generated content\u2019s impact fully, as there are still unknown variables, and professional AI engineers are rare.", "title": "Is AI Killing the Stock Industry? A Data Perspective."}, {"url": "https://platform.openai.com/docs/guides/gpt-best-practices", "summary": "This guide on GPT best practices discusses strategies and tactics for using GPTs effectively. It emphasizes the importance of providing context and details to help GPTs produce better results, and suggests tactics such as breaking down complex tasks and measuring performance.", "title": "GPT best practices"}, {"url": "https://a16z.com/2023/06/06/ai-will-save-the-world/", "summary": "The article discusses how AI has the potential to revolutionize various fields, including coding, medicine, law, and arts. While there are concerns about its negative impact, its benefits outweigh the risks if developed ethically and safely. AI can augment human intelligence and help us achieve better outcomes in every domain of activity while becoming an AI tutor for every person maximizing their potential.", "title": "Why AI Will Save the World"}], "papers": [{"url": "https://github.com/DAMO-NLP-SG/Video-LLaMA", "summary": "Video-LLaMA is a new language model for video understanding, built on top of BLIP-2 and MiniGPT-4. It has two components: the Vision-Language component and the Audio-Language component, and can improve video accessibility by assisting in automated captioning, search, and navigation.", "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"}, {"url": "https://finegrainedrlhf.github.io/", "summary": "The article introduces fine-grained RLHF as a solution to improve language models\u2019 output quality, offering detailed rewards for explicit training signals and tailoring the language model for specific needs. This method achieves better performance than traditional methods.", "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"}, {"url": "https://arxiv.org/abs/2306.02707", "summary": "Orca, a 13-billion model, enhances AI models\u2019 capabilities through imitation learning and outperforms other models in complex reasoning benchmarks. It also performs well in professional and academic exams like LSAT, GMAT, SAT, and GRE.", "title": "Orca: Progressive Learning from Complex Explanation Traces of GPT-4"}, {"url": "https://arxiv.org/abs/2306.05284v1", "summary": "MUSIC GEN is a single Language Model that generates high-quality music conditioned on textual descriptions or melodic features, allowing control over the output. It achieved superior quality compared to previous models and uses an unsupervised melody conditioning technique to follow specific harmonic and melodic structures.", "title": "Simple and Controllable Music Generation"}, {"url": "https://arxiv.org/abs/2306.05422v1", "summary": "OmniMotion is a new motion estimation method that surpasses traditional optical flow and particle video tracking methods. It utilizes a globally consistent motion representation to ensure accurate tracking, estimate full-length motion trajectories for each pixel, and model camera and object motion. It outperforms previous approaches both quantitatively and qualitatively on TAP-Vid benchmark and real-world footage.", "title": "Tracking Everything Everywhere All at Once"}], "datetime": "2023-06-12"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-june-6th-2023-d09400cf1f49", "news": [{"url": "https://website-754fwhahs-humanloopml.vercel.app/blog/open_ai_talk", "summary": "OpenAI plans to prioritize creating a cheaper and faster GPT-4, extending context windows, and releasing multimodality in 2024. However, they are currently limited by GPU shortages and are unable to make models millions of times bigger in the near future. The finetuning API will be extended to the latest models.", "title": "OpenAI\u2019s plans according to Sam Altman", "topics": ["Multimodal AI (image, video, audio)", "AI Chips and GPUs", "GPT-4 and GPT-4 turbo", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.deeplearning.ai/the-batch/jpmorgan-trained-ai-to-interpret-the-federal-reserves-intent/", "summary": "JPMorgan Chase used a ChatGPT-based model to accurately predict US financial regulator\u2019s statements on interest rate changes. Large language models can offer accurate predictions, leading to big profits for investors adjusting their strategies.", "title": "JPMorgan Trained AI to Interpret the Federal Reserve\u2019s Intent", "topics": ["ChatGPT"], "sentiment": "positive"}, {"url": "https://www.theverge.com/2023/5/28/23740908/nvidia-ace-demo-voice-ai-npc-game-characters", "summary": "Nvidia showcases a powerful AI-powered demo of conversational AI for game characters that enhances realism and engages players, providing game developers a tool to improve their games\u2019 storytelling and overall engagement.", "title": "Nvidia demo about speaking to AI game characters", "topics": ["NVIDIA", "Multimodal AI (image, video, audio)", "Text-to-speech", "Speech-to-text"], "sentiment": "positive"}, {"url": "https://simonwillison.net/2023/May/27/lawyer-chatgpt/", "summary": "A lawyer cited fake cases generated by ChatGPT in their legal filings, highlighting the importance of verifying AI-generated content for accuracy and legitimacy. While generative AI tech like ChatGPT can benefit the legal industry, it is not perfect and may cause ethical and legal issues if not carefully reviewed.", "title": "Lawyer cites fake cases invented by ChatGPT, judge is not amused", "topics": ["AI safety", "AI regulation", "ChatGPT"], "sentiment": "negative"}, {"url": "https://www.washingtonpost.com/technology/2023/05/25/nvidia-ai-stock-gpu-chatbots/", "summary": "NVIDIA\u2019s GPUs have become a crucial component in developing AI, leading its worth to $939.3 billion; with AI applications requiring huge amounts of data, companies are buying thousands of NVIDIA\u2019s expensive chips. NVIDIA\u2019s dominance in the industry is predicted to persist as startups compete with big tech for access to its costly GPUs.", "title": "Why Nvidia is suddenly one of the most valuable companies in the world", "topics": ["NVIDIA", "AI Chips and GPUs"], "sentiment": "positive"}], "guides": [{"url": "https://openai.com/research/improving-mathematical-reasoning-with-process-supervision", "summary": "Researchers found that using process supervision can significantly enhance mathematical problem-solving skills in AI models. By rewarding each correct step of reasoning, the model achieved a new state-of-the-art, surpassing outcome supervision in terms of performance and alignment with human thinking.", "title": "Improving mathematical reasoning with process supervision"}, {"url": "https://ai.googleblog.com/2023/05/barkour-benchmarking-animal-level.html", "summary": "Google has created Barkour, a quadruped agility benchmark inspired by dog agility competitions, which evaluates robot performance relative to animals. The benchmark encourages efficient, controllable, and versatile locomotion controllers for quadruped robots and uses a new policy trained with a student-teacher framework to ensure greater robustness, versatility, and dynamism.", "title": "Barkour: Benchmarking animal-level agility with quadruped robots"}, {"url": "/llamaindex-blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b", "summary": "LlamaIndex\u2019s SQLAutoVectorQueryEngine is a powerful tool that can query both structured and unstructured data by combining SQL\u2019s expressivity with semantic search over a vector database, providing a comprehensive solution for AI professionals.", "title": "Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation"}], "papers": [{"url": "https://research.nvidia.com/labs/dir/neuralangelo/", "summary": "Neuralangelo is an efficient and flexible framework for high-quality 3D surface reconstruction from RGB video captures. Its optimization techniques enable users to create photorealistic 3D models of both object-centric and large-scale real-world scenes with highly detailed 3D geometry.", "title": "Neuralangelo"}, {"url": "https://github.com/lyuchenyang/Macaw-LLM", "summary": "Macaw-LLM is a multi-modal language modeling framework that integrates images, videos, audio, and text by aligning multi-modal data, encoding with CLIP and Whisper, and feeding outputs to LLaMA for efficient learning. It supports multiple languages and is expandable with the addition of more models.", "title": "lyuchenyang/Macaw-LLM: Macaw-LLM: Multi-Modal Language Modeling with Image, Video, Audio, and Text Integration"}, {"url": "https://arxiv.org/abs/2305.18583", "summary": "Control-GPT is an AI model that improves text-to-image generation by using Large Language Models to generate TikZ code sketches. These sketches act as a reference for diffusion models, enhancing spatial reasoning and improving image generation\u2019s controllability. Control-GPT sets a new standard for spatial arrangement and object positioning, and doubles the accuracy of prior models by empowering users to control object positions and sizes.", "title": "Controllable Text-to-Image Generation with GPT-4"}, {"url": "https://voyager.minedojo.org/", "summary": "Voyager is a new embodied lifelong learning agent that explores and conquers Minecraft without human intervention. It has an automatic curriculum, a skill library, and uses code as the action space for greater compositional actions. Its iterative prompting mechanism learns from mistakes and progresses.", "title": "An Open-Ended Embodied Agent with Large Language Models"}, {"url": "https://arxiv.org/abs/2305.12544", "summary": "A new document highlights untapped areas of exploration in NLP beyond LLMs and provides brief descriptions of 14 research areas rich in exploration. This document serves as a valuable guide for identifying exciting areas to explore in the field of NLP for PhD students and researchers.", "title": "A PhD Student\u2019s Perspective on Research in NLP"}], "datetime": "2023-06-06"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-may-29th-2023-dd3d80acd1cb", "news": [{"url": "https://wccftech.com/intel-aurora-genai-chatgpt-competitor-generative-ai-model-with-1-trillion-parameters/", "summary": "Intel has announced the Aurora genAI model, with 1 trillion parameters, which will be trained on scientific texts and structured scientific data to target cancer research, systems biology, cosmology, polymer chemistry, materials, and climate science. It will be powered by the Aurora supercomputer and has potential to suggest experiments and accelerate drug design targets.", "title": "Intel Announces Aurora genAI, Generative AI Model With 1 Trillion Parameters", "topics": ["AI in healthcare", "AI Chips and GPUs"], "sentiment": "positive"}, {"url": "https://ai.facebook.com/blog/multilingual-model-speech-recognition/", "summary": "Meta\u2019s Massively Multilingual Speech project uses self-supervised learning with wav2vec 2.0 and a unique dataset to enable AI to understand and generate speech in over 1,100 languages, outperforming existing models with reduced character error rates and increased language coverage.", "title": "Introducing speech-to-text, text-to-speech, and more for 1,100+ languages", "topics": ["AI datasets", "Text-to-speech", "Speech-to-text", "Meta"], "sentiment": "positive"}, {"url": "https://www.theverge.com/2023/5/25/23737405/google-search-labs-waitlist-invites-ai", "summary": "Google\u2019s Search Labs is now open for experimentation, offering AI-generated summaries at the top of search results that are globally compatible in multiple languages, potentially changing the web\u2019s business model and impacting SEO.", "title": "Google is starting to let users into its AI search experiment", "topics": ["Google"], "sentiment": "positive"}, {"url": "https://www.bbc.com/news/health-65709834", "summary": "Researchers have discovered a new antibiotic using AI to identify a compound that kills Acinetobacter baumannii with few signs of resistance, demonstrating the power of AI in identifying potential new antibiotics and hastening the pace of discovery of new treatments.", "title": "New superbug-killing antibiotic discovered using AI", "topics": ["AI in healthcare"], "sentiment": "positive"}, {"url": "https://appmaster.io/news/anthropic-secures-450m-series-c-funding-advance-next-gen-ai-assistants", "summary": "Anthropic secures $450M in Series C funding led by Spark Capital, with participation from Google, Salesforce, and Zoom, signaling major tech players\u2019 investments in the company\u2019s AI achievements. The funding will help Anthropic expand its product offerings and develop next-generation AI assistants, promising transformative innovation in businesses and customer experiences.", "title": "Generative AI Startup Anthropic Secures $450M in Series C Funding to Advance Next-Gen Intelligent Assistants", "topics": ["Funding", "Anthropic", "Google"], "sentiment": "positive"}, {"url": "https://www.theverge.com/2023/5/23/23734129/google-product-studio-generative-ai-ads-merchant-center", "summary": "Google\u2019s new Product Studio uses generative AI to help Shopping merchants customize product images and improve their ad campaigns, with integration into the Merchant Center for added convenience.", "title": "Google\u2019s new generative AI tool can jazz up product images", "topics": ["AI for images", "Multimodal AI (image, video, audio)", "Google"], "sentiment": "positive"}], "guides": [{"url": "https://lightning.ai/pages/blog/how-to-finetune-gpt-like-large-language-models-on-a-custom-dataset/", "summary": "Lit-Parrot is a nanoGPT-based tool developed by Lightning AI that offers clean and optimized LLM implementations for fine-tuning on custom data. It includes prompt adaptation, LLM approximation, and LLM cascade for cost reduction. Lightning AI offers step-by-step guidance on installation, dataset preparation, and model fine-tuning.", "title": "How To Finetune GPT Like Large Language Models on a Custom Dataset"}, {"url": "https://huggingface.co/blog/4bit-transformers-bitsandbytes", "summary": "Introducing QLoRA, a 4-bit finetuning breakthrough for Language Models that enables Guanaco, a chatbot, to achieve an astounding 97% ChatGPT performance on the Vicuna benchmark, all on a single GPU. QLoRA uses a frozen 4-bit base model with adapters on top to achieve memory efficiency through inventive tricks, including the 4-bit NormalFloat, Paged Optimizers, and Double Quantization.", "title": "Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA"}, {"url": "https://docs.cohere.com/docs/llmu", "summary": "LLM University by Cohere is a new program offering a holistic curriculum on NLP and Large Language Models, suitable for beginners and advanced learners. The program covers the architecture, embeddings, similarity, and attention mechanisms of LLMs, and teaches learners how to apply them to real-world scenarios such as semantic search, text generation, and classification. Plenty of code samples are provided for practical implementation.", "title": "Welcome to LLM University"}, {"url": "https://a16z.com/2022/11/17/the-generative-ai-revolution-in-games/", "summary": "Generative AI tools like Stable Diffusion and Dreambooth are revolutionizing the gaming industry by allowing artists to create high-quality images in hours, democratizing creative tools, and enabling more risk-taking in game development. However, artists skilled in using AI tools collaboratively and training the AI to reflect a consistent style will be in high demand.", "title": "The Generative AI Revolution in Games"}], "papers": [{"url": "https://arxiv.org/abs/2305.14342v1", "summary": "Sophia, a new optimizer for stochastic optimization, can drastically reduce the cost of pre-training language models. It achieves the same validation pre-training loss with 50% fewer steps than previous optimizers, and can be easily integrated into existing training pipelines.", "title": "Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training"}, {"url": "https://arxiv.org/abs/2305.15717v1", "summary": "Google researchers have found that while using imitation models to improve a weaker language model may initially show improvement, it is limited to tasks heavily supported in training data and cannot close the gap on unsupported tasks. The researchers suggest that improving base capabilities is the best way to improve open-source language models.", "title": "The False Promise of Imitating Proprietary LLMs"}, {"url": "https://arxiv.org/abs/2305.14540", "summary": "A recent paper explores the accuracy of large language models (LLMs) in detecting factual inconsistencies. Although some LLMs performed well, most struggled with complex formulations, indicating issues with current benchmarks. Researchers created the new SUMM EDITS benchmark which evaluates LLMs\u2019 ability to detect factual inconsistencies. LLMs struggle with complex factual reasoning, revealing gaps in their ability to reason and detect inconsistencies in facts.", "title": "LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond"}, {"url": "https://arxiv.org/abs/2305.11206", "summary": "LIMA is a language model that can improve alignment without reinforcement learning or human preference modeling, trained on 1,000 curated prompts and responses. It outperformed GPT-4 in 43% of cases without human feedback and highlights the importance of pretraining over large-scale instruction tuning and reinforcement learning.", "title": "LIMA: Less Is More for Alignment"}, {"url": "https://arxiv.org/abs/2305.14992v1", "summary": "The Reasoning via Planning (RAP) framework uses Monte Carlo Tree Search to enable large language models to perform effective planning and refine existing reasoning, outperforming other models in tasks like plan generation, math reasoning, and logical inference with a 33% relative improvement in plan gen settings. This framework has potential to improve large language models\u2019 reasoning skills in decision-making and motor control.", "title": "Reasoning with Language Model is Planning with World Model"}, {"url": "https://arxiv.org/abs/2305.14720v1", "summary": "BLIP-Diffusion is a new subject-driven text-to-image generation model that uses a pre-trained multimodal encoder for efficient scaling and fast fine-tuning, allowing for zero-shot generation and up to 20x speedup. It adopts a 2-stage pre-training strategy and excels in zero-shot subject-driven generation with faster fine-tuning compared to DreamBooth. It can be extended to other applications without extra training.", "title": "BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing"}], "datetime": "2023-05-29"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-may-22th-2023-ec7339c48c22", "news": [{"url": "https://www.theverge.com/2023/5/17/23726751/stability-ai-stablestudio-dreamstudio-stable-diffusion-artificial-intelligence", "summary": "Stability AI has launched StableStudio as part of its open-source AI efforts. StableStudio is a new open-source variant of its DreamStudio AI text-to-image web app.", "title": "Stability AI releases StableStudio in latest push for open-source AI", "topics": ["AI for images", "Multimodal AI (image, video, audio)", "Stable Diffusion", "Stability AI"], "sentiment": "positive"}, {"url": "https://openai.com/blog/introducing-the-chatgpt-app-for-ios", "summary": "ChatGPT released iOS app with open-source speech recognition for subscribers.", "title": "Introducing the ChatGPT app for iOS", "topics": ["Apple", "ChatGPT", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.theverge.com/2023/5/16/23725342/zoom-anthropic-ai-powered-chatbot-claude", "summary": "The article discusses the integration of Anthropic\u2019s chatbot into Zoom\u2019s platform, which could have potential implications for the use of artificial intelligence in video conferencing.", "title": "Zoom will soon integrate Anthropic\u2019s chatbot across its platform", "topics": ["Anthropic"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2023/05/16/apple-reveals-upcoming-accessibility-features-for-iphone-ipad-and-mac/", "summary": "An established leader in mainstream tech accessibility, Apple emphasizes that these tools are built with feedback from disabled communities.", "title": "Apple reveals new accessibility features, like custom text-to-speech voices", "topics": ["Text-to-speech", "Apple"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2023/05/18/meta-bets-big-on-ai-with-custom-chips-and-a-supercomputer/", "summary": "Meta lifted the curtains on its efforts to develop in-house infrastructure for AI workloads, including generative AI.", "title": "Meta bets big on AI with custom chips and a supercomputer", "topics": ["AI Chips and GPUs", "Meta"], "sentiment": "positive"}], "guides": [{"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2Jsb2cubGFuZ2NoYWluLmRldi9hdXRvLWV2YWx1YXRpb24tb2YtYW50aHJvcGljLTEwMGstY29udGV4dC13aW5kb3cvP3V0bV9zb3VyY2U9bmxwbGFuZXQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Vla2x5LWFpLWFuZC1ubHAtbmV3cy1tYXktMjJ0aC0yMDIzIiwicG9zdF9pZCI6ImJlODA4NDJjLWQ2NmQtNDBiZS05NzEwLTNiM2ZkNjVkYWEyYyIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiJiZjBlODZlZS03MGJhLTQ4MzQtOTE5My0wNzc5MzhjNTNlOTMiLCJpYXQiOjE2ODQ3NDU1OTMuNzczLCJpc3MiOiJvcmNoaWQifQ.y2O62VoJYz-Je736GWmU4OHdU6fmaglrpeXsD-TTrT4", "summary": "Anthropic\u2019s 100k-token Claude model sparks debate on whether document retrieval is necessary for Q&A. The retriever-less architecture performs well but has higher latency than retriever-based approaches, and is most suitable for small corpora and non-critical latency apps.", "title": "Auto-Evaluation of Anthropic 100k Context Window"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2Fzc2lzdGVkZXZlcnl0aGluZy5zdWJzdGFjay5jb20vcC90aGUtdGhyZWUtaGlsbHMtbW9kZWwtZm9yLWV2YWx1YXRpbmc_dXRtX3NvdXJjZT1ubHBsYW5ldC5iZWVoaWl2LmNvbSZ1dG1fbWVkaXVtPXJlZmVycmFsJnV0bV9jYW1wYWlnbj13ZWVrbHktYWktYW5kLW5scC1uZXdzLW1heS0yMnRoLTIwMjMiLCJwb3N0X2lkIjoiYmU4MDg0MmMtZDY2ZC00MGJlLTk3MTAtM2IzZmQ2NWRhYTJjIiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6ImJmMGU4NmVlLTcwYmEtNDgzNC05MTkzLTA3NzkzOGM1M2U5MyIsImlhdCI6MTY4NDc0NTU5My43NzMsImlzcyI6Im9yY2hpZCJ9.iFMFSBAC3LqrE3PpALuhwHfTLhBT3HxqzlEJJiFSxgI", "summary": "\u201cThree-Hills\u201d Model assesses success of GPT startups with \u201cProductivity Enhancements,\u201d \u201cNon zero-sum-game Value,\u201d and \u201cMoat = Value from Context\u201d in 3 levels. It identifies 3 moat categories to avoid generic GPT displacement.", "title": "The \u201cThree-Hills\u201d Model for evaluating GPT startups"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2Jsb2cudGhlYXByaWNvdC5pby9wb3N0cy9jaGF0Z3B0LXZzLWJhcmQvP3V0bV9zb3VyY2U9bmxwbGFuZXQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Vla2x5LWFpLWFuZC1ubHAtbmV3cy1tYXktMjJ0aC0yMDIzIiwicG9zdF9pZCI6ImJlODA4NDJjLWQ2NmQtNDBiZS05NzEwLTNiM2ZkNjVkYWEyYyIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiJiZjBlODZlZS03MGJhLTQ4MzQtOTE5My0wNzc5MzhjNTNlOTMiLCJpYXQiOjE2ODQ3NDU1OTMuNzczLCJpc3MiOiJvcmNoaWQifQ.vVVZ-_s6GZzi85FYHiTNr1-myhUhGKknXMo3JdqYVjA", "summary": "The article compares ChatGPT and Bard\u2019s performance through an unscientific field experiment.", "title": "ChatGPT vs. Bard: A realistic comparison"}], "papers": [{"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2dpdGh1Yi5jb20vcmF5LXByb2plY3QvbGxtLW51bWJlcnM_dXRtX3NvdXJjZT1ubHBsYW5ldC5iZWVoaWl2LmNvbSZ1dG1fbWVkaXVtPXJlZmVycmFsJnV0bV9jYW1wYWlnbj13ZWVrbHktYWktYW5kLW5scC1uZXdzLW1heS0yMnRoLTIwMjMiLCJwb3N0X2lkIjoiYmU4MDg0MmMtZDY2ZC00MGJlLTk3MTAtM2IzZmQ2NWRhYTJjIiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6ImJmMGU4NmVlLTcwYmEtNDgzNC05MTkzLTA3NzkzOGM1M2U5MyIsImlhdCI6MTY4NDc0NTU5My43NzMsImlzcyI6Im9yY2hpZCJ9.AuGKeWkdeK42AaCJFq_1RVLnhh2Jdl-h6T7BX-GWex4", "summary": "Numbers every LLM developer should know, like: GPT-3.5 Turbo vs OpenAI cost, 13B model training on 1.4T tokens, average tokens/word, GPU memory, LLM requests batching.", "title": "ray-project/llm-numbers"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2dpdGh1Yi5jb20vbWljcm9zb2Z0L2d1aWRhbmNlP3V0bV9zb3VyY2U9bmxwbGFuZXQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Vla2x5LWFpLWFuZC1ubHAtbmV3cy1tYXktMjJ0aC0yMDIzIiwicG9zdF9pZCI6ImJlODA4NDJjLWQ2NmQtNDBiZS05NzEwLTNiM2ZkNjVkYWEyYyIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiJiZjBlODZlZS03MGJhLTQ4MzQtOTE5My0wNzc5MzhjNTNlOTMiLCJpYXQiOjE2ODQ3NDU1OTMuNzczLCJpc3MiOiJvcmNoaWQifQ.Ewu3wmY0FY344JqqOETLmFCYAK9tbUVx13FbsiCSh8Q", "summary": "Microsoft\u2019s Guidance Language streamlines the creation of precise and parseable outputs with features such as smart caching, role-based models, and token healing.", "title": "microsoft/guidance"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwNS4xMDYwMXYxP3V0bV9zb3VyY2U9bmxwbGFuZXQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Vla2x5LWFpLWFuZC1ubHAtbmV3cy1tYXktMjJ0aC0yMDIzIiwicG9zdF9pZCI6ImJlODA4NDJjLWQ2NmQtNDBiZS05NzEwLTNiM2ZkNjVkYWEyYyIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiJiZjBlODZlZS03MGJhLTQ4MzQtOTE5My0wNzc5MzhjNTNlOTMiLCJpYXQiOjE2ODQ3NDU1OTMuNzczLCJpc3MiOiJvcmNoaWQifQ.PfwZbMeKTX7Y41azmLYuYSgKMVkIgSyZfrUVXnnQBXA", "summary": "Introducing \u201cTree of Thoughts\u201d framework to language models improves problem-solving by exploring text units, optimizing tasks like planning or search.", "title": "Tree of Thoughts: Deliberate Problem Solving"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwNS4wNzkyMnYxP3V0bV9zb3VyY2U9bmxwbGFuZXQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Vla2x5LWFpLWFuZC1ubHAtbmV3cy1tYXktMjJ0aC0yMDIzIiwicG9zdF9pZCI6ImJlODA4NDJjLWQ2NmQtNDBiZS05NzEwLTNiM2ZkNjVkYWEyYyIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiJiZjBlODZlZS03MGJhLTQ4MzQtOTE5My0wNzc5MzhjNTNlOTMiLCJpYXQiOjE2ODQ3NDU1OTMuNzczLCJpc3MiOiJvcmNoaWQifQ.Nrk9eT5RO2eQbR8w5NCX7SRWWarADoefpmHNlQ1KIRQ", "summary": "Introducing CodeT5+: flexible Code LLMs for various code-related tasks with a mix of pretraining objectives, achieving state-of-the-art performance. Also explores instruction-tuning.", "title": "CodeT5+: Open Code Large Language Models for"}], "datetime": "2023-05-22"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-may-16th-2023-4839ef0c4733", "news": [{"url": "https://techcrunch.com/2023/05/10/google-launches-palm-2-its-next-gen-large-language-model/", "summary": "PaLM 2 will power Google\u2019s updated Bard chat tool, the company\u2019s competitor to OpenAI\u2019s ChatGPT, and function as the foundation model for most of the new AI features the company is announcing today. PaLM 2 is now available to developers through Google\u2019s PaLM API, Firebase, and on Colab.", "title": "Google launches PaLM 2, its next-gen large language model", "topics": ["Google"], "sentiment": "positive"}, {"url": "https://www.theverge.com/2023/5/9/23716558/meta-imagebind-open-source-multisensory-modal-ai-model-research", "summary": "Meta has unveiled ImageBind, an open-source AI model indexing six data types (visual, audio, text, thermal, depth, movement) for multisensory AI.", "title": "Meta open-sources multisensory AI model that combines six types of data", "topics": ["Multimodal AI (image, video, audio)", "Meta"], "sentiment": "positive"}, {"url": "https://www.anthropic.com/index/100k-context-windows", "summary": "AI language model, Claude, now analyzes and synthesizes vast text in seconds with a 100K token context window. Summarize docs, assess risks, and more.", "title": "Introducing 100K Context Windows: Claude, by Anthropic", "topics": ["Mistral", "Anthropic"], "sentiment": "positive"}, {"url": "https://openai.com/research/language-models-can-explain-neurons-in-language-models", "summary": "GPT-4 automates understanding of language models by producing natural language explanations of neuron behavior.", "title": "Language models can explain neurons in language models, by OpenAI", "topics": ["GPT-4 and GPT-4 turbo", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.theverge.com/2023/5/10/23717120/google-search-ai-results-generated-experience-io", "summary": "Google\u2019s new \u201cAI snapshots\u201d use language models to generate summaries for richer searches. Users can opt-in to the experiment called Search Generative Experience.", "title": "The AI takeover of Google Search starts now", "topics": ["Google"], "sentiment": "positive"}, {"url": "https://nypost.com/2023/05/08/godfather-of-ai-says-its-threat-is-more-urgent-than-climate-change/", "summary": "AI expert Geoffrey Hinton, \u201cGodfather of AI,\u201d considers risks of unbridled AI more pressing for humanity than climate change. Hinton warns of threats like job loss and misinformation, but does not support a stoppage of AI development.", "title": "\u2018Godfather of AI\u2019 says AI threat is \u2018more urgent\u2019 to humanity than climate change", "topics": ["AI safety", "AI regulation"], "sentiment": "negative"}, {"url": "https://techcrunch.com/2023/05/11/amp-robotics-attracts-investment-from-microsofts-climate-innovation-fund/", "summary": "AMP Robotics, a Denver, Colorado-based startup creating robotic systems that can automatically sort recyclable material, announced that it extended its Series C round to $99 million, thanks to an investment from Microsoft\u2019s Climate Innovation Fund.", "title": "AMP Robotics attracts investment from Microsoft\u2019s Climate Innovation Fund", "topics": ["Robotics", "Funding", "Microsoft"], "sentiment": "positive"}, {"url": "https://www.reuters.com/technology/eu-lawmakers-committees-agree-tougher-draft-ai-rules-2023-05-11/", "summary": "EU to classify AI tools by risk level; proposed draft legislation bans facial recognition in public spaces, with grace period of 2 years before law.", "title": "EU lawmakers\u2019 committees agree tougher draft AI rules", "topics": ["AI regulation"], "sentiment": "positive"}], "guides": [{"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2Jsb2cubGFuZ2NoYWluLmRldi9wbGFuLWFuZC1leGVjdXRlLWFnZW50cy8_dXRtX3NvdXJjZT1ubHBsYW5ldC5iZWVoaWl2LmNvbSZ1dG1fbWVkaXVtPXJlZmVycmFsJnV0bV9jYW1wYWlnbj13ZWVrbHktYWktYW5kLW5scC1uZXdzLW1heS0xNnRoLTIwMjMiLCJwb3N0X2lkIjoiNzY3ODUwZGMtNmJhOS00MDdlLTk0ODctMzhkYjFmMDA5ZGVhIiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6Ijc4YmFlY2RmLWNlYjMtNGRmZC1hY2Y4LTZhYTIwODg4OTY3OCIsImlhdCI6MTY4NDIyNzk4OS40NTYsImlzcyI6Im9yY2hpZCJ9.EvlyovNQrZleLA94gWzSe78IpSVeMfi0u_juEbi8e0A", "summary": "Introducing Plan-and-Execute: a new agent executor that separates higher-level planning from short-term execution for complex long-term planning.", "title": "Plan-and-Execute Agents"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2ludGVyY29ubmVjdC5zdWJzdGFjay5jb20vcC9haS13aWxsLWNyZWF0ZS1tb3JlLWRldmVsb3BlcnMtbm90P3V0bV9zb3VyY2U9bmxwbGFuZXQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Vla2x5LWFpLWFuZC1ubHAtbmV3cy1tYXktMTZ0aC0yMDIzIiwicG9zdF9pZCI6Ijc2Nzg1MGRjLTZiYTktNDA3ZS05NDg3LTM4ZGIxZjAwOWRlYSIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiI3OGJhZWNkZi1jZWIzLTRkZmQtYWNmOC02YWEyMDg4ODk2NzgiLCJpYXQiOjE2ODQyMjc5ODkuNDU2LCJpc3MiOiJvcmNoaWQifQ.UyNLW195cnvCjrVfUAGjW8EkRPFIMzbrojO2cXCbBRU", "summary": "AI will accelerate global developer growth by minimizing entry barriers with open- and closed-source products for different audiences, building smaller firms.", "title": "AI Will Create More Developers, Not Less"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL21lZGl1bS5jb20vcG9pbnQtbmluZS1uZXdzL2xlYXJuaW5ncy1leHBsb3JpbmctdGhlLWdwdC1sbG0tc3BhY2UtNjdhNDlkNGUyZTE5P3V0bV9zb3VyY2U9bmxwbGFuZXQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Vla2x5LWFpLWFuZC1ubHAtbmV3cy1tYXktMTZ0aC0yMDIzIiwicG9zdF9pZCI6Ijc2Nzg1MGRjLTZiYTktNDA3ZS05NDg3LTM4ZGIxZjAwOWRlYSIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiI3OGJhZWNkZi1jZWIzLTRkZmQtYWNmOC02YWEyMDg4ODk2NzgiLCJpYXQiOjE2ODQyMjc5ODkuNDU2LCJpc3MiOiJvcmNoaWQifQ.FnQXnvpsZsh7w1jLmao3_XRukGvtaiOSsucCpAhpb-U", "summary": "LLM has huge potential for businesses but requires software engineering skills, GPT-4 is the best-performing, but costly. Training costs are expected to decrease.", "title": "Learnings exploring the GPT/ LLM space"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2Vremh1Lm1lZGl1bS5jb20vZ3B0LTRzLW1hemUtbmF2aWdhdGlvbi1hLWRlZXAtZGl2ZS1pbnRvLXJlYWN0LWFnZW50LWFuZC1sbG0tcy10aG91Z2h0cy1iMTgyM2ZiMjY2ZWU_dXRtX3NvdXJjZT1ubHBsYW5ldC5iZWVoaWl2LmNvbSZ1dG1fbWVkaXVtPXJlZmVycmFsJnV0bV9jYW1wYWlnbj13ZWVrbHktYWktYW5kLW5scC1uZXdzLW1heS0xNnRoLTIwMjMiLCJwb3N0X2lkIjoiNzY3ODUwZGMtNmJhOS00MDdlLTk0ODctMzhkYjFmMDA5ZGVhIiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6Ijc4YmFlY2RmLWNlYjMtNGRmZC1hY2Y4LTZhYTIwODg4OTY3OCIsImlhdCI6MTY4NDIyNzk4OS40NTYsImlzcyI6Im9yY2hpZCJ9.FyK8R1Ia3J427QbPXTLEEAU24rHmrIOcJB25sfDZrwk", "summary": "The article explores GPT-4 for maze navigation. It uses memorization-based navigation, labeling, and A* search but lacks planning skills.", "title": "GPT-4\u2019s Maze Navigation: A Deep Dive into ReAct Agent and LLM\u2019s Thoughts"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FwcGx5aW5nbWwuY29tL21lbnRvcnMvYWRpdHlhLW5hbWJpYXIvP3V0bV9zb3VyY2U9bmxwbGFuZXQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Vla2x5LWFpLWFuZC1ubHAtbmV3cy1tYXktMTZ0aC0yMDIzIiwicG9zdF9pZCI6Ijc2Nzg1MGRjLTZiYTktNDA3ZS05NDg3LTM4ZGIxZjAwOWRlYSIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiI3OGJhZWNkZi1jZWIzLTRkZmQtYWNmOC02YWEyMDg4ODk2NzgiLCJpYXQiOjE2ODQyMjc5ODkuNDU2LCJpc3MiOiJvcmNoaWQifQ.o8fZq7PMg879S_8ohh92-oHBFiHVKpomFbZ47vOXSEA", "summary": "An interview with Aditya Nambiar, Founding engineer at Fennel, about building ML infrastructure.", "title": "Building ML infrastructure"}], "papers": [{"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwNS4wNjUwMHYxP3V0bV9zb3VyY2U9bmxwbGFuZXQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Vla2x5LWFpLWFuZC1ubHAtbmV3cy1tYXktMTZ0aC0yMDIzIiwicG9zdF9pZCI6Ijc2Nzg1MGRjLTZiYTktNDA3ZS05NDg3LTM4ZGIxZjAwOWRlYSIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiI3OGJhZWNkZi1jZWIzLTRkZmQtYWNmOC02YWEyMDg4ODk2NzgiLCJpYXQiOjE2ODQyMjc5ODkuNDU2LCJpc3MiOiJvcmNoaWQifQ.xFRu4KVSjDcqXb4VU4fDBgiIxyVM36e7k-JgPTRKIqA", "summary": "The authors conduct a systematic and comprehensive study on vision-language instruction tuning based on the pre-trained BLIP-2 models The resulting InstructBLIP models achieve state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and the larger Flamingo.", "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwNS4wNDc1N3YxP3V0bV9zb3VyY2U9bmxwbGFuZXQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Vla2x5LWFpLWFuZC1ubHAtbmV3cy1tYXktMTZ0aC0yMDIzIiwicG9zdF9pZCI6Ijc2Nzg1MGRjLTZiYTktNDA3ZS05NDg3LTM4ZGIxZjAwOWRlYSIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiI3OGJhZWNkZi1jZWIzLTRkZmQtYWNmOC02YWEyMDg4ODk2NzgiLCJpYXQiOjE2ODQyMjc5ODkuNDU2LCJpc3MiOiJvcmNoaWQifQ.eIHiN0dw7tpfd8-hfExhScZQ_nqwklYAz3A0zS8wd5I", "summary": "The authors propose a novel Parametric Knowledge Guiding (PKG) framework, which equips LLMs with a knowledge-guiding module to access relevant knowledge at runtime without altering the LLMs\u2019 parameters.", "title": "Augmented Large Language Models with Parametric Knowledge Guiding"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwNS4wNjk4M3YxP3V0bV9zb3VyY2U9bmxwbGFuZXQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Vla2x5LWFpLWFuZC1ubHAtbmV3cy1tYXktMTZ0aC0yMDIzIiwicG9zdF9pZCI6Ijc2Nzg1MGRjLTZiYTktNDA3ZS05NDg3LTM4ZGIxZjAwOWRlYSIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiI3OGJhZWNkZi1jZWIzLTRkZmQtYWNmOC02YWEyMDg4ODk2NzgiLCJpYXQiOjE2ODQyMjc5ODkuNDU2LCJpc3MiOiJvcmNoaWQifQ.TSFWxLtMmLO3PBXts7XJQFN138yP0f3YeqzGGTANqh0", "summary": "The authors propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic retrieval-augmented generation method which generates articles/overviews by iteratively using a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens.", "title": "Active Retrieval Augmented Generation"}], "datetime": "2023-05-16"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-may-9th-2023-b06f93dae6d3", "news": [{"url": "https://archive.is/elhiG", "summary": "Google is shifting the way it presents search results to incorporate conversations with artificial intelligence, along with more short video and social media posts, a departure from the list of website results that has made it the dominant search engine for decades.", "title": "Google Plans to Make Search More \u2018Personal\u2019 with AI Chat and Video Clips", "topics": ["Google Gemini", "Google"], "sentiment": "positive"}, {"url": "https://www.mosaicml.com/blog/mpt-7b", "summary": "MPT-7B is a transformer trained from scratch on 1T tokens of text and code. It is open-source, available for commercial use, and matches the quality of LLaMA-7B.", "title": "Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs, by MosaicML", "topics": ["Model release", "LLaMA"], "sentiment": "positive"}, {"url": "https://www.together.xyz/blog/redpajama-models-v1", "summary": "The release includes the first models trained on the RedPajama base dataset: a 3 billion and a 7B parameter base model that aims to replicate the LLaMA recipe as closely as possible.", "title": "Releasing 3B and 7B RedPajama-INCITE family of models including base, instruction-tuned & chat models", "topics": ["Model release", "LLaMA"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2023/05/04/hugging-face-and-servicenow-release-a-free-code-generating-model/", "summary": "AI startup Hugging Face and ServiceNow Research have released StarCoder, a free alternative to code-generating AI systems along the lines of GitHub\u2019s Copilot.", "title": "Hugging Face and ServiceNow release a free code-generating model", "topics": ["Hugging Face", "AI for coding", "Model release"], "sentiment": "positive"}, {"url": "https://www.theverge.com/2023/5/4/23710071/microsoft-bing-chat-ai-public-preview-plug-in-support", "summary": "All you need is a Microsoft account to get access to the GPT-4-powered version of Bing.", "title": "Microsoft\u2019s Bing Chat AI is now open to everyone, with plug-ins coming soon", "topics": ["Microsoft", "ChatGPT", "GPT-4 and GPT-4 turbo"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2023/05/03/malware-chatgpt-lures-facebook/", "summary": "As public interest in generative AI chatbots grows, hackers are increasingly using ChatGPT-themed lures to spread malware across Facebook, Instagram, and WhatsApp.", "title": "Hackers are increasingly using ChatGPT lures to spread malware on Facebook", "topics": ["ChatGPT", "Meta"], "sentiment": "negative"}, {"url": "https://www.ted.com/talks/sal_khan_the_amazing_ai_super_tutor_for_students_and_teachers/c", "summary": "Sal Khan, the founder and CEO of Khan Academy, thinks artificial intelligence could spark the greatest positive transformation education has ever seen. He demos some exciting new features for their educational chatbot, Khanmigo.", "title": "The amazing AI super tutor for students and teachers, Khan Academy", "topics": [], "sentiment": "positive"}, {"url": "https://www.theverge.com/2023/5/1/23706311/hinton-godfather-of-ai-threats-fears-warnings", "summary": "Geoffrey Hinton who won the \u2018Nobel Prize of computing\u2019 for his trailblazing work on neural networks is now free to speak about the risks of AI.", "title": "\u201cGodfather of AI\u201d quits Google with regrets and fears about his life\u2019s work", "topics": ["AI safety", "AI regulation", "Google"], "sentiment": "negative"}, {"url": "https://www.vice.com/en/article/4a3w3g/scientists-use-gpt-ai-to-passively-read-peoples-thoughts-in-breakthrough", "summary": "An AI model similar to ChatGPT was combined with fMRI readings to non-invasively decode continuous language from subjects, a new study reports.", "title": "Scientists Use GPT AI to Passively Read People\u2019s Thoughts in Breakthrough", "topics": ["AI in healthcare", "ChatGPT"], "sentiment": "positive"}], "guides": [{"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy5vbmV1c2VmdWx0aGluZy5vcmcvcC9pdC1pcy1zdGFydGluZy10by1nZXQtc3RyYW5nZT91dG1fc291cmNlPW5scGxhbmV0LmJlZWhpaXYuY29tJnV0bV9tZWRpdW09cmVmZXJyYWwmdXRtX2NhbXBhaWduPXdlZWtseS1haS1hbmQtbmxwLW5ld3MtbWF5LTl0aC0yMDIzIiwicG9zdF9pZCI6ImQ0Zjg4NjU5LTBjMDItNDU4MS05MjhmLTg4ODY4MTNhMmFkMCIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiI0OGRmZWVhZC1jNmQyLTQ2MTItODQwZC1mNjRlYjM4ZDZkN2QiLCJpYXQiOjE2ODM2MjcyNTguMzAzLCJpc3MiOiJvcmNoaWQifQ.0p2Bhn3EMgIdHuz4dPCdR3-NOFFr6waSAz_VxEEoKss", "summary": "Between the expanding capabilities of GPT-4, and the soon-to-be everywhere Microsoft Copilot, work is going to start changing in a matter of months, not years.", "title": "Let\u2019s talk about ChatGPT with Code Interpreter & Microsoft Copilot"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2h1Z2dpbmdmYWNlLmNvL2Jsb2cvdGV4dC10by12aWRlbz91dG1fc291cmNlPW5scGxhbmV0LmJlZWhpaXYuY29tJnV0bV9tZWRpdW09cmVmZXJyYWwmdXRtX2NhbXBhaWduPXdlZWtseS1haS1hbmQtbmxwLW5ld3MtbWF5LTl0aC0yMDIzIiwicG9zdF9pZCI6ImQ0Zjg4NjU5LTBjMDItNDU4MS05MjhmLTg4ODY4MTNhMmFkMCIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiI0OGRmZWVhZC1jNmQyLTQ2MTItODQwZC1mNjRlYjM4ZDZkN2QiLCJpYXQiOjE2ODM2MjcyNTguMzAzLCJpc3MiOiJvcmNoaWQifQ.D_D4DKeM6qIFHWFu50kucJ_sICVFZTNKdL2VZgmYjBA", "summary": "This blog post discusses the past, present, and future of text-to-video models. It starts by reviewing the differences between the text-to-video and text-to-image tasks, and discusses the unique challenges of unconditional and text-conditioned video generation.", "title": "Text-to-Video: The Task, Challenges and the Current State"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy5zZW1pYW5hbHlzaXMuY29tL3AvZ29vZ2xlLXdlLWhhdmUtbm8tbW9hdC1hbmQtbmVpdGhlcj91dG1fc291cmNlPW5scGxhbmV0LmJlZWhpaXYuY29tJnV0bV9tZWRpdW09cmVmZXJyYWwmdXRtX2NhbXBhaWduPXdlZWtseS1haS1hbmQtbmxwLW5ld3MtbWF5LTl0aC0yMDIzIiwicG9zdF9pZCI6ImQ0Zjg4NjU5LTBjMDItNDU4MS05MjhmLTg4ODY4MTNhMmFkMCIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiI0OGRmZWVhZC1jNmQyLTQ2MTItODQwZC1mNjRlYjM4ZDZkN2QiLCJpYXQiOjE2ODM2MjcyNTguMzAzLCJpc3MiOiJvcmNoaWQifQ.0f032XgpU6lZlMIEJBltPNtij1S8zZ43VjrCZfWlXIM", "summary": "A Leaked internal Google document claims open-source AI could outcompete Google and OpenAI. The opinions are just from a Google employee, not from the firm.", "title": "Google \u201cWe Have No Moat, And Neither Does OpenAI\u201d"}], "papers": [{"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2dpdGh1Yi5jb20vbWxjLWFpL21sYy1sbG0_dXRtX3NvdXJjZT1ubHBsYW5ldC5iZWVoaWl2LmNvbSZ1dG1fbWVkaXVtPXJlZmVycmFsJnV0bV9jYW1wYWlnbj13ZWVrbHktYWktYW5kLW5scC1uZXdzLW1heS05dGgtMjAyMyIsInBvc3RfaWQiOiJkNGY4ODY1OS0wYzAyLTQ1ODEtOTI4Zi04ODg2ODEzYTJhZDAiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiNDhkZmVlYWQtYzZkMi00NjEyLTg0MGQtZjY0ZWIzOGQ2ZDdkIiwiaWF0IjoxNjgzNjI3MjU4LjMwMywiaXNzIjoib3JjaGlkIn0.N1XA1b9Gau8sINDlStmmAmM6G1axsatBwSusP4MZpOY", "summary": "Enable everyone to develop, optimize and deploy Large Language Models natively on everyone\u2019s devices.", "title": "mlc-ai/mlc-llm"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2dpdGh1Yi5jb20vU2lnbmlmaWNhbnQtR3Jhdml0YXMvQXV0by1HUFQtUGx1Z2lucz91dG1fc291cmNlPW5scGxhbmV0LmJlZWhpaXYuY29tJnV0bV9tZWRpdW09cmVmZXJyYWwmdXRtX2NhbXBhaWduPXdlZWtseS1haS1hbmQtbmxwLW5ld3MtbWF5LTl0aC0yMDIzIiwicG9zdF9pZCI6ImQ0Zjg4NjU5LTBjMDItNDU4MS05MjhmLTg4ODY4MTNhMmFkMCIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiI0OGRmZWVhZC1jNmQyLTQ2MTItODQwZC1mNjRlYjM4ZDZkN2QiLCJpYXQiOjE2ODM2MjcyNTguMzAzLCJpc3MiOiJvcmNoaWQifQ.XZf_CST48UiImZld0EJe14AhifEz-RosMxc0ObpFGDo", "summary": "Plugins for Auto-GPT.", "title": "Significant-Gravitas/Auto-GPT-Plugins"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3BhcGVyc3dpdGhjb2RlLmNvbS9wYXBlci9kaXN0aWxsaW5nLXN0ZXAtYnktc3RlcC1vdXRwZXJmb3JtaW5nLWxhcmdlcj91dG1fc291cmNlPW5scGxhbmV0LmJlZWhpaXYuY29tJnV0bV9tZWRpdW09cmVmZXJyYWwmdXRtX2NhbXBhaWduPXdlZWtseS1haS1hbmQtbmxwLW5ld3MtbWF5LTl0aC0yMDIzIiwicG9zdF9pZCI6ImQ0Zjg4NjU5LTBjMDItNDU4MS05MjhmLTg4ODY4MTNhMmFkMCIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiI0OGRmZWVhZC1jNmQyLTQ2MTItODQwZC1mNjRlYjM4ZDZkN2QiLCJpYXQiOjE2ODM2MjcyNTguMzAzLCJpc3MiOiJvcmNoaWQifQ.XjOCHHKmg9J6h0SMwOYKF8j9DJSfvuUntA48mD5jBi8", "summary": "The authors introduce Distilling step-by-step, a new mechanism that trains smaller models that outperform LLMs, and achieves so by leveraging less training data needed by finetuning or distillation.", "title": "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3BhcGVyc3dpdGhjb2RlLmNvbS9wYXBlci91bmxpbWlmb3JtZXItbG9uZy1yYW5nZS10cmFuc2Zvcm1lcnMtd2l0aD91dG1fc291cmNlPW5scGxhbmV0LmJlZWhpaXYuY29tJnV0bV9tZWRpdW09cmVmZXJyYWwmdXRtX2NhbXBhaWduPXdlZWtseS1haS1hbmQtbmxwLW5ld3MtbWF5LTl0aC0yMDIzIiwicG9zdF9pZCI6ImQ0Zjg4NjU5LTBjMDItNDU4MS05MjhmLTg4ODY4MTNhMmFkMCIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiI0OGRmZWVhZC1jNmQyLTQ2MTItODQwZC1mNjRlYjM4ZDZkN2QiLCJpYXQiOjE2ODM2MjcyNTguMzAzLCJpc3MiOiJvcmNoaWQifQ.jpZ8UxQWbuIyO5Jtu5GwnL4abFcYM1npjez5JXDcO6k", "summary": "A general approach that can wrap any existing pretrained encoder-decoder transformer, and offload the attention computation across all layers to a single k-nearest-neighbor index.", "title": "Unlimiformer: Long-Range Transformers with Unlimited Length Input"}], "datetime": "2023-05-09"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-may-2nd-2023-4ddc28d9c681", "news": [{"url": "https://huggingface.co/chat/", "summary": "A ChatGPT-like UI for the OpenAssistant model, hosted by Hugging Face.", "title": "HuggingChat", "topics": ["Hugging Face"], "sentiment": "positive"}, {"url": "https://www.reuters.com/markets/europe/g7-should-adopt-risk-based-ai-regulation-ministers-say-2023-04-30/", "summary": "G7 agrees on \u201crisk-based\u201d AI regulation, but will maintain an open environment. EU & Japan have introduced AI policies.", "title": "G7 should adopt \u2018risk-based\u2019 AI regulation, ministers say", "topics": ["AI regulation"], "sentiment": "positive"}, {"url": "https://www.sequoiacap.com/article/partnering-with-harvey-putting-llms-to-work/", "summary": "Harvey uses GPT-4 to empower legal teams with custom models, data segregation, security, and compliance. Sequoia led its Series A to expand its AI app.", "title": "Partnering with Harvey: Putting LLMs to Work", "topics": ["Funding", "GPT-4 and GPT-4 turbo"], "sentiment": "positive"}, {"url": "https://www.wired.com/story/andy-warhol-fair-use-prince-generative-ai/", "summary": "The US Copyright Office determined recently that art created solely by AI isn\u2019t eligible for copyright protection. Artists can attempt to register works made with assistance from AI, but they must show significant \u201chuman authorship\u201d.", "title": "The Andy Warhol Copyright Case That Could Transform Generative AI", "topics": ["AI and copyright", "AI regulation"], "sentiment": "negative"}, {"url": "https://artificialintelligenceradio.com/", "summary": "A web radio with AI-generated music.", "title": "Artificial Intelligence Radio", "topics": ["Multimodal AI (image, video, audio)"], "sentiment": "positive"}, {"url": "https://www.semafor.com/article/04/25/2023/theres-a-new-ai-unicorn-that-will-make-coders-faster", "summary": "AI-coding-platform Replit raised $97.4M in a funding round led by Andreessen Horowitz and Khosla Ventures on the back of growing popularity and 22.5M users.", "title": "There\u2019s a new AI unicorn that will make coders faster", "topics": ["AI for coding", "Funding"], "sentiment": "positive"}, {"url": "https://www.vice.com/en/article/qjvb4x/palantir-demos-ai-to-fight-wars-but-says-it-will-be-totally-ethical-dont-worry-about-it", "summary": "Peter Thiel\u2019s Palantir has launched the Palantir AI Platform to operate language models like GPT-4 on private networks. They insist on legal and ethical use.", "title": "Palantir Demos AI to Fight Wars But Says It Will Be Totally Ethical", "topics": ["GPT-4 and GPT-4 turbo"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2023/04/28/openai-funding-valuation-chatgpt/", "summary": "VCs have put in just over $300 million at a valuation of $27 billion \u2014 $29 billion. This is separate to a big investment from Microsoft announced earlier this year.", "title": "OpenAI closes $300M share sale at $27B-29B valuation", "topics": ["Funding", "Microsoft", "OpenAI"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2023/04/27/pinecone-drops-100m-investment-on-750m-valuation-as-vector-database-demand-grows/", "summary": "With the rise of LLMs in the public consciousness, companies are beginning to see the value of vector databases more and more.", "title": "Pinecone drops $100M investment on $750M valuation, as vector database demand grows", "topics": ["Funding"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2023/04/28/amazon-working-improved-llm-to-power-alexa/", "summary": "Amazon executives believe that the addition of an improved LLM will help Amazon work toward its goal of building \u201cthe world\u2019s best personal assistant,\u201d but acknowledged that it will be difficult to do so across many domains.", "title": "Amazon is developing an improved LLM to power Alexa", "topics": ["Amazon"], "sentiment": "positive"}], "guides": [{"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3B5dG9yY2gub3JnL2Jsb2cvaW50cm9kdWNpbmctaGlkZXQvP3V0bV9zb3VyY2U9bmxwbGFuZXQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Vla2x5LWFpLWFuZC1ubHAtbmV3cy1tYXktMm5kLTIwMjMiLCJwb3N0X2lkIjoiMjlmMDg1ODItMTFiOS00YjBjLWFjZDYtYWZhM2RkMTFkMTkyIiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6IjIzYWE2Y2Y2LWY5YjUtNDc5ZS1hYjIwLTFlMWE1ZDQ4OTczMCIsImlhdCI6MTY4MzAyMTQxNC4zOTMsImlzcyI6Im9yY2hpZCJ9.qSi9jrtfs44YAQxLts9kTt_6R4JB5SCLS2kCKNMj4uM", "summary": "Hidet simplifies implementing high-performing deep learning ops on modern accelerators. Integrated with PyTorch, improve model inference.", "title": "Introducing Hidet: A Deep Learning Compiler for Efficient Model Serving"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2ExNnouY29tLzIwMjMvMDQvMjcvbmF2aWdhdGluZy10aGUtaGlnaC1jb3N0LW9mLWFpLWNvbXB1dGUvP3V0bV9zb3VyY2U9bmxwbGFuZXQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Vla2x5LWFpLWFuZC1ubHAtbmV3cy1tYXktMm5kLTIwMjMiLCJwb3N0X2lkIjoiMjlmMDg1ODItMTFiOS00YjBjLWFjZDYtYWZhM2RkMTFkMTkyIiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6IjIzYWE2Y2Y2LWY5YjUtNDc5ZS1hYjIwLTFlMWE1ZDQ4OTczMCIsImlhdCI6MTY4MzAyMTQxNC4zOTMsImlzcyI6Im9yY2hpZCJ9.6k33oRAb_UtlBaTFfWbdUS5HsKn1313MizkbrWUh88E", "summary": "AI infrastructure is costly due to high computational expenses for GPT models. Using specialized chips and transformer-based architectures can cut costs.", "title": "Navigating the High Cost of AI Compute"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy50ZWNobm9sb2d5cmV2aWV3LmNvbS8yMDIzLzA0LzI3LzEwNzIxMDIvdGhlLWZ1dHVyZS1vZi1nZW5lcmF0aXZlLWFpLWlzLW5pY2hlLW5vdC1nZW5lcmFsaXplZC8_dXRtX3NvdXJjZT1ubHBsYW5ldC5iZWVoaWl2LmNvbSZ1dG1fbWVkaXVtPXJlZmVycmFsJnV0bV9jYW1wYWlnbj13ZWVrbHktYWktYW5kLW5scC1uZXdzLW1heS0ybmQtMjAyMyIsInBvc3RfaWQiOiIyOWYwODU4Mi0xMWI5LTRiMGMtYWNkNi1hZmEzZGQxMWQxOTIiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiMjNhYTZjZjYtZjliNS00NzllLWFiMjAtMWUxYTVkNDg5NzMwIiwiaWF0IjoxNjgzMDIxNDE0LjM5MywiaXNzIjoib3JjaGlkIn0.E3mw90D_aR__zIGpcYBaO4uNzsrVBai5eY93tfRRqg8", "summary": "Organizations aim to train tools on large language models for domain-specific data.", "title": "The future of generative AI is niche, not generalized"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy5vbmV1c2VmdWx0aGluZy5vcmcvcC9hLWd1aWRlLXRvLXByb21wdGluZy1haS1mb3Itd2hhdD91dG1fc291cmNlPW5scGxhbmV0LmJlZWhpaXYuY29tJnV0bV9tZWRpdW09cmVmZXJyYWwmdXRtX2NhbXBhaWduPXdlZWtseS1haS1hbmQtbmxwLW5ld3MtbWF5LTJuZC0yMDIzIiwicG9zdF9pZCI6IjI5ZjA4NTgyLTExYjktNGIwYy1hY2Q2LWFmYTNkZDExZDE5MiIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiIyM2FhNmNmNi1mOWI1LTQ3OWUtYWIyMC0xZTFhNWQ0ODk3MzAiLCJpYXQiOjE2ODMwMjE0MTQuMzkzLCJpc3MiOiJvcmNoaWQifQ.44ehYhLBV3uXRHjwxyCKj7s1e0sBBEhkqUlQAho0lDE", "summary": "Effective prompts can enhance AI output by adding context and constraints. However, overemphasizing prompts can lead to mistakes.", "title": "A guide to prompting AI"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL25ld3MuYWdwdC5jby8_dXRtX3NvdXJjZT1ubHBsYW5ldC5iZWVoaWl2LmNvbSZ1dG1fbWVkaXVtPXJlZmVycmFsJnV0bV9jYW1wYWlnbj13ZWVrbHktYWktYW5kLW5scC1uZXdzLW1heS0ybmQtMjAyMyIsInBvc3RfaWQiOiIyOWYwODU4Mi0xMWI5LTRiMGMtYWNkNi1hZmEzZGQxMWQxOTIiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiMjNhYTZjZjYtZjliNS00NzllLWFiMjAtMWUxYTVkNDg5NzMwIiwiaWF0IjoxNjgzMDIxNDE0LjM5MywiaXNzIjoib3JjaGlkIn0.PWbGVKSnJwIo24JP_jWg5YQFnzmABPMNobcjP7LIo4E", "summary": "Auto-GPT aims to provide the best autonomous AI assistant for every person, ensuring that everyone can accomplish more every day. It has +100k stars on GitHub now.", "title": "Auto-GPT News & Updates"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy5tYXR0cHJkLmNvbS9wL3RoZS1jb21wbGV0ZS1iZWdpbm5lcnMtZ3VpZGUtdG8tYXV0b25vbW91cy1hZ2VudHM_dXRtX3NvdXJjZT1ubHBsYW5ldC5iZWVoaWl2LmNvbSZ1dG1fbWVkaXVtPXJlZmVycmFsJnV0bV9jYW1wYWlnbj13ZWVrbHktYWktYW5kLW5scC1uZXdzLW1heS0ybmQtMjAyMyIsInBvc3RfaWQiOiIyOWYwODU4Mi0xMWI5LTRiMGMtYWNkNi1hZmEzZGQxMWQxOTIiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiMjNhYTZjZjYtZjliNS00NzllLWFiMjAtMWUxYTVkNDg5NzMwIiwiaWF0IjoxNjgzMDIxNDE0LjM5MywiaXNzIjoib3JjaGlkIn0.YrcPDUCZ1u1OfhiY4jBjgzbCrFt2punJ3CEv0EsNZeo", "summary": "AI-powered task agents automate digital tasks that were previously done by humans.", "title": "The Complete Beginners Guide To Autonomous Agents"}], "papers": [{"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2dpdGh1Yi5jb20vQUlHQy1BdWRpby9BdWRpb0dQVD91dG1fc291cmNlPW5scGxhbmV0LmJlZWhpaXYuY29tJnV0bV9tZWRpdW09cmVmZXJyYWwmdXRtX2NhbXBhaWduPXdlZWtseS1haS1hbmQtbmxwLW5ld3MtbWF5LTJuZC0yMDIzIiwicG9zdF9pZCI6IjI5ZjA4NTgyLTExYjktNGIwYy1hY2Q2LWFmYTNkZDExZDE5MiIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiIyM2FhNmNmNi1mOWI1LTQ3OWUtYWIyMC0xZTFhNWQ0ODk3MzAiLCJpYXQiOjE2ODMwMjE0MTQuMzkzLCJpc3MiOiJvcmNoaWQifQ.-nAOnmB_qHA5jXVp_UQrY8x3KfynFq9mIS_oBqLC9vE", "summary": "A resource for audio apps and tasks like speech recognition, synthesis, and sound detection.", "title": "AIGC-Audio/AudioGPT: AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3N0YWJpbGl0eS5haS9ibG9nL2RlZXBmbG95ZC1pZi10ZXh0LXRvLWltYWdlLW1vZGVsP3V0bV9zb3VyY2U9bmxwbGFuZXQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Vla2x5LWFpLWFuZC1ubHAtbmV3cy1tYXktMm5kLTIwMjMiLCJwb3N0X2lkIjoiMjlmMDg1ODItMTFiOS00YjBjLWFjZDYtYWZhM2RkMTFkMTkyIiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6IjIzYWE2Y2Y2LWY5YjUtNDc5ZS1hYjIwLTFlMWE1ZDQ4OTczMCIsImlhdCI6MTY4MzAyMTQxNC4zOTMsImlzcyI6Im9yY2hpZCJ9.t3GG9wcUYOJEFTJiogX89O3QZuSl7mdIGTkc5XV4n8Q", "summary": "A powerful text-to-image model that can smartly integrate text into images.", "title": "Stability AI releases DeepFloyd IF"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwNC4xMzcxMnYyP3V0bV9zb3VyY2U9bmxwbGFuZXQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Vla2x5LWFpLWFuZC1ubHAtbmV3cy1tYXktMm5kLTIwMjMiLCJwb3N0X2lkIjoiMjlmMDg1ODItMTFiOS00YjBjLWFjZDYtYWZhM2RkMTFkMTkyIiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6IjIzYWE2Y2Y2LWY5YjUtNDc5ZS1hYjIwLTFlMWE1ZDQ4OTczMCIsImlhdCI6MTY4MzAyMTQxNC4zOTMsImlzcyI6Im9yY2hpZCJ9.cbIn_CItxyKt5kW1-ux5x173k87LMDjvFxsujihkig8", "summary": "A comprehensive and practical guide for practitioners and end-users working with Large Language Models in their NLP tasks.", "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwNC4xMzY1M3YxP3V0bV9zb3VyY2U9bmxwbGFuZXQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Vla2x5LWFpLWFuZC1ubHAtbmV3cy1tYXktMm5kLTIwMjMiLCJwb3N0X2lkIjoiMjlmMDg1ODItMTFiOS00YjBjLWFjZDYtYWZhM2RkMTFkMTkyIiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6IjIzYWE2Y2Y2LWY5YjUtNDc5ZS1hYjIwLTFlMWE1ZDQ4OTczMCIsImlhdCI6MTY4MzAyMTQxNC4zOTMsImlzcyI6Im9yY2hpZCJ9.gsOAbFgaYr-UO5Te7VQHIG4nIuDFJAq6KQN3C9dSJuo", "summary": "Even though the agents were optimized for scoring, in experiments they walked 156% faster, took 63% less time to get up, and kicked 24% faster than a scripted baseline.", "title": "Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning"}], "datetime": "2023-05-02"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-april-24th-2023-6b63ba962240", "news": [{"url": "https://www.wired.com/story/stack-overflow-will-charge-ai-giants-for-training-data/", "summary": "Stack Overflow to charge large-scale AI devs for access to 50M Q&A to boost LLM data quality and result in faster high-quality LLM development.", "title": "Stack Overflow Will Charge AI Giants for Training Data", "topics": ["AI and copyright", "AI datasets"], "sentiment": "positive"}, {"url": "https://www.together.xyz/blog/redpajama", "summary": "RedPajama aims to reproduce LLaMA models with a 7B parameter model, using a filtered dataset of 1.2T tokens. Their goal is open-source reproducibility.", "title": "RedPajama, a project to create leading open-source models, starts by reproducing LLaMA training dataset of over 1.2 trillion tokens", "topics": ["AI datasets", "LLaMA"], "sentiment": "positive"}, {"url": "https://www.axios.com/2023/04/21/humane-ai-powered-wearable", "summary": "Humane debuted its AI wearable, which projects info without a nearby cell phone. It raised $230M with funding from Microsoft, OpenAI CEO, and others.", "title": "Humane previews AI-powered wearable", "topics": ["Funding", "Microsoft", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.theinformation.com/articles/microsoft-readies-ai-chip-as-machine-learning-costs-surge", "summary": "Microsoft creating Athena chip for AI\u2019s large-language models, aiming to save money and time. Amazon, Google & Facebook are creating their own AI chips too.", "title": "Microsoft Readies AI Chip as Machine Learning Costs Surge", "topics": ["AI Chips and GPUs", "Microsoft"], "sentiment": "positive"}, {"url": "https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models", "summary": "Stability AI launched StableLM, an open-source language model (3\u20137B parameters) for text/code that performs well in conversational and coding tasks. Fine-tuned models are available for research.", "title": "Stability AI Launches the First of its StableLM Suite of Language Models", "topics": ["AI for coding", "Stability AI"], "sentiment": "positive"}, {"url": "https://www.atlassian.com/blog/announcements/unleashing-power-of-ai", "summary": "Atlassian now has Atlassian Intelligence \u2014 an AI-powered teammate that makes custom teamwork graphs, understands natural language, and answers queries.", "title": "Introducing Atlassian Intelligence", "topics": [], "sentiment": "positive"}], "guides": [{"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy5ub3Rib3JpbmcuY28vcC9pbnRlbGxpZ2VuY2Utc3VwZXJhYnVuZGFuY2U_dXRtX3NvdXJjZT1ubHBsYW5ldC5iZWVoaWl2LmNvbSZ1dG1fbWVkaXVtPXJlZmVycmFsJnV0bV9jYW1wYWlnbj13ZWVrbHktYWktYW5kLW5scC1uZXdzLWFwcmlsLTI0dGgtMjAyMyIsInBvc3RfaWQiOiI2Mzk1MDAxOS04M2I0LTRjOTUtODJmYS0xYmIxODcwZGY2OGMiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiNzVjNmVjMTItZTA1OC00OWRiLTg3MTUtYTQzMmVjZjdkODVjIiwiaWF0IjoxNjgyMzQ0MTI3LjA2OCwiaXNzIjoib3JjaGlkIn0.OqrHuu3953Vtj_pKG-FLKc08uZsgahAFDxLfrpJrke0", "summary": "The article examines how AI can create more demand for tasks requiring intelligence due to increased overall supply. Various industries are discussed.", "title": "Intelligence Superabundance"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy5hbmFseXRpY3NpbnNpZ2h0Lm5ldC93aGF0LWlzLWZ1enp5LWxvZ2ljLXJvYm90aWNzLWZ1dHVyZS1vZi1hcnRpZmljaWFsLWludGVsbGlnZW5jZT91dG1fc291cmNlPW5scGxhbmV0LmJlZWhpaXYuY29tJnV0bV9tZWRpdW09cmVmZXJyYWwmdXRtX2NhbXBhaWduPXdlZWtseS1haS1hbmQtbmxwLW5ld3MtYXByaWwtMjR0aC0yMDIzIiwicG9zdF9pZCI6IjYzOTUwMDE5LTgzYjQtNGM5NS04MmZhLTFiYjE4NzBkZjY4YyIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiI3NWM2ZWMxMi1lMDU4LTQ5ZGItODcxNS1hNDMyZWNmN2Q4NWMiLCJpYXQiOjE2ODIzNDQxMjcuMDY4LCJpc3MiOiJvcmNoaWQifQ.fchm_KsD4MyjJyy91z2HnkkF8vj_GeIWjy3NS5l2gPk", "summary": "This article covers the concepts of fuzzy logic and robotics in AI along with future applications in self-driving cars, cybernetics, healthcare, education, and decision-making systems.", "title": "What is Fuzzy Logic, Robotics & Future of Artificial Intelligence?"}], "papers": [{"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL21pbmlncHQtNC5naXRodWIuaW8vP3V0bV9zb3VyY2U9bmxwbGFuZXQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Vla2x5LWFpLWFuZC1ubHAtbmV3cy1hcHJpbC0yNHRoLTIwMjMiLCJwb3N0X2lkIjoiNjM5NTAwMTktODNiNC00Yzk1LTgyZmEtMWJiMTg3MGRmNjhjIiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6Ijc1YzZlYzEyLWUwNTgtNDlkYi04NzE1LWE0MzJlY2Y3ZDg1YyIsImlhdCI6MTY4MjM0NDEyNy4wNjgsImlzcyI6Im9yY2hpZCJ9.EB22mp4zfRT0ppMZ1oXTtwfYxVwjO9I_ARC7Q2oUkGs", "summary": "MiniGPT-4 aligns a visual encoder with a large language model to generate detailed image descriptions and create websites.", "title": "Minigpt-4"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3R4dC5jb2hlcmUuY29tL2VtYmVkZGluZy1hcmNoaXZlcy13aWtpcGVkaWEvP3V0bV9zb3VyY2U9bmxwbGFuZXQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Vla2x5LWFpLWFuZC1ubHAtbmV3cy1hcHJpbC0yNHRoLTIwMjMiLCJwb3N0X2lkIjoiNjM5NTAwMTktODNiNC00Yzk1LTgyZmEtMWJiMTg3MGRmNjhjIiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6Ijc1YzZlYzEyLWUwNTgtNDlkYi04NzE1LWE0MzJlY2Y3ZDg1YyIsImlhdCI6MTY4MjM0NDEyNy4wNjgsImlzcyI6Im9yY2hpZCJ9.dSD13j8-T1lCZQCBpoBdn87pMontXn9BOJzWIYZfDHM", "summary": "Cohere\u2019s Embedding Archives contain free, multilingual vectors generated from millions of Wikipedia articles, ideal for AI developers building search systems. Available on Hugging Face Datasets.", "title": "The Embedding Archives: Millions of Wikipedia Article Embeddings in Many Languages"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2dpdGh1Yi5jb20vc3Vuby1haS9iYXJrP3V0bV9zb3VyY2U9bmxwbGFuZXQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Vla2x5LWFpLWFuZC1ubHAtbmV3cy1hcHJpbC0yNHRoLTIwMjMiLCJwb3N0X2lkIjoiNjM5NTAwMTktODNiNC00Yzk1LTgyZmEtMWJiMTg3MGRmNjhjIiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6Ijc1YzZlYzEyLWUwNTgtNDlkYi04NzE1LWE0MzJlY2Y3ZDg1YyIsImlhdCI6MTY4MjM0NDEyNy4wNjgsImlzcyI6Im9yY2hpZCJ9.qwwkeJFXSPCSm5DDCeVqWHzGQj9bWfXYynZwi0Q5bMI", "summary": "\ud83d\udd0a Text-Prompted Generative Audio Model. Bark is a multilingual text-to-audio model that generates realistic speech, music, and nonverbal sounds. Find it on GitHub for research purposes.", "title": "suno-ai/bark: "}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwNC4wOTg0OHYxP3V0bV9zb3VyY2U9bmxwbGFuZXQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Vla2x5LWFpLWFuZC1ubHAtbmV3cy1hcHJpbC0yNHRoLTIwMjMiLCJwb3N0X2lkIjoiNjM5NTAwMTktODNiNC00Yzk1LTgyZmEtMWJiMTg3MGRmNjhjIiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6Ijc1YzZlYzEyLWUwNTgtNDlkYi04NzE1LWE0MzJlY2Y3ZDg1YyIsImlhdCI6MTY4MjM0NDEyNy4wNjgsImlzcyI6Im9yY2hpZCJ9.1cpwikXuJFIx3kJLtysgehD1Tf4RmEv34DwX1v82-nM", "summary": "The paper finds generative search engines often lack full citation support (51.5%). Proposed metrics aim to encourage comprehensive citation use. It emphasizes the need for trustworthy, informative generative search engines.", "title": "Evaluating Veri\ufb01ability in Generative Search Engines"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwNC4wODQ2N3YxP3V0bV9zb3VyY2U9bmxwbGFuZXQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Vla2x5LWFpLWFuZC1ubHAtbmV3cy1hcHJpbC0yNHRoLTIwMjMiLCJwb3N0X2lkIjoiNjM5NTAwMTktODNiNC00Yzk1LTgyZmEtMWJiMTg3MGRmNjhjIiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6Ijc1YzZlYzEyLWUwNTgtNDlkYi04NzE1LWE0MzJlY2Y3ZDg1YyIsImlhdCI6MTY4MjM0NDEyNy4wNjgsImlzcyI6Im9yY2hpZCJ9.oP28h9NqKPx1Epijn2QP8LEfPj4cvFbit960ztun-7M", "summary": "Proposed \u201cgisting\u201d effectively compresses prompts into reusable \u201cgist\u201d tokens for more efficient computation, reducing FLOPs and latency by up to 26x, without compromising output quality.", "title": "Learning to Compress Prompts with Gist Tokens"}], "datetime": "2023-04-24"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-april-17th-2023-88bc420332ea", "news": [{"url": "https://www.bbc.com/news/business-65236848", "summary": "Chinese technology giant Alibaba has announced plans to roll out its own artificial intelligence (AI) ChatGPT-style product called Tongyi Qianwen.", "title": "AI: China tech giant Alibaba to roll out ChatGPT rival", "topics": ["ChatGPT"], "sentiment": "positive"}, {"url": "https://www.washingtonpost.com/technology/2023/04/08/amazon-ai-chatgpt-falling-behind/", "summary": "Amazon reassures employees they\u2019re not behind despite absence in generative AI.", "title": "Amazon tells employees it isn\u2019t falling behind on AI", "topics": ["Amazon"], "sentiment": "positive"}, {"url": "https://www.businessinsider.com/elon-musk-twitter-investment-generative-ai-project-2023-4", "summary": "Elon Musk is pursuing a Twitter AI project with a large language model despite recently calling to halt AI training.", "title": "Elon Musk is moving forward with a new generative-AI project at Twitter after purchasing thousands of GPUs", "topics": ["AI Chips and GPUs"], "sentiment": "positive"}, {"url": "https://www.cnbc.com/2023/04/10/japan-government-weighs-ai-adoption-as-sam-altman-visits-pm-kishida.html", "summary": "Japan exploring OpenAI\u2019s ChatGPT, awaits resolution of cybersecurity concerns.", "title": "Japan government weighs A.I. adoption as OpenAI CEO Sam Altman visits Prime Minister Fumio Kishida", "topics": ["AI regulation", "ChatGPT", "OpenAI"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2023/04/12/chatgpt-italy-gdpr-order/", "summary": "Italy\u2019s Garante gave OpenAI measures to lift suspension of ChatGPT; issue is GDPR violation. Demands inc. data notice, age gate, rights access.", "title": "Italy gives OpenAI initial to-do list for lifting ChatGPT suspension order", "topics": ["AI regulation", "OpenAI", "ChatGPT"], "sentiment": "negative"}, {"url": "https://pyfound.blogspot.com/2023/04/the-eus-proposed-cra-law-may-have.html\u001bOLI", "summary": "EU\u2019s proposed Cyber Resilience Act & Product Liability Act may harm the Python ecosystem by making open-source authors legally responsible for others\u2019 products. Python Software Foundation seeks clear language to avoid unintended consequences.", "title": "Python Software Foundation News: The EU\u2019s Proposed CRA Law May Have Unintended Consequences for the Python Ecosystem", "topics": [], "sentiment": "negative"}, {"url": "https://techcrunch.com/2023/04/13/with-bedrock-amazon-enters-the-generative-ai-race/", "summary": "Amazon Bedrock facilitates building AI-powered apps with pre-trained models from startups and AWS through an API to generate images, logos, and graphics.", "title": "With Bedrock, Amazon enters the generative AI race", "topics": ["AI for images", "Multimodal AI (image, video, audio)", "Amazon"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2023/04/13/chatgpt-spain-gdpr/", "summary": "Spain\u2019s data protection authority probes potential GDPR non-compliance by ChatGPT by OpenAI, following Italy\u2019s example.", "title": "Spain\u2019s privacy watchdog says it\u2019s probing ChatGPT too", "topics": ["AI regulation", "OpenAI"], "sentiment": "negative"}], "guides": [{"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3R4dC5jb2hlcmUuYWkvdW5sb2NraW5nLW5ldy1wb3NzaWJpbGl0aWVzLW1hcmNoLTIwMjNzLXRvcC1ubHAtcGFwZXJzLz91dG1fc291cmNlPW5scGxhbmV0LmJlZWhpaXYuY29tJnV0bV9tZWRpdW09cmVmZXJyYWwmdXRtX2NhbXBhaWduPXdlZWtseS1haS1hbmQtbmxwLW5ld3MtYXByaWwtMTd0aC0yMDIzIiwicG9zdF9pZCI6ImM5OGRiMmJkLWMyNWEtNDU3Ny1iMDc1LWI5ZWFlZWNhNzVlYyIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiJmMmUyZmU1Mi0wZjg0LTRmMDItOTI3MC0xMDFiYmU4NzQ1YTQiLCJpYXQiOjE2ODE3MzYxNTMuMTU1LCJpc3MiOiJvcmNoaWQifQ.W9kNR3o7vGQsR7W2tTkgSecCQivnnffqh4EKsWrp2RA", "summary": "PaLM-E, Aligning Perception with Language Models, and Scaling Expert Language Models.", "title": "Top NLP papers of March"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2h1Z2dpbmdmYWNlLmNvL2Jsb2cvZ3JhcGhtbC1jbGFzc2lmaWNhdGlvbj91dG1fc291cmNlPW5scGxhbmV0LmJlZWhpaXYuY29tJnV0bV9tZWRpdW09cmVmZXJyYWwmdXRtX2NhbXBhaWduPXdlZWtseS1haS1hbmQtbmxwLW5ld3MtYXByaWwtMTd0aC0yMDIzIiwicG9zdF9pZCI6ImM5OGRiMmJkLWMyNWEtNDU3Ny1iMDc1LWI5ZWFlZWNhNzVlYyIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiJmMmUyZmU1Mi0wZjg0LTRmMDItOTI3MC0xMDFiYmU4NzQ1YTQiLCJpYXQiOjE2ODE3MzYxNTMuMTU2LCJpc3MiOiJvcmNoaWQifQ.aau9rONzR5kR5AQh1Nke98lXxRU4iYNq1hs7pwzpB4o", "summary": "Microsoft\u2019s Graphormer model.", "title": "Graph classification with Transformers"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3R4dC5jb2hlcmUuYWkvd2hhdC1hcmUtdHJhbnNmb3JtZXItbW9kZWxzLz91dG1fc291cmNlPW5scGxhbmV0LmJlZWhpaXYuY29tJnV0bV9tZWRpdW09cmVmZXJyYWwmdXRtX2NhbXBhaWduPXdlZWtseS1haS1hbmQtbmxwLW5ld3MtYXByaWwtMTd0aC0yMDIzIiwicG9zdF9pZCI6ImM5OGRiMmJkLWMyNWEtNDU3Ny1iMDc1LWI5ZWFlZWNhNzVlYyIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiJmMmUyZmU1Mi0wZjg0LTRmMDItOTI3MC0xMDFiYmU4NzQ1YTQiLCJpYXQiOjE2ODE3MzYxNTMuMTU2LCJpc3MiOiJvcmNoaWQifQ.t8zW8BW6-zbrCQvsMq4Ak9Y-vpBiLX-x-T87qttdSEw", "summary": "An intuitive guide of the Transformers architecture.", "title": "What Are Transformer Models and How Do They Work?"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FpLmZhY2Vib29rLmNvbS9ibG9nL2FpLWRhdGFzZXQtYW5pbWF0aW9uLWRyYXdpbmdzLz91dG1fc291cmNlPW5scGxhbmV0LmJlZWhpaXYuY29tJnV0bV9tZWRpdW09cmVmZXJyYWwmdXRtX2NhbXBhaWduPXdlZWtseS1haS1hbmQtbmxwLW5ld3MtYXByaWwtMTd0aC0yMDIzIiwicG9zdF9pZCI6ImM5OGRiMmJkLWMyNWEtNDU3Ny1iMDc1LWI5ZWFlZWNhNzVlYyIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiJmMmUyZmU1Mi0wZjg0LTRmMDItOTI3MC0xMDFiYmU4NzQ1YTQiLCJpYXQiOjE2ODE3MzYxNTMuMTU2LCJpc3MiOiJvcmNoaWQifQ.OumkXLzQyY3IiOxSETjnLe1tZUkXbSQdk4_ItFBNkUQ", "summary": "Analyzing and understanding human imagination through drawings.", "title": "A new, unique AI dataset for animating amateur drawings"}], "papers": [{"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy55b3V0dWJlLmNvbS93YXRjaD92PWRkRzJmTTlpNEtrJnV0bV9zb3VyY2U9bmxwbGFuZXQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Vla2x5LWFpLWFuZC1ubHAtbmV3cy1hcHJpbC0xN3RoLTIwMjMiLCJwb3N0X2lkIjoiYzk4ZGIyYmQtYzI1YS00NTc3LWIwNzUtYjllYWVlY2E3NWVjIiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6ImYyZTJmZTUyLTBmODQtNGYwMi05MjcwLTEwMWJiZTg3NDVhNCIsImlhdCI6MTY4MTczNjE1My4xNTYsImlzcyI6Im9yY2hpZCJ9.ec2EiRtgC0QwOsXIAQX93H9Zybv5kaGNQduVply8pm4", "summary": "Open Assistant released their open-source LLM. It understands tasks, interacts with third-party systems, and is open to personalization.", "title": "Open-Assistant model released"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2dpdGh1Yi5jb20vZGF0YWJyaWNrc2xhYnMvZG9sbHk_dXRtX3NvdXJjZT1ubHBsYW5ldC5iZWVoaWl2LmNvbSZ1dG1fbWVkaXVtPXJlZmVycmFsJnV0bV9jYW1wYWlnbj13ZWVrbHktYWktYW5kLW5scC1uZXdzLWFwcmlsLTE3dGgtMjAyMyIsInBvc3RfaWQiOiJjOThkYjJiZC1jMjVhLTQ1NzctYjA3NS1iOWVhZWVjYTc1ZWMiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiZjJlMmZlNTItMGY4NC00ZjAyLTkyNzAtMTAxYmJlODc0NWE0IiwiaWF0IjoxNjgxNzM2MTUzLjE1NiwiaXNzIjoib3JjaGlkIn0.DCjylTUz32yh6J2W_azWazqPU4Qdxg9b_nEjRk-1770", "summary": "Dolly 2 is an instruction-following large language model trained on the Databricks machine-learning platform that is licensed for commercial use.", "title": "Dolly 2 released"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3RlY2hjcnVuY2guY29tLzIwMjMvMDQvMTIvb3BlbmFpLWxvb2tzLWJleW9uZC1kaWZmdXNpb24td2l0aC1jb25zaXN0ZW5jeS1iYXNlZC1pbWFnZS1nZW5lcmF0b3IvP3V0bV9zb3VyY2U9bmxwbGFuZXQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Vla2x5LWFpLWFuZC1ubHAtbmV3cy1hcHJpbC0xN3RoLTIwMjMiLCJwb3N0X2lkIjoiYzk4ZGIyYmQtYzI1YS00NTc3LWIwNzUtYjllYWVlY2E3NWVjIiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6ImYyZTJmZTUyLTBmODQtNGYwMi05MjcwLTEwMWJiZTg3NDVhNCIsImlhdCI6MTY4MTczNjE1My4xNTYsImlzcyI6Im9yY2hpZCJ9.zfjk1BuOOXRQP1KtITLhrcC_8tGWbbWrOObK8RPY2Mk", "summary": "OpenAI\u2019s new \u201cconsistency models\u201d generate images in just 1\u20132 steps, faster & more efficient than diffusion models.", "title": "OpenAI looks beyond diffusion with consistency-based image generator"}], "datetime": "2023-04-17"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-april-11th-2023-992142d079e4", "news": [{"url": "https://techcrunch.com/2023/04/06/anthropics-5b-4-year-plan-to-take-on-openai/", "summary": "Anthropic aims to raise $5B in 2 yrs to compete with OpenAI using its new AI model, Claude-Next. It has various industries as potential customers.", "title": "Anthropic\u2019s $5B, 4-year plan to take on OpenAI", "topics": ["Funding", "Claude", "Anthropic", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.techradar.com/news/samsung-workers-leaked-company-secrets-by-using-chatgpt", "summary": "Samsung workers leaked proprietary data through ChatGPT. OpenAI may now possess trade secrets. The incident raised alarms about data privacy and GDPR rule.", "title": "Samsung workers made a major error by using ChatGPT", "topics": ["AI safety", "AI and copyright", "AI regulation", "ChatGPT", "OpenAI"], "sentiment": "negative"}, {"url": "https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/", "summary": "Introducing Segment Anything: democratizing image segmentation with SAM \u2014 a versatile, promptable model trained on a versatile dataset under Apache 2.0.", "title": "Meta introduced Segment Anything: Working toward the first foundation model for image segmentation", "topics": ["AI for images", "Multimodal AI (image, video, audio)", "Model release", "Meta"], "sentiment": "positive"}, {"url": "https://blog.langchain.dev/announcing-our-10m-seed-round-led-by-benchmark/", "summary": "Python and TypeScript package LangChain raised $10M to empower developers building language model-powered apps. Offers modular components to ease prototyping.", "title": "LangChain announced a $10M seed round led by Benchmark", "topics": ["Funding"], "sentiment": "positive"}, {"url": "https://www.latimes.com/business/story/2023-03-29/335-000-pay-offered-for-ai-whisperer-jobs-in-red-hot-market", "summary": "Skills in prompt engineering, training on AI tools, paying up to $335K p.a., are in demand. Red-hot market attracting various sectors, may be overestimating technology.", "title": "$335,000 pay offered for \u2018AI whisperer\u2019 jobs in red-hot market", "topics": [], "sentiment": "positive"}, {"url": "https://www.washingtonpost.com/technology/2023/04/08/amazon-ai-chatgpt-falling-behind/", "summary": "Amazon execs reassure cloud staff that despite no generative AI wars involvement, company not behind AI game. Partners with Stability AI, Hugging Face.", "title": "Amazon tells employees it isn\u2019t falling behind on AI", "topics": ["Amazon", "Stability AI", "Hugging Face"], "sentiment": "positive"}, {"url": "https://jonathanturley.org/2023/04/06/defamed-by-chatgpt-my-own-bizarre-experience-with-artificiality-of-artificial-intelligence/", "summary": "AI program ChatGPT created false claim of sexual harassment w/o factual basis, warns of biases & dangers of AI in tackling disinformation.", "title": "Defamed by ChatGPT: My Own Bizarre Experience with Artificiality of \u201cArtificial Intelligence\u201d", "topics": ["AI safety", "AI regulation", "ChatGPT", "OpenAI"], "sentiment": "negative"}], "guides": [{"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2h1Z2dpbmdmYWNlLmNvL2Jsb2cvc3RhY2tsbGFtYT91dG1fc291cmNlPW5scGxhbmV0LmJlZWhpaXYuY29tJnV0bV9tZWRpdW09cmVmZXJyYWwmdXRtX2NhbXBhaWduPXdlZWtseS1haS1hbmQtbmxwLW5ld3MtYXByaWwtMTF0aC0yMDIzIiwicG9zdF9pZCI6ImUxZTk1ZDQ1LWQ1NDEtNGM2YS1iMzI2LTY0OTljYzExZmE0OSIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiIxY2JjZWM2Yy1jODUxLTQ2ZjItYjMxYi1hZjA3NDEzYTFiZWMiLCJpYXQiOjE2ODEyMDcyNTMuMDgyLCJpc3MiOiJvcmNoaWQifQ.n2ZXNNQKGOwETny0bVbnd75DiAOjI5XLp6O782ltsKE", "summary": "Learn to train a LLaMA model using Reinforcement Learning from Human Feedback, Supervised Fine-tuning, and Reward Modeling.", "title": "StackLLaMA: A hands-on guide to train LLaMA with RLHF"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy50ZWNobm9sb2d5cmV2aWV3LmNvbS8yMDIzLzA0LzAzLzEwNzA3NTAvYWktam9icy1sZWdhbC1maWVsZC1ncHQtNC8_dXRtX3NvdXJjZT1ubHBsYW5ldC5iZWVoaWl2LmNvbSZ1dG1fbWVkaXVtPXJlZmVycmFsJnV0bV9jYW1wYWlnbj13ZWVrbHktYWktYW5kLW5scC1uZXdzLWFwcmlsLTExdGgtMjAyMyIsInBvc3RfaWQiOiJlMWU5NWQ0NS1kNTQxLTRjNmEtYjMyNi02NDk5Y2MxMWZhNDkiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiMWNiY2VjNmMtYzg1MS00NmYyLWIzMWItYWYwNzQxM2ExYmVjIiwiaWF0IjoxNjgxMjA3MjUzLjA4MiwiaXNzIjoib3JjaGlkIn0.dO-QEj9tRcIdter-CAsuAUYyBa68SHm_hmt6u9XbmeU", "summary": "AI can impact 300 million jobs globally. Legal industry shows promise in automating language-based jobs but human guidance is still vital for AI.", "title": "AI might not steal your job, but it could change it"}], "papers": [{"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2dpdGh1Yi5jb20vVG9yYW50dWxpbm8vQXV0by1HUFQ_dXRtX3NvdXJjZT1ubHBsYW5ldC5iZWVoaWl2LmNvbSZ1dG1fbWVkaXVtPXJlZmVycmFsJnV0bV9jYW1wYWlnbj13ZWVrbHktYWktYW5kLW5scC1uZXdzLWFwcmlsLTExdGgtMjAyMyIsInBvc3RfaWQiOiJlMWU5NWQ0NS1kNTQxLTRjNmEtYjMyNi02NDk5Y2MxMWZhNDkiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiMWNiY2VjNmMtYzg1MS00NmYyLWIzMWItYWYwNzQxM2ExYmVjIiwiaWF0IjoxNjgxMjA3MjUzLjA4MiwiaXNzIjoib3JjaGlkIn0.LjigW9bhJPswSp0vPXYSKpt6As0ddAaS7xRRVnh1_S4", "summary": "Auto-GPT is an open-source app exemplifying GPT-4 capabilities, autonomously managing businesses to increase net worth.", "title": "Torantulino/Auto-GPT: An experimental open-source attempt to make GPT-4 fully autonomous"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2dpdGh1Yi5jb20vbWF5b29lYXIvZ3B0NC1wZGYtY2hhdGJvdC1sYW5nY2hhaW4_dXRtX3NvdXJjZT1ubHBsYW5ldC5iZWVoaWl2LmNvbSZ1dG1fbWVkaXVtPXJlZmVycmFsJnV0bV9jYW1wYWlnbj13ZWVrbHktYWktYW5kLW5scC1uZXdzLWFwcmlsLTExdGgtMjAyMyIsInBvc3RfaWQiOiJlMWU5NWQ0NS1kNTQxLTRjNmEtYjMyNi02NDk5Y2MxMWZhNDkiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiMWNiY2VjNmMtYzg1MS00NmYyLWIzMWItYWYwNzQxM2ExYmVjIiwiaWF0IjoxNjgxMjA3MjUzLjA4MiwiaXNzIjoib3JjaGlkIn0.bOWdHRktKeZVsd8cLoL-YaloGGAo2z4hhkDc-ZBsIv8", "summary": "Make a question-answering chatbot with GPT-4 & LangChain over several large PDF files.", "title": "mayooear/gpt4-pdf-chatbot-langchain: GPT4 & LangChain Chatbot for large PDF docs"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMy4xNDA3MHYzP3V0bV9zb3VyY2U9bmxwbGFuZXQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Vla2x5LWFpLWFuZC1ubHAtbmV3cy1hcHJpbC0xMXRoLTIwMjMiLCJwb3N0X2lkIjoiZTFlOTVkNDUtZDU0MS00YzZhLWIzMjYtNjQ5OWNjMTFmYTQ5IiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6IjFjYmNlYzZjLWM4NTEtNDZmMi1iMzFiLWFmMDc0MTNhMWJlYyIsImlhdCI6MTY4MTIwNzI1My4wODIsImlzcyI6Im9yY2hpZCJ9.WS-3NTp2KzEJnwKwEhnjE94U8XhoY8CvmvVka9TYkRw", "summary": "ChatDoctor uses 205k medical conversations to fine-tune a language model that can understand patients\u2019 needs and give informed advice. The framework and dataset are publicly available for further dialogue model development.", "title": "ChatDoctor: A Medical Chat Model Fine-tuned"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2dpdGh1Yi5jb20vSURFQS1SZXNlYXJjaC9Hcm91bmRlZC1TZWdtZW50LUFueXRoaW5nP3V0bV9zb3VyY2U9bmxwbGFuZXQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Vla2x5LWFpLWFuZC1ubHAtbmV3cy1hcHJpbC0xMXRoLTIwMjMiLCJwb3N0X2lkIjoiZTFlOTVkNDUtZDU0MS00YzZhLWIzMjYtNjQ5OWNjMTFmYTQ5IiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6IjFjYmNlYzZjLWM4NTEtNDZmMi1iMzFiLWFmMDc0MTNhMWJlYyIsImlhdCI6MTY4MTIwNzI1My4wODIsImlzcyI6Im9yY2hpZCJ9.PQJnEdVZQYcN3o-LgvbnWBbI-NR7V5CFTsI1ndwddp4", "summary": "Combine Grounding DINO, Segment Anything, and Stable Diffusion models for a powerful pipeline solving complex problems. Demos, tips, and future possibilities included.", "title": "IDEA-Research/Grounded-Segment-Anything: Marrying Grounding DINO with Segment Anything & Stable Diffusion & BLIP"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMy4xNzU4MD91dG1fc291cmNlPW5scGxhbmV0LmJlZWhpaXYuY29tJnV0bV9tZWRpdW09cmVmZXJyYWwmdXRtX2NhbXBhaWduPXdlZWtseS1haS1hbmQtbmxwLW5ld3MtYXByaWwtMTF0aC0yMDIzIiwicG9zdF9pZCI6ImUxZTk1ZDQ1LWQ1NDEtNGM2YS1iMzI2LTY0OTljYzExZmE0OSIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiIxY2JjZWM2Yy1jODUxLTQ2ZjItYjMxYi1hZjA3NDEzYTFiZWMiLCJpYXQiOjE2ODEyMDcyNTMuMDgyLCJpc3MiOiJvcmNoaWQifQ.4WvKrMkbBA4R6_raMa2m6mmSgOFRr86y1JvBAIhAFRU", "summary": "HuggingGPT uses language models like ChatGPT to connect AI models in the Hugging Face community, simplifying complex AI tasks across modalities and domains.", "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwNC4wMTg1MnYyP3V0bV9zb3VyY2U9bmxwbGFuZXQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Vla2x5LWFpLWFuZC1ubHAtbmV3cy1hcHJpbC0xMXRoLTIwMjMiLCJwb3N0X2lkIjoiZTFlOTVkNDUtZDU0MS00YzZhLWIzMjYtNjQ5OWNjMTFmYTQ5IiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6IjFjYmNlYzZjLWM4NTEtNDZmMi1iMzFiLWFmMDc0MTNhMWJlYyIsImlhdCI6MTY4MTIwNzI1My4wODMsImlzcyI6Im9yY2hpZCJ9.8zISlZVXy1xewg7JeBvX99NzAZaw9x33V7PHO-bwcYY", "summary": "Survey of ChatGPT & GPT-4 models, including potential applications in education, medicine, physics, etc. Ethical concerns and future advancements discussed.", "title": "Summary of ChatGPT/GPT-4 Research"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwOi8vb3BlbnNhbWl6ZGF0LmNvbS9wb3N0cy9jaGF0Z3B0X3N1cnZleS8_dXRtX3NvdXJjZT1ubHBsYW5ldC5iZWVoaWl2LmNvbSZ1dG1fbWVkaXVtPXJlZmVycmFsJnV0bV9jYW1wYWlnbj13ZWVrbHktYWktYW5kLW5scC1uZXdzLWFwcmlsLTExdGgtMjAyMyIsInBvc3RfaWQiOiJlMWU5NWQ0NS1kNTQxLTRjNmEtYjMyNi02NDk5Y2MxMWZhNDkiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiMWNiY2VjNmMtYzg1MS00NmYyLWIzMWItYWYwNzQxM2ExYmVjIiwiaWF0IjoxNjgxMjA3MjUzLjA4MywiaXNzIjoib3JjaGlkIn0.UNVbX-1v8zRNKVotmHeds5dCQpKoSTnUQ3xSCJmoflY", "summary": "Survey of 141 arXiv papers comparing ChatGPT to other NLP models found its performance to be lower than expected. Reasons include suboptimal utilization, biased results, and incomplete multilingual evaluation.", "title": "ChatGPT Survey: Performance on NLP datasets"}], "datetime": "2023-04-11"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-march-27th-2023-83d5909d6324", "news": [{"url": "https://openai.com/blog/chatgpt-plugins", "summary": "ChatGPT testing new plugins with select users before launching for widespread use. Features from Wolfram, Slack, and Expedia have been created.", "title": "ChatGPT plugins", "topics": ["ChatGPT"], "sentiment": "positive"}, {"url": "https://beta.tome.app/", "summary": "Tome is an AI-powered storytelling platform that allows users to explain complex topics using narration and embeds from other sources.", "title": "The AI-powered storytelling format", "topics": ["Multimodal AI (image, video, audio)"], "sentiment": "positive"}, {"url": "https://stability.ai/blog/stable-diffusion-reimagine", "summary": "Stability.ai\u2019s Stable Diffusion Reimagine uses AI to make new images from originals. It has limits and biases, but will soon be open-sourced for betterment.", "title": "Stable Diffusion Reimagine \u2014 Stability AI", "topics": ["AI for images", "Stable Diffusion", "Stability AI"], "sentiment": "positive"}, {"url": "https://github.blog/2023-03-22-github-copilot-x-the-ai-powered-developer-experience/", "summary": "GitHub unveils GitHub Copilot X, a developer experience that uses OpenAI\u2019s GPT-4 to provide a chat interface, and voice-to-code AI.", "title": "GitHub Copilot X: The AI-powered developer experience", "topics": ["AI for coding", "GPT-4 and GPT-4 turbo", "OpenAI"], "sentiment": "positive"}, {"url": "https://bard.google.com/", "summary": "Bard is a Google AI experiment aiming to improve creativity and productivity by collaborating with users.", "title": "Meet Bard", "topics": ["Google"], "sentiment": "positive"}, {"url": "https://abuqader.substack.com/p/releasing-alpaca-30b", "summary": "Article explains release of Alpaca-30B, \u201cinstruction-tuned\u201d version of Facebook\u2019s Llama model, benefits of fine-tuning, usage steps and community involvement.", "title": "Releasing Alpaca-30B", "topics": ["Model release", "LLaMA", "Meta"], "sentiment": "positive"}, {"url": "https://research.runwayml.com/gen2", "summary": "Gen-2 by Runway is an AI tool that can synthesize new videos using only an image, text prompt, or words without needing a camera or lights.", "title": "Gen-2 by Runway", "topics": ["AI for images", "Multimodal AI (image, video, audio)"], "sentiment": "positive"}, {"url": "https://blog.mozilla.org/en/mozilla/introducing-mozilla-ai-investing-in-trustworthy-ai/", "summary": "Mozilla.ai: Investing in trustworthy AI. Mozilla creates startup Mozilla.ai with $30m initial investment, led by AI expert Moez Draief. They focus on generative AI and people-centric systems.", "title": "Introducing ", "topics": ["Funding"], "sentiment": "positive"}], "guides": [{"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy5hc3NlbWJseWFpLmNvbS9ibG9nL2FpLXRyZW5kcy1ncmFwaC1uZXVyYWwtbmV0d29ya3MvIiwicG9zdF9pZCI6ImI5NTk0NzM0LWMwNGMtNDU1Mi04NzVlLWU0MmQwZWZhOWQ2MyIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiI1OWVlNTJkYy1iOTIxLTQ0NDgtYTFiNi02ZDQwNTMxMTBiMGYiLCJpYXQiOjE2Nzk5MjUyMDMuMDU0LCJpc3MiOiJvcmNoaWQifQ.pmi3-1rbKK5RmorcMTaOL2F-GbSQ5WzSdeU6qA3miWA", "summary": "GNNs process graph data, understand entity relationships, and have exponential growth in published research. They are expected to continue growing in popularity.", "title": "AI trends in 2023: Graph Neural Networks"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL29uZXVzZWZ1bHRoaW5nLnN1YnN0YWNrLmNvbS9wL3VzaW5nLWFpLXRvLW1ha2UtdGVhY2hpbmctZWFzaWVyIiwicG9zdF9pZCI6ImI5NTk0NzM0LWMwNGMtNDU1Mi04NzVlLWU0MmQwZWZhOWQ2MyIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiI1OWVlNTJkYy1iOTIxLTQ0NDgtYTFiNi02ZDQwNTMxMTBiMGYiLCJpYXQiOjE2Nzk5MjUyMDMuMDU0LCJpc3MiOiJvcmNoaWQifQ.VhEfLnmAyuSxPy-GOos-YVG-PvPSuiryEAnlqBkCnmU", "summary": "AI in classrooms improves learning by providing personalized experiences, generating examples, giving feedback, creating study groups, and diagnostic tests.", "title": "Using AI to make teaching easier & more impactful"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FuZHJld21heW5lYmxvZy53b3JkcHJlc3MuY29tLzIwMjMvMDMvMjMvY2hhdGdwdC1jb2RlLWludGVycHJldGVyLW1hZ2ljLyIsInBvc3RfaWQiOiJiOTU5NDczNC1jMDRjLTQ1NTItODc1ZS1lNDJkMGVmYTlkNjMiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiNTllZTUyZGMtYjkyMS00NDQ4LWExYjYtNmQ0MDUzMTEwYjBmIiwiaWF0IjoxNjc5OTI1MjAzLjA1NCwiaXNzIjoib3JjaGlkIn0.RZJRnQJgwXNMRRL0WzHjp_Lxag5EjQVw0nT7LotluJU", "summary": "OpenAI\u2019s ChatGPT integrates third-party tools like a Code Interpreter for generating & running code, analyzing data, graphing spreadsheets, and more.", "title": "ChatGPT + Code Interpreter = Magic"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwOi8vbGFibWwuYWkiLCJwb3N0X2lkIjoiYjk1OTQ3MzQtYzA0Yy00NTUyLTg3NWUtZTQyZDBlZmE5ZDYzIiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6IjU5ZWU1MmRjLWI5MjEtNDQ0OC1hMWI2LTZkNDA1MzExMGIwZiIsImlhdCI6MTY3OTkyNTIwMy4wNTUsImlzcyI6Im9yY2hpZCJ9.-8gTrSeL2Kmd75N_qp21YhaeVwYBihoC7XdG29IcXIM", "summary": "Annotated PyTorch Paper Implementations. Get simple PyTorch implementations with documented explanations in labml.ai\u2019s Annotated PyTorch Paper Implementations. Updated constantly; easy to use.", "title": "labml.ai"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy5nYXRlc25vdGVzLmNvbS9UaGUtQWdlLW9mLUFJLUhhcy1CZWd1biIsInBvc3RfaWQiOiJiOTU5NDczNC1jMDRjLTQ1NTItODc1ZS1lNDJkMGVmYTlkNjMiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiNTllZTUyZGMtYjkyMS00NDQ4LWExYjYtNmQ0MDUzMTEwYjBmIiwiaWF0IjoxNjc5OTI1MjAzLjA1NSwiaXNzIjoib3JjaGlkIn0.0SJNvP6cYAPx_5O0LwEzjm-5xUyDAJ8WNdKWtD8aReE", "summary": "AI is groundbreaking, changing work, learning, travel, health care, communication. Also, it can reduce inequities and help address climate change.", "title": "The Age of AI has begun"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy5waGlsc2NobWlkLmRlL2dldHRpbmctc3RhcnRlZC1weXRvcmNoLTItMC10cmFuc2Zvcm1lcnMiLCJwb3N0X2lkIjoiYjk1OTQ3MzQtYzA0Yy00NTUyLTg3NWUtZTQyZDBlZmE5ZDYzIiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6IjU5ZWU1MmRjLWI5MjEtNDQ0OC1hMWI2LTZkNDA1MzExMGIwZiIsImlhdCI6MTY3OTkyNTIwMy4wNTUsImlzcyI6Im9yY2hpZCJ9.fUKnysT0bYSLuA_-Lwqrm_mPUtZNgtlhaxkUrYkGm7A", "summary": "PyTorch 2.0 boosts performance with TorchDynamo, AOTAutograd, PrimTorch, and TorchInductor. Pairs with HuggingFace; Hub offers model versioning.", "title": "Getting started with Pytorch 2.0 and Hugging Face Transformers"}], "papers": [{"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMy4xMjcxMiIsInBvc3RfaWQiOiJiOTU5NDczNC1jMDRjLTQ1NTItODc1ZS1lNDJkMGVmYTlkNjMiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiNTllZTUyZGMtYjkyMS00NDQ4LWExYjYtNmQ0MDUzMTEwYjBmIiwiaWF0IjoxNjc5OTI1MjAzLjA1NSwiaXNzIjoib3JjaGlkIn0.pjQoMSpo6pN73xpWe5o4n1ToF8zeLzMMMK00cMHWaY8", "summary": "A paper investigates early GPT-4 version, showing human-level performance in various domains. Societal implications and autoregressive challenges discussed.", "title": "Sparks of Artificial General Intelligence:"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMy4xMDEzMCIsInBvc3RfaWQiOiJiOTU5NDczNC1jMDRjLTQ1NTItODc1ZS1lNDJkMGVmYTlkNjMiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiNTllZTUyZGMtYjkyMS00NDQ4LWExYjYtNmQ0MDUzMTEwYjBmIiwiaWF0IjoxNjc5OTI1MjAzLjA1NSwiaXNzIjoib3JjaGlkIn0.3FOESVb78XO29iA4WKhMekqRRHz5b_KArEr95wRvIbk", "summary": "A study on GPTs\u2019 impact on the US workforce found 80% of the workers may face up to 10% task displacement, and 19% may see more than 50% displacement. Traits of LLMs suggest significant economic and social implications.", "title": "GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL211bHRpbW9kYWwtcmVhY3QuZ2l0aHViLmlvLyIsInBvc3RfaWQiOiJiOTU5NDczNC1jMDRjLTQ1NTItODc1ZS1lNDJkMGVmYTlkNjMiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiNTllZTUyZGMtYjkyMS00NDQ4LWExYjYtNmQ0MDUzMTEwYjBmIiwiaWF0IjoxNjc5OTI1MjAzLjA1NSwiaXNzIjoib3JjaGlkIn0.zEvdQy9pSbUh_IhnM9j9VULaWT5rJCMNImAgGWD4g5s", "summary": "MM-ReAct article explores ChatGPT\u2019s potential for multimodal reasoning & action.", "title": "MM-ReAct: Prompting ChatGPT for Multimodal Reasoning and Action"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FpLmdvb2dsZWJsb2cuY29tLzIwMjMvMDMvdmlkMnNlcS1wcmV0cmFpbmVkLXZpc3VhbC1sYW5ndWFnZS5odG1sIiwicG9zdF9pZCI6ImI5NTk0NzM0LWMwNGMtNDU1Mi04NzVlLWU0MmQwZWZhOWQ2MyIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiI1OWVlNTJkYy1iOTIxLTQ0NDgtYTFiNi02ZDQwNTMxMTBiMGYiLCJpYXQiOjE2Nzk5MjUyMDMuMDU1LCJpc3MiOiJvcmNoaWQifQ.0Bh3hir4N0UeKTBY3slDLSTs-5ZR05I8rx9OvtDuJgA", "summary": "Introducing Vid2Seq model for video captioning. Predicts events and captions as a sequence of tokens, achieves outstanding results on video benchmarks.", "title": "Vid2Seq: a pretrained visual language model for describing multi-event videos"}], "datetime": "2023-03-27"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-march-20th-2023-bee6c0c2526b", "news": [{"url": "https://openai.com/product/gpt-4", "summary": "GPT-4 is multimodal and accepts also images as inputs. It can handle over 25,000 words of text. GPT-4 can be tested with ChatGPT Plus. Otherwise, there\u2019s an API waitlist.", "title": "GPT-4 is out", "topics": ["Multimodal AI (image, video, audio)", "GPT-4 and GPT-4 turbo", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.microsoft.com/en-us/microsoft-365/blog/2023/03/16/introducing-microsoft-365-copilot-a-whole-new-way-to-work/", "summary": "which combines the power of large language models (LLMs) with your data in the Microsoft Graph \u2014 your calendar, emails, chats, documents, meetings, and more \u2014 and the Microsoft 365 apps.", "title": "Microsoft announced Microsoft 365 Copilot", "topics": ["Microsoft"], "sentiment": "positive"}, {"url": "https://www.theverge.com/2023/3/13/23637675/microsoft-chatgpt-bing-millions-dollars-supercomputer-openai", "summary": "Microsoft says it connected tens of thousands of Nvidia A100 chips and reworked server racks to build the hardware behind ChatGPT and its own Bing AI bot.", "title": "Microsoft spent hundreds of millions of dollars on a ChatGPT supercomputer", "topics": ["NVIDIA", "AI Chips and GPUs", "Microsoft", "ChatGPT"], "sentiment": "positive"}, {"url": "https://pytorch.org/get-started/pytorch-2.0/", "summary": "", "title": "PyTorch 2.0 is out", "topics": [], "sentiment": "positive"}, {"url": "https://www.platformer.news/p/microsoft-just-laid-off-one-of-its", "summary": "", "title": "Microsoft just laid off one of its responsible AI teams", "topics": ["Microsoft"], "sentiment": "negative"}, {"url": "https://blog.google/technology/ai/ai-developers-google-cloud-workspace/", "summary": "", "title": "New generative AI features in Google Workspace and Cloud", "topics": ["Google"], "sentiment": "positive"}, {"url": "https://www.adept.ai/blog/series-b", "summary": "", "title": "Adept raised $350M in new funding as part of its Series B", "topics": ["Funding"], "sentiment": "positive"}], "guides": [{"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3RlY2hjcnVuY2guY29tLzIwMjMvMDMvMTQvNS13YXlzLWdwdC00LW91dHNtYXJ0cy1jaGF0Z3B0IiwicG9zdF9pZCI6ImMzOWZhNDE0LTY0MjItNGIxMS05ZGM5LTA3MDU0MDI0ZjY1NyIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiI0MjUyNDI1OS0zMTZkLTQ0YWQtYmM3Ny03ZWIyY2VmZDk4NDgiLCJpYXQiOjE2NzkzMTExMTkuMTc5LCJpc3MiOiJvcmNoaWQifQ.0QirWGtNL0qyslGGPjAV8BVx-etxN93ns5djn591Czk", "summary": "GPT-4 can see and understand images, is harder to trick, and has a longer memory.", "title": "5 ways GPT-4 outsmarts ChatGPT"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy55b3V0dWJlLmNvbS93YXRjaD92PTJ6VzMzTGZmZlBjIiwicG9zdF9pZCI6ImMzOWZhNDE0LTY0MjItNGIxMS05ZGM5LTA3MDU0MDI0ZjY1NyIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiI0MjUyNDI1OS0zMTZkLTQ0YWQtYmM3Ny03ZWIyY2VmZDk4NDgiLCJpYXQiOjE2NzkzMTExMTkuMTc5LCJpc3MiOiJvcmNoaWQifQ.8uOPlTAEX2ySs4Hj9CBNzTgxJZxUSDqzmUhr9CYaZHM", "summary": "", "title": "What we know so far on GPT4"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FsYmVydG9yb21nYXIubWVkaXVtLmNvbS9wcm9tcHQtZW5naW5lZXJpbmctaXMtcHJvYmFibHktbW9yZS1pbXBvcnRhbnQtdGhhbi15b3UtdGhpbmstOTg4NjZiMjU0N2UwIiwicG9zdF9pZCI6ImMzOWZhNDE0LTY0MjItNGIxMS05ZGM5LTA3MDU0MDI0ZjY1NyIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiI0MjUyNDI1OS0zMTZkLTQ0YWQtYmM3Ny03ZWIyY2VmZDk4NDgiLCJpYXQiOjE2NzkzMTExMTkuMTc5LCJpc3MiOiJvcmNoaWQifQ.XieDUnqt56gjOP2rhbrpL1Qzj_Ez-QeNce9S4pxgChk", "summary": "", "title": "Prompt Engineering Is Probably More Important Than You Think"}], "papers": [{"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2dpdGh1Yi5jb20vdGF0c3UtbGFiL3N0YW5mb3JkX2FscGFjYSIsInBvc3RfaWQiOiJjMzlmYTQxNC02NDIyLTRiMTEtOWRjOS0wNzA1NDAyNGY2NTciLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiNDI1MjQyNTktMzE2ZC00NGFkLWJjNzctN2ViMmNlZmQ5ODQ4IiwiaWF0IjoxNjc5MzExMTE5LjE3OSwiaXNzIjoib3JjaGlkIn0.3xh5uEOsklpfVbY9_xsOEFZPMTdqFvx4TBAsL-0SgFA", "summary": "which aims to build and share an instruction-following LLaMA model.", "title": "Stanford published the Alpaca project"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2Nkbi5vcGVuYWkuY29tL3BhcGVycy9ncHQtNC5wZGYiLCJwb3N0X2lkIjoiYzM5ZmE0MTQtNjQyMi00YjExLTlkYzktMDcwNTQwMjRmNjU3IiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6IjQyNTI0MjU5LTMxNmQtNDRhZC1iYzc3LTdlYjJjZWZkOTg0OCIsImlhdCI6MTY3OTMxMTExOS4xNzksImlzcyI6Im9yY2hpZCJ9.KiochxEi0BurZqc0V6GCy0UB2_VAGCrXW2I-l138va4", "summary": "", "title": "The GPT-4 technical report"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjIxMi4xMDU2MCIsInBvc3RfaWQiOiJjMzlmYTQxNC02NDIyLTRiMTEtOWRjOS0wNzA1NDAyNGY2NTciLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiNDI1MjQyNTktMzE2ZC00NGFkLWJjNzctN2ViMmNlZmQ5ODQ4IiwiaWF0IjoxNjc5MzExMTE5LjE3OSwiaXNzIjoib3JjaGlkIn0.L0d3sUBWj7ZOIV3az93OYejojQUj-lK2j7RXT__b_vw", "summary": "Self-Instruct is a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off its own generations", "title": "Self-Instruct: Aligning Language Model with Self Generated Instructions"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMy4wNjg2NSIsInBvc3RfaWQiOiJjMzlmYTQxNC02NDIyLTRiMTEtOWRjOS0wNzA1NDAyNGY2NTciLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiNDI1MjQyNTktMzE2ZC00NGFkLWJjNzctN2ViMmNlZmQ5ODQ4IiwiaWF0IjoxNjc5MzExMTE5LjE4LCJpc3MiOiJvcmNoaWQifQ.H1auKuYl9iMwd0DOWR1omy2D3MQ_iymX5FjwZ0VeaxQ", "summary": "The paper studies how to do high-throughput LLM inference using limited resources.", "title": "High-throughput Generative Inference of Large Language Models with a Single GPU"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMy4wNzIyNiIsInBvc3RfaWQiOiJjMzlmYTQxNC02NDIyLTRiMTEtOWRjOS0wNzA1NDAyNGY2NTciLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiNDI1MjQyNTktMzE2ZC00NGFkLWJjNzctN2ViMmNlZmQ5ODQ4IiwiaWF0IjoxNjc5MzExMTE5LjE4LCJpc3MiOiJvcmNoaWQifQ.eysIxmulK1PxvTq7BANG7yFCpZ8QzS4NaHqazPJp0UY", "summary": "", "title": "Scaling Vision-Language Models with Sparse Mixture of Experts"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMy4wOTQ2MSIsInBvc3RfaWQiOiJjMzlmYTQxNC02NDIyLTRiMTEtOWRjOS0wNzA1NDAyNGY2NTciLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiNDI1MjQyNTktMzE2ZC00NGFkLWJjNzctN2ViMmNlZmQ5ODQ4IiwiaWF0IjoxNjc5MzExMTE5LjE4LCJpc3MiOiJvcmNoaWQifQ.5gdyybMJ_cPqYk9TXeSZNHrh6b0968cbw5r4dUHUVDs", "summary": "The model slightly passes the \u201cAlgorithms and Data Structures\u201d exam.", "title": "ChatGPT Participates in a Computer Science Exam"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMi4xNDA0NSIsInBvc3RfaWQiOiJjMzlmYTQxNC02NDIyLTRiMTEtOWRjOS0wNzA1NDAyNGY2NTciLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiNDI1MjQyNTktMzE2ZC00NGFkLWJjNzctN2ViMmNlZmQ5ODQ4IiwiaWF0IjoxNjc5MzExMTE5LjE4LCJpc3MiOiJvcmNoaWQifQ.VkeMF_tsH-GXa6KQgVinfSLmBUQDH3pqewmHPkoi37o", "summary": "The authors introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot).", "title": "Language Is Not All You Need: Aligning Perception with Language Models"}], "datetime": "2023-03-20"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-march-13th-2023-6b8c27a0a442", "news": [{"url": "https://www.heise.de/news/GPT-4-is-coming-next-week-and-it-will-be-multimodal-says-Microsoft-Germany-7540972.html", "summary": "Microsoft Germany recently presented their Large Language Models (LLM) like the GPT series as a disruptive force for companies and their Azure-OpenAI offering.", "title": "Microsoft mentioned the imminent release of GPT-4", "topics": ["Microsoft", "GPT-4 and GPT-4 turbo", "OpenAI"], "sentiment": "positive"}, {"url": "https://fortune.com/2023/03/04/stability-ai-raise-funds-4-billion-valuation-artificial-intelligence-captivates-investors/", "summary": "", "title": "Stability AI looks to raise funds at $4B valuation as artificial intelligence captivates investors.", "topics": ["Funding", "Stability AI"], "sentiment": "positive"}, {"url": "https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html", "summary": "USM is a family of state-of-the-art speech models with 2B parameters, trained on 12 million hours of speech and 28 billion sentences of text, spanning 300+ languages.", "title": "Google\u2019s Universal Speech Model (USM): State-of-the-art speech AI for 100+ languages", "topics": ["Speech-to-text", "Google"]}, {"url": "https://ai.facebook.com/blog/muavic-audio-visual-speech-translation-benchmark/", "summary": "Using visual information to improve performance for English speech recognition tasks.", "title": "MuAViC: The first audio-video speech translation benchmark", "topics": ["Speech-to-text", "Multimodal AI (image, video, audio)"], "sentiment": "positive"}, {"url": "https://arstechnica.com/information-technology/2023/02/man-beats-machine-at-go-in-human-victory-over-ai/", "summary": "", "title": "Man beats machine at Go. Amateur exploited weakness in systems that have otherwise dominated grandmasters.", "topics": ["Reinforcement learning", "DeepMind"], "sentiment": "positive"}, {"url": "https://www.washingtonpost.com/technology/2023/03/05/ai-voice-scam/", "summary": "", "title": "They thought loved ones were calling for help. It was an AI scam.", "topics": ["AI safety", "Text-to-speech", "AI regulation"], "sentiment": "negative"}], "guides": [{"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL21sY29udGVzdHMuY29tL3N0YXRlLW9mLWNvbXBldGl0aXZlLW1hY2hpbmUtbGVhcm5pbmctMjAyMiIsInBvc3RfaWQiOiI5ZTJhYjQ5MC00ZWVlLTRjYTEtYjQ4NC1hNWM2ZDJhNDEyNTEiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiMmI3NDBjMDktMzk3NS00MmVhLWJlZDctMDNkYmM1NWM1ZDA1IiwiaWF0IjoxNjc4NzAyNzE0LjA0NSwiaXNzIjoib3JjaGlkIn0.PhnSGgAElLfqzA8QatpAKqJDj6pmJkq7H4mk1-OyIMI", "summary": "An analysis of the winning models of 200+ ML competitions in 2022.", "title": "The State of Competitive Machine Learning"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy5hc3NlbWJseWFpLmNvbS9ibG9nL2VtZXJnZW50LWFiaWxpdGllcy1vZi1sYXJnZS1sYW5ndWFnZS1tb2RlbHMvIiwicG9zdF9pZCI6IjllMmFiNDkwLTRlZWUtNGNhMS1iNDg0LWE1YzZkMmE0MTI1MSIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiIyYjc0MGMwOS0zOTc1LTQyZWEtYmVkNy0wM2RiYzU1YzVkMDUiLCJpYXQiOjE2Nzg3MDI3MTQuMDQ1LCJpc3MiOiJvcmNoaWQifQ.xG0b6MiZ3hZt2LJzfPsH416jdvFUbFZeyxFyCtiMGlU", "summary": "Emergence can be defined as the sudden appearance of novel behavior. Large Language Models apparently display emergence by suddenly gaining new abilities as they grow.", "title": "Emergent Abilities of Large Language Models"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy56ZXRhLWFscGhhLmNvbS9wb3N0L211c3QtcmVhZC10aGUtMTAwLW1vc3QtY2l0ZWQtYWktcGFwZXJzLWluLTIwMjIiLCJwb3N0X2lkIjoiOWUyYWI0OTAtNGVlZS00Y2ExLWI0ODQtYTVjNmQyYTQxMjUxIiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6IjJiNzQwYzA5LTM5NzUtNDJlYS1iZWQ3LTAzZGJjNTVjNWQwNSIsImlhdCI6MTY3ODcwMjcxNC4wNDUsImlzcyI6Im9yY2hpZCJ9.3c9sucKWNJncvULyjUFcT6IblKn03MxbQGyvNJdaonA", "summary": "", "title": "The 100 most cited AI papers in 2022."}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2h1Z2dpbmdmYWNlLmNvL2Jsb2cvdHJsLXBlZnQiLCJwb3N0X2lkIjoiOWUyYWI0OTAtNGVlZS00Y2ExLWI0ODQtYTVjNmQyYTQxMjUxIiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6IjJiNzQwYzA5LTM5NzUtNDJlYS1iZWQ3LTAzZGJjNTVjNWQwNSIsImlhdCI6MTY3ODcwMjcxNC4wNDUsImlzcyI6Im9yY2hpZCJ9.mwkIRLCXLCCa3LF-NS5DWcDFWDqhgsI94t_S-0FrIgM", "summary": "", "title": "Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU"}], "papers": [{"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3BhbG0tZS5naXRodWIuaW8vIiwicG9zdF9pZCI6IjllMmFiNDkwLTRlZWUtNGNhMS1iNDg0LWE1YzZkMmE0MTI1MSIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiIyYjc0MGMwOS0zOTc1LTQyZWEtYmVkNy0wM2RiYzU1YzVkMDUiLCJpYXQiOjE2Nzg3MDI3MTQuMDQ1LCJpc3MiOiJvcmNoaWQifQ._sYA7zufadK7keFi0ikZbuTjkNPMQfQD_RUkq-ZG3N8", "summary": "Inputs to the embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings.", "title": "PaLM-E: An Embodied Multimodal Language Model"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2dpdGh1Yi5jb20vbWljcm9zb2Z0L3Zpc3VhbC1jaGF0Z3B0IiwicG9zdF9pZCI6IjllMmFiNDkwLTRlZWUtNGNhMS1iNDg0LWE1YzZkMmE0MTI1MSIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiIyYjc0MGMwOS0zOTc1LTQyZWEtYmVkNy0wM2RiYzU1YzVkMDUiLCJpYXQiOjE2Nzg3MDI3MTQuMDQ1LCJpc3MiOiJvcmNoaWQifQ.yWskirhCTWYCfn0imtRkDblWtLIN4wqqNVMRZe4U2rA", "summary": "", "title": "Visual ChatGPT connects ChatGPT and a series of Visual Foundation Models to enable sending and receiving images during chatting."}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjIxMi4xMzEzOCIsInBvc3RfaWQiOiI5ZTJhYjQ5MC00ZWVlLTRjYTEtYjQ4NC1hNWM2ZDJhNDEyNTEiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiMmI3NDBjMDktMzk3NS00MmVhLWJlZDctMDNkYmM1NWM1ZDA1IiwiaWF0IjoxNjc4NzAyNzE0LjA0NSwiaXNzIjoib3JjaGlkIn0.F2ROUiTWv643MnFqfezlp7mXW_Euh6rr3_nRqEje0q0", "summary": "Introducing MultiMedQA, a benchmark combining six existing open-question answering datasets spanning professional medical exams, research, and consumer queries. Flan-PaLM is SOTA.", "title": "Large Language Models Encode Clinical Knowledge"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMy4wMjUwNiIsInBvc3RfaWQiOiI5ZTJhYjQ5MC00ZWVlLTRjYTEtYjQ4NC1hNWM2ZDJhNDEyNTEiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiMmI3NDBjMDktMzk3NS00MmVhLWJlZDctMDNkYmM1NWM1ZDA1IiwiaWF0IjoxNjc4NzAyNzE0LjA0NSwiaXNzIjoib3JjaGlkIn0.kI-FiV2xxcfPJnyTKDG6bOAHz47dyzu3A3z-kxRaj3c", "summary": "Prismer only requires training of a small number of components, with the majority of network weights inherited from readily-available, pre-trained domain experts, and kept frozen during training.", "title": "Prismer: A Vision-Language Model with An Ensemble of Experts"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2dpdGh1Yi5jb20vWnJyU2t5d2Fsa2VyL0NhRm8iLCJwb3N0X2lkIjoiOWUyYWI0OTAtNGVlZS00Y2ExLWI0ODQtYTVjNmQyYTQxMjUxIiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6IjJiNzQwYzA5LTM5NzUtNDJlYS1iZWQ3LTAzZGJjNTVjNWQwNSIsImlhdCI6MTY3ODcwMjcxNC4wNDUsImlzcyI6Im9yY2hpZCJ9.7ZFSug-zfO4XC-g4IoV-HrcpPNzuLCHJEvewUUgV7rE", "summary": "", "title": "Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMy4wMzg0NiIsInBvc3RfaWQiOiI5ZTJhYjQ5MC00ZWVlLTRjYTEtYjQ4NC1hNWM2ZDJhNDEyNTEiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiMmI3NDBjMDktMzk3NS00MmVhLWJlZDctMDNkYmM1NWM1ZDA1IiwiaWF0IjoxNjc4NzAyNzE0LjA0NSwiaXNzIjoib3JjaGlkIn0.Vl-SgFAh1G0bZzGvfh6rt1JbP4FAAtyMRFb6kwsAT1Q", "summary": "", "title": "Larger language models do in-context learning differently"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMy4wNTM5OCIsInBvc3RfaWQiOiI5ZTJhYjQ5MC00ZWVlLTRjYTEtYjQ4NC1hNWM2ZDJhNDEyNTEiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiMmI3NDBjMDktMzk3NS00MmVhLWJlZDctMDNkYmM1NWM1ZDA1IiwiaWF0IjoxNjc4NzAyNzE0LjA0NSwiaXNzIjoib3JjaGlkIn0.3UXRvJs-DmYSE5Zkr1CuPELMtIpmZ3m6HHNPuE4vEEs", "summary": "It uses the Zero-shot chain-of-thought prompting technique to generate multiple Algebraic expressions or Python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results.", "title": "MathPrompter: Mathematical Reasoning using Large Language Models"}], "datetime": "2023-03-13"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-march-6th-2023-4d74ecda92b1", "news": [{"url": "https://openai.com/blog/introducing-chatgpt-and-whisper-apis", "summary": "The ChatGPT API is x10 cheaper than GPT3 API.", "title": "Introducing ChatGPT and Whisper APIs", "topics": ["ChatGPT", "GPT-3, GPT-3.5, and GPT-3.5 turbo", "OpenAI"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2023/02/27/anthropic-begins-supplying-its-text-generating-ai-models-to-startups/", "summary": "", "title": "Anthropic begins supplying its text-generating AI models to select startups", "topics": ["Anthropic"], "sentiment": "positive"}, {"url": "https://www.theverge.com/2023/2/28/23618214/microsoft-windows-11-update-bing-ai-taskbar-touch-improvements-screen-recording-features", "summary": "New Windows updates will have a lot of AI.", "title": "Microsoft brings its new AI-powered Bing to the Windows 11 taskbar", "topics": ["Microsoft"], "sentiment": "positive"}, {"url": "https://www.theverge.com/2023/3/1/23620143/youtube-ai-tool-features-ceo-neal-mohan-google-alphabet", "summary": "", "title": "YouTube\u2019s new leader teases AI tools that can virtually swap creators\u2019 outfits and locations", "topics": ["AI for images", "Multimodal AI (image, video, audio)"], "sentiment": "positive"}, {"url": "https://openai.com/blog/planning-for-agi-and-beyond", "summary": "OpenAI\u2019s steps in making sure that AGI will benefit anyone.", "title": "Planning for AGI and beyond", "topics": ["AI safety", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.theverge.com/2023/2/27/23614959/snapchat-my-ai-chatbot-chatgpt-openai-plus-subscription", "summary": "Initially available only to Snapchat Plus subscribers.", "title": "Snapchat is releasing its own AI chatbot powered by ChatGPT", "topics": ["ChatGPT", "OpenAI"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2023/02/24/voicemod", "summary": "", "title": "Voicemod raises $14M for its speech-to-speech AI", "topics": ["Text-to-speech", "Funding"], "sentiment": "positive"}, {"url": "https://www.yitay.net/blog/flan-ul2-20b", "summary": "A UL2 model finetuned on the FLAN dataset (instruction tuning). An alternative to Flan-T5.", "title": "A New Open Source Flan 20B with UL2", "topics": ["AI datasets", "Model release"], "sentiment": "positive"}], "guides": [{"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3R4dC5jb2hlcmUuYWkvYmVzdC1vZi1uYXR1cmFsLWxhbmd1YWdlLXByb2Nlc3NpbmctZmVicnVhcnktMjAyMy10b3AtbmxwLXBhcGVycy8iLCJwb3N0X2lkIjoiZmViYWQ3ZTktZGU4Yy00OWNjLWIyOWEtYTBiNWIzMTIwZDgwIiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6IjBhYzliOWViLWM0MDYtNGU5Ny04OTAzLWM3Y2Q5NzAxZGE4ZCIsImlhdCI6MTY3ODEwMTI0NS4xOTUsImlzcyI6Im9yY2hpZCJ9.eY7m1x5GiV6G6v5xRotvzrrHMnpwyJNUHGhspSvpoWo", "summary": "", "title": "The Best of NLP: February 2023\u2019s Top NLP Papers"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3R4dC5jb2hlcmUuYWkvZ2VuZXJhdGl2ZS1haS1mdXR1cmUtb3ItcHJlc2VudC8iLCJwb3N0X2lkIjoiZmViYWQ3ZTktZGU4Yy00OWNjLWIyOWEtYTBiNWIzMTIwZDgwIiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6IjBhYzliOWViLWM0MDYtNGU5Ny04OTAzLWM3Y2Q5NzAxZGE4ZCIsImlhdCI6MTY3ODEwMTI0NS4xOTUsImlzcyI6Im9yY2hpZCJ9.NVbQFBIxVxhCy4JL6Pxg2FSGX2gs8avARy-WoTrUVbM", "summary": "", "title": "What\u2019s the big deal with Generative AI? Is it the future or the present?"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy50b3Bib3RzLmNvbS90aGUtZXZvbHV0aW9uLW9mLXNjaWVuY2UtZnJvbS1kZXNjYXJ0ZXMtdG8tZ2VuZXJhdGl2ZS1haS8iLCJwb3N0X2lkIjoiZmViYWQ3ZTktZGU4Yy00OWNjLWIyOWEtYTBiNWIzMTIwZDgwIiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6IjBhYzliOWViLWM0MDYtNGU5Ny04OTAzLWM3Y2Q5NzAxZGE4ZCIsImlhdCI6MTY3ODEwMTI0NS4xOTUsImlzcyI6Im9yY2hpZCJ9.ojX7ufEPzsr-1MAEI8EVBKgYqFPQEJ-Ryhvo3zB-G7E", "summary": "", "title": "The Evolution Of Science: From Descartes To Generative AI"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2dpdGh1Yi5ibG9nLzIwMjMtMDItMDYtdGhlLXRlY2hub2xvZ3ktYmVoaW5kLWdpdGh1YnMtbmV3LWNvZGUtc2VhcmNoIiwicG9zdF9pZCI6ImZlYmFkN2U5LWRlOGMtNDljYy1iMjlhLWEwYjViMzEyMGQ4MCIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiIwYWM5YjllYi1jNDA2LTRlOTctODkwMy1jN2NkOTcwMWRhOGQiLCJpYXQiOjE2NzgxMDEyNDUuMTk1LCJpc3MiOiJvcmNoaWQifQ.wBaa3EX01nyeicsJYpqQUsmcUnAZRGO_tW3y4TGRZks", "summary": "A look at what went into building the world\u2019s largest public code search index.", "title": "The technology behind GitHub\u2019s new code search"}], "papers": [{"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2dpdGh1Yi5jb20vbS1iYWluL3doaXNwZXJYIiwicG9zdF9pZCI6ImZlYmFkN2U5LWRlOGMtNDljYy1iMjlhLWEwYjViMzEyMGQ4MCIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiIwYWM5YjllYi1jNDA2LTRlOTctODkwMy1jN2NkOTcwMWRhOGQiLCJpYXQiOjE2NzgxMDEyNDUuMTk1LCJpc3MiOiJvcmNoaWQifQ.DfniSmHdGNPHYiL82KMMeviKy8ki71FRYDb6Sm9lVdc", "summary": "A repo for Whisper-Based Automatic Speech Recognition (ASR) with improved timestamp accuracy using forced alignment.", "title": "WhisperX"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMi4xMTM4MiIsInBvc3RfaWQiOiJmZWJhZDdlOS1kZThjLTQ5Y2MtYjI5YS1hMGI1YjMxMjBkODAiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiMGFjOWI5ZWItYzQwNi00ZTk3LTg5MDMtYzdjZDk3MDFkYThkIiwiaWF0IjoxNjc4MTAxMjQ1LjE5NSwiaXNzIjoib3JjaGlkIn0.Pv-mu7QPYZ8PJrJAFWEzbC5DQxqe4yAi-aqQOoNPuds", "summary": "A catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs.", "title": "A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMi4xMjgxMyIsInBvc3RfaWQiOiJmZWJhZDdlOS1kZThjLTQ5Y2MtYjI5YS1hMGI1YjMxMjBkODAiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiMGFjOWI5ZWItYzQwNi00ZTk3LTg5MDMtYzdjZDk3MDFkYThkIiwiaWF0IjoxNjc4MTAxMjQ1LjE5NSwiaXNzIjoib3JjaGlkIn0.HeUwVYaS81RVEi2cgHZ7FswhQ9t81BSubQTWftuSZAo", "summary": "The paper introduces LLM-Augmenter system, which augments a black-box LLM with a set of plug-and-play modules", "title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2hvaG9udS12aWNtbC5naXRodWIuaW8vRGlyZWN0ZWREaWZmdXNpb24uUGFnZSIsInBvc3RfaWQiOiJmZWJhZDdlOS1kZThjLTQ5Y2MtYjI5YS1hMGI1YjMxMjBkODAiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiMGFjOWI5ZWItYzQwNi00ZTk3LTg5MDMtYzdjZDk3MDFkYThkIiwiaWF0IjoxNjc4MTAxMjQ1LjE5NSwiaXNzIjoib3JjaGlkIn0.UZgcESlxHWmxoJ1hIy_Qi5cyUUuaKOzq7_yu9HhI-Sw", "summary": "Models often struggle to compose scenes containing several key objects such as characters in specified positional relationships.", "title": "Directed Diffusion: Direct Control of Object Placement through Attention Guidance"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMi4xMzc5NSIsInBvc3RfaWQiOiJmZWJhZDdlOS1kZThjLTQ5Y2MtYjI5YS1hMGI1YjMxMjBkODAiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiMGFjOWI5ZWItYzQwNi00ZTk3LTg5MDMtYzdjZDk3MDFkYThkIiwiaWF0IjoxNjc4MTAxMjQ1LjE5NSwiaXNzIjoib3JjaGlkIn0.ebzEx9yfwhwakL5eBqLyvewdX9TJaxD-W0YdwEYVzX0", "summary": "The paper analyzes over 300,000 tweets and more than 150 scientific papers to investigate how ChatGPT is perceived and discussed.", "title": "ChatGPT: A Meta-Analysis after 2.5 Months"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMi4xNDgzOCIsInBvc3RfaWQiOiJmZWJhZDdlOS1kZThjLTQ5Y2MtYjI5YS1hMGI1YjMxMjBkODAiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiMGFjOWI5ZWItYzQwNi00ZTk3LTg5MDMtYzdjZDk3MDFkYThkIiwiaWF0IjoxNjc4MTAxMjQ1LjE5NiwiaXNzIjoib3JjaGlkIn0.2-6FnFVuQLP-A2N-xnMHZusf-XyzrIHUO5zT2Zu8mDQ", "summary": "The paper explores the use of LMs as adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm.", "title": "EvoPrompting: Language Models for Code-Level Neural Architecture Search"}], "datetime": "2023-03-06"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-february-27th-2023-d04721df28d7", "news": [{"url": "https://techcrunch.com/2023/02/21/openai-foundry-will-let-customers-buy-dedicated-capacity-to-run-its-ai-models/", "summary": "It will soon be possible to have dedicated GPT3 models.", "title": "OpenAI Foundry will let customers buy dedicated compute to run GPT3 and their other models", "topics": ["GPT-3, GPT-3.5, and GPT-3.5 turbo", "OpenAI"], "sentiment": "positive"}, {"url": "https://huggingface.co/blog/aws-partnership", "summary": "Making Generative AI models more open-source.", "title": "Hugging Face and AWS partner to make AI more accessible", "topics": ["Hugging Face", "Amazon"], "sentiment": "positive"}, {"url": "https://scitechdaily.com/how-ai-can-help-create-and-optimize-drugs-to-treat-opioid-addiction/", "summary": "It could save thousands of lives every year.", "title": "How AI Can Help Create and Optimize Drugs To Treat Opioid Addiction", "topics": ["AI in healthcare"], "sentiment": "positive"}, {"url": "https://www.wired.com/story/roblox-generative-ai-gaming-universe/", "summary": "The company aims to draw on the new technology\u2019s code-writing ability to make its digital worlds even more customizable.", "title": "Roblox Is Bringing Generative AI to Its Gaming Universe", "topics": ["AI for coding"], "sentiment": "positive"}, {"url": "https://www.semafor.com/article/02/17/2023/how-chatbots-can-change-journalism-or-not", "summary": "Experiments refining newspaper articles with Claude, the chatbot from Anthropic.", "title": "How chatbots can change journalism. Or not", "topics": ["Claude", "Anthropic"], "sentiment": "positive"}, {"url": "https://ai.googleblog.com/2023/02/google-research-2022-beyond-natural.html", "summary": "Advances in physics, biology, and natural sciences.", "title": "Google Research on Natural sciences", "topics": ["Google"], "sentiment": "positive"}], "guides": [{"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2RjYWkuY3NhaWwubWl0LmVkdS8iLCJwb3N0X2lkIjoiY2FlZGUyMTAtMDA0Ny00YzM3LTgxN2YtMzk0NGJjZjU0NmQ3IiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6IjFiMWMxOTk3LTZlZTktNDMwNy05NmEzLTNjNzQ1YmUwZGU0NyIsImlhdCI6MTY3NzQ5MDg5Ni40NTMsImlzcyI6Im9yY2hpZCJ9.b0FmAxqh7V06RbJWlXL6G81vZLD4iyf6VHQH_-RYM8A", "summary": "A free and open course that focuses on data quality.", "title": "MIT course on Introduction to Data-Centric AI"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL29uZXVzZWZ1bHRoaW5nLnN1YnN0YWNrLmNvbS9wL215LWNsYXNzLXJlcXVpcmVkLWFpLWhlcmVzLXdoYXQtaXZlIiwicG9zdF9pZCI6ImNhZWRlMjEwLTAwNDctNGMzNy04MTdmLTM5NDRiY2Y1NDZkNyIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiIxYjFjMTk5Ny02ZWU5LTQzMDctOTZhMy0zYzc0NWJlMGRlNDciLCJpYXQiOjE2Nzc0OTA4OTYuNDUzLCJpc3MiOiJvcmNoaWQifQ.ZrZOb5XtuqBohYg_ATYKk-s7JdbGhB69spApD7xv_Pg", "summary": "Experiments from a teacher giving homeworks with ChatGPT.", "title": "Lessons learned while using ChatGPT in education"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2V2ZXJ5LnRvL2NoYWluLW9mLXRob3VnaHQvd3JpdGluZy1lc3NheXMtd2l0aC1haS1hLWd1aWRlIiwicG9zdF9pZCI6ImNhZWRlMjEwLTAwNDctNGMzNy04MTdmLTM5NDRiY2Y1NDZkNyIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiIxYjFjMTk5Ny02ZWU5LTQzMDctOTZhMy0zYzc0NWJlMGRlNDciLCJpYXQiOjE2Nzc0OTA4OTYuNDUzLCJpc3MiOiJvcmNoaWQifQ.Yh6AxtjEyV867JZuja5lqXi2TKu3UTGJaWOimvLBtbU", "summary": "Advice on how to incorporate generative AI in your writing process.", "title": "Writing Essays With AI: A Guide"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy5zaGlueW9iamVjdHMuZ2cvYmxvZy9UZXh0LXRvLUltYWdlLURpZmZ1c2lvbi1Nb2RlbHMiLCJwb3N0X2lkIjoiY2FlZGUyMTAtMDA0Ny00YzM3LTgxN2YtMzk0NGJjZjU0NmQ3IiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6IjFiMWMxOTk3LTZlZTktNDMwNy05NmEzLTNjNzQ1YmUwZGU0NyIsImlhdCI6MTY3NzQ5MDg5Ni40NTQsImlzcyI6Im9yY2hpZCJ9.pr0kHfRIJv9-hFAQr5yWft2KMcAXPhUiKgW0MqFPHHQ", "summary": "A simple explanation of how text-conditioned diffusion models work.", "title": "Text-to-Image Diffusion Models: A Guide for Non-Technical Readers"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy50b3Bib3RzLmNvbS9vdmVyY29taW5nLXRoZS1saW1pdGF0aW9ucy1vZi1sYXJnZS1sYW5ndWFnZS1tb2RlbHMvIiwicG9zdF9pZCI6ImNhZWRlMjEwLTAwNDctNGMzNy04MTdmLTM5NDRiY2Y1NDZkNyIsInB1YmxpY2F0aW9uX2lkIjoiMmZjYmZjNTctZDU4NC00MjAzLWJiM2UtYzEwNDhlN2QzOTkzIiwidmlzaXRfdG9rZW4iOiIxYjFjMTk5Ny02ZWU5LTQzMDctOTZhMy0zYzc0NWJlMGRlNDciLCJpYXQiOjE2Nzc0OTA4OTYuNDU0LCJpc3MiOiJvcmNoaWQifQ.2P2ytGYM6Z4qdvYLY79e0TLeY9_EDvXWRJO2VaySyHQ", "summary": "Ideas to complement the intelligence of LLMs.", "title": "Overcoming The Limitations Of Large Language Models"}], "papers": [{"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMi4wODU4MiIsInBvc3RfaWQiOiJjYWVkZTIxMC0wMDQ3LTRjMzctODE3Zi0zOTQ0YmNmNTQ2ZDciLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiMWIxYzE5OTctNmVlOS00MzA3LTk2YTMtM2M3NDViZTBkZTQ3IiwiaWF0IjoxNjc3NDkwODk2LjQ1NCwiaXNzIjoib3JjaGlkIn0.SCd-xeygDlnikoY_j7RrzjbxzTtMrQ_mHt3o97jI-RY", "summary": "Exploring alternative objectives for pretraining LMs in a way that also guides them to generate text aligned with human preferences.", "title": "Pretraining Language Models with Human Preferences"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMi4wOTIxMCIsInBvc3RfaWQiOiJjYWVkZTIxMC0wMDQ3LTRjMzctODE3Zi0zOTQ0YmNmNTQ2ZDciLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiMWIxYzE5OTctNmVlOS00MzA3LTk2YTMtM2M3NDViZTBkZTQ3IiwiaWF0IjoxNjc3NDkwODk2LjQ1NCwiaXNzIjoib3JjaGlkIn0.AnJMcUiCcbGVUF-pU93V2VsuClEteNnImOEHAagbrMU", "summary": "GPT models achieve very competitive translation quality for high resource languages, while having limited capabilities for low resource languages.", "title": "How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMi4wNTIwNiIsInBvc3RfaWQiOiJjYWVkZTIxMC0wMDQ3LTRjMzctODE3Zi0zOTQ0YmNmNTQ2ZDciLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiMWIxYzE5OTctNmVlOS00MzA3LTk2YTMtM2M3NDViZTBkZTQ3IiwiaWF0IjoxNjc3NDkwODk2LjQ1NCwiaXNzIjoib3JjaGlkIn0.yA-K6V1RTNHXfAolunjo_iAvnoMOkQ2jXSZAZAw2vXM", "summary": "An alternative approach to RLHF: converting feedback to instruction by relabeling the original one and training the model for better alignment in a supervised manner.", "title": "The Wisdom of Hindsight Makes Language Models Better Instruction Followers"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMi4wNTQ0MiIsInBvc3RfaWQiOiJjYWVkZTIxMC0wMDQ3LTRjMzctODE3Zi0zOTQ0YmNmNTQ2ZDciLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiMWIxYzE5OTctNmVlOS00MzA3LTk2YTMtM2M3NDViZTBkZTQ3IiwiaWF0IjoxNjc3NDkwODk2LjQ1NCwiaXNzIjoib3JjaGlkIn0.sysCcwy0jnBmZAK0fMuw-bjDqt_-jf-ACdreRpR_cK4", "summary": "From 4B to 22B parameters, ViT demonstrates increasing performance with scale.", "title": "Scaling Vision Transformers to 22 Billion Parameters"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMi4xMDIwNSIsInBvc3RfaWQiOiJjYWVkZTIxMC0wMDQ3LTRjMzctODE3Zi0zOTQ0YmNmNTQ2ZDciLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiMWIxYzE5OTctNmVlOS00MzA3LTk2YTMtM2M3NDViZTBkZTQ3IiwiaWF0IjoxNjc3NDkwODk2LjQ1NCwiaXNzIjoib3JjaGlkIn0.6PixgzWVjuUrJVgpm5YPLMSHM04zEWKOO0K_SLD3v4w", "summary": "Using ChatGPT for entity-relation triple extraction, named entity recognition, and event extraction.", "title": "Zero-Shot Information Extraction via Chatting with ChatGPT"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMi4xMjE5MiIsInBvc3RfaWQiOiJjYWVkZTIxMC0wMDQ3LTRjMzctODE3Zi0zOTQ0YmNmNTQ2ZDciLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiMWIxYzE5OTctNmVlOS00MzA3LTk2YTMtM2M3NDViZTBkZTQ3IiwiaWF0IjoxNjc3NDkwODk2LjQ1NCwiaXNzIjoib3JjaGlkIn0.14FMe5TbDEB8klhmDIj_CkhbsuU6YwMugxptlySWYW4", "summary": "RLHF for text-to-image models.", "title": "Aligning Text-to-Image Models using Human Feedback"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2dpdGh1Yi5jb20vZmFjZWJvb2tyZXNlYXJjaC9sbGFtYSIsInBvc3RfaWQiOiJjYWVkZTIxMC0wMDQ3LTRjMzctODE3Zi0zOTQ0YmNmNTQ2ZDciLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiMWIxYzE5OTctNmVlOS00MzA3LTk2YTMtM2M3NDViZTBkZTQ3IiwiaWF0IjoxNjc3NDkwODk2LjQ1NCwiaXNzIjoib3JjaGlkIn0.VK6OU-meP3MJMU0IVwdyUUSNamFxe-J9KgLbFUgCaDI", "summary": "A collection of foundation language models ranging from 7B to 65B parameters by Meta.", "title": "LLaMA: A repository for Open and Efficient Foundation Language Models"}], "datetime": "2023-02-27"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-february-20th-2023-2f3a8360f3eb", "news": [{"url": "https://huggingface.co/blog/peft", "summary": "now available on Hugging Face.", "title": "PEFT, a method for Finetuning transformers with high efficiency and low data", "topics": ["Hugging Face"], "sentiment": "positive"}, {"url": "https://gpt3demo.com/map", "summary": "and organized by category.", "title": "A collection of 600+ apps powered by GPT3", "topics": ["GPT-3, GPT-3.5, and GPT-3.5 turbo", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web", "summary": "OpenAI\u2019s chatbot offers paraphrases, whereas Google offers quotes. Which do we prefer?", "title": "ChatGPT Is a Blurry JPEG of the Web", "topics": ["Google", "ChatGPT", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.theverge.com/2023/2/10/23593980/microsoft-bing-chatgpt-ai-teams-outlook-integration", "summary": "", "title": "Microsoft to integrate ChatGPT into Word, PowerPoint, and Outlook soon", "topics": ["Microsoft", "ChatGPT"], "sentiment": "positive"}, {"url": "https://dkb.blog/p/bing-ai-cant-be-trusted", "summary": "", "title": "After Google ads showed Bard inaccuracies, Bing showed inaccuracies as well in their AI demo", "topics": ["Google", "Microsoft"], "sentiment": "negative"}, {"url": "https://www.wired.com/story/apple-spotify-audiobook-narrators-ai-contract/", "summary": "AI legal controversies may go into audio generation as well.", "title": "Audiobook Narrators Fear Apple Used Their Voices to Train AI", "topics": ["AI and copyright", "Text-to-speech", "AI regulation", "Apple"], "sentiment": "negative"}, {"url": "https://hai.stanford.edu/news/could-stable-diffusion-solve-gap-medical-imaging-data", "summary": "Stanford scholars found a way to generate synthetic chest X-rays by fine-tuning the open-source Stable Diffusion foundation model.", "title": "Could Stable Diffusion Solve a Gap in Medical Imaging Data?", "topics": ["AI in healthcare", "AI for images", "Multimodal AI (image, video, audio)", "Stable Diffusion", "Stability AI"], "sentiment": "positive"}, {"url": "https://www.ft.com/content/baf68476-5b7e-4078-9b3e-ddfce710a6e2", "summary": "", "title": "A law firm has been using its own chatbot to draft legal contracts", "topics": [], "sentiment": "positive"}, {"url": "https://openai.com/blog/how-should-ai-systems-behave/", "summary": "a summary of how ChatGPT\u2019s behavior is shaped and how OpenAI plans to improve ChatGPT\u2019s default behavior.", "title": "How should AI systems behave, and who should decide", "topics": ["AI safety", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.forbes.com/sites/johnkoetsier/2023/02/10/chatgpt-burns-millions-every-day-can-computer-scientists-make-ai-one-million-times-more-efficient/", "summary": "", "title": "ChatGPT Burns Millions Every Day. Can Computer Scientists Make AI One Million Times More Efficient?", "topics": ["ChatGPT", "OpenAI"], "sentiment": "negative"}, {"url": "https://huggingface.co/blog/blip-2", "summary": "The guide introduces BLIP-2 from Salesforce Research which enables a suite of state-of-the-art visual-language models that are now available in Hugging Face Transformers.", "title": "Zero-shot image-to-text generation with BLIP-2", "topics": ["Hugging Face", "AI for images", "Multimodal AI (image, video, audio)"], "sentiment": "positive"}], "guides": [], "papers": [{"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9wZGYvMjMwMi4wMTU2MC5wZGYiLCJwb3N0X2lkIjoiNDY4ZmUzMjMtYWNlMS00ZTY2LWFmNjEtMWQxNDEyNzRmNzE5IiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6IjgxYTE3Zjk0LTU1NDAtNDU2Yy1iN2U1LTM0OGJkMTBhODBhNCIsImlhdCI6MTY3Njg4NDkxOS4wMDUsImlzcyI6Im9yY2hpZCJ9.zDv948OCEdNcA1mOzck3fEUa0CL052AErHEoKWVW_aI", "summary": "an interactive planning approach based on Large Language Models tested on Minecraft.", "title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMi4wNTgxNyIsInBvc3RfaWQiOiI0NjhmZTMyMy1hY2UxLTRlNjYtYWY2MS0xZDE0MTI3NGY3MTkiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiODFhMTdmOTQtNTU0MC00NTZjLWI3ZTUtMzQ4YmQxMGE4MGE0IiwiaWF0IjoxNjc2ODg0OTE5LjAwNSwiaXNzIjoib3JjaGlkIn0.WpH6Rrv_p4j5vZRQH9xeZOh9QGI99Eclq_Cyk413Vys", "summary": "generating levels of the Sokoban videogame using LLMs.", "title": "Level Generation Through Large Language Models"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMi4wNzg0MiIsInBvc3RfaWQiOiI0NjhmZTMyMy1hY2UxLTRlNjYtYWY2MS0xZDE0MTI3NGY3MTkiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiODFhMTdmOTQtNTU0MC00NTZjLWI3ZTUtMzQ4YmQxMGE4MGE0IiwiaWF0IjoxNjc2ODg0OTE5LjAwNSwiaXNzIjoib3JjaGlkIn0.xfJ_2T7WBaY2cAkytU5Lj00zAiLXvfmJEIiT1okai3M", "summary": "This survey reviews works in which language models are augmented with reasoning skills and the ability to use tools.", "title": "Augmented Language Models: a Survey"}], "datetime": "2023-02-20"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-february-13th-2023-519daf5c3b57", "news": [{"url": "https://blog.google/technology/ai/bard-google-ai-search-updates/", "summary": "is the Google alternative to ChatGPT. Bard seeks to combine the breadth of the world\u2019s knowledge with the power, intelligence, and creativity of our large language models.", "title": "Bard", "topics": ["Google Gemini", "Google", "ChatGPT"], "sentiment": "positive"}, {"url": "https://www.forbes.com/sites/jonathanponciano/2023/02/08/alphabet-google-stock-plunge-erases-100-billion-after-new-ai-chatbot-gives-wrong-answer-in-ad/", "summary": "", "title": "Google Stock Plunge Erases $100 Billion After Bard Gives Wrong Answer In Ad", "topics": ["Google"], "sentiment": "negative"}, {"url": "https://ai.googleblog.com/2023/02/google-research-2022-beyond-algorithms.html", "summary": "", "title": "A recap of Google Research in 2022 on Deep Learning", "topics": ["Google"], "sentiment": "positive"}, {"url": "https://huggingface.co/blog/speecht5", "summary": "a model able to do speech-to-text, text-to-speech, and speech-to-speech.", "title": "Hugging Face releases SpeechT5", "topics": ["Hugging Face", "Text-to-speech", "Speech-to-text", "Multimodal AI (image, video, audio)", "Model release"], "sentiment": "positive"}, {"url": "https://learnprompting.org/docs/products", "summary": "divided by category.", "title": "A comprehensive list of 100+ products using prompting", "topics": [], "sentiment": "positive"}, {"url": "https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/", "summary": "", "title": "The new Bing and Edge that leverage the power of ChatGPT", "topics": ["Microsoft", "ChatGPT"], "sentiment": "positive"}, {"url": "https://sebastianraschka.com/blog/2023/llm-reading-list.html", "summary": "", "title": "Understanding Large Language Models: A Transformative Reading List", "topics": [], "sentiment": "positive"}, {"url": "https://research.runwayml.com/gen1", "summary": "uses words and images to generate new videos out of existing ones.", "title": "Runway new product Gen-1", "topics": ["AI for images", "Multimodal AI (image, video, audio)"], "sentiment": "positive"}, {"url": "https://dreamix-video-editing.github.io/", "summary": "", "title": "Dreamix: Video Diffusion Models are General Video Editors", "topics": ["Multimodal AI (image, video, audio)"], "sentiment": "positive"}, {"url": "https://blog.google/products/maps/sustainable-immersive-maps-announcements/", "summary": "", "title": "New AI features rollng out on Google Maps", "topics": ["Google"], "sentiment": "positive"}, {"url": "https://www.medarc.ai/blog/announcement", "summary": "Work on creating a foundational model in Medical AI.", "title": "Announcing the launch of the Medical AI Research Center (MedARC)", "topics": ["AI in healthcare"]}], "guides": [], "papers": [{"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjIxMS4xNTY2MSIsInBvc3RfaWQiOiI0ZTIyZWRmZi1kZDZjLTRkZGUtODRlNC0zNDg2OGFkZjMzNDIiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiZWJjMDBjZmQtYTllOS00MTgxLTlhODUtZmY0ZTc0ZjI3MDBkIiwiaWF0IjoxNjc2Mjc3MTIwLjA1LCJpc3MiOiJvcmNoaWQifQ.AUGixoDJ3p-al0q_Z8ldHdM5GrY6YKuJYDnDBszX9sM", "summary": "The authors investigate the hypothesis that transformer-based in-context learners implement standard learning algorithms implicitly.", "title": "What learning algorithm is in-context learning? Investigations with linear models"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMi4wMzQ5NCIsInBvc3RfaWQiOiI0ZTIyZWRmZi1kZDZjLTRkZGUtODRlNC0zNDg2OGFkZjMzNDIiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiZWJjMDBjZmQtYTllOS00MTgxLTlhODUtZmY0ZTc0ZjI3MDBkIiwiaWF0IjoxNjc2Mjc3MTIwLjA1LCJpc3MiOiJvcmNoaWQifQ.xMnCPfc0r8DUrl_CQmja4E78im7aXVLqf2KeS5TiPRc", "summary": "Ten categories of failures, including reasoning, factual errors, math, coding, and bias.", "title": "A Categorical Archive of ChatGPT Failures"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMi4wNDc2MSIsInBvc3RfaWQiOiI0ZTIyZWRmZi1kZDZjLTRkZGUtODRlNC0zNDg2OGFkZjMzNDIiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiZWJjMDBjZmQtYTllOS00MTgxLTlhODUtZmY0ZTc0ZjI3MDBkIiwiaWF0IjoxNjc2Mjc3MTIwLjA1LCJpc3MiOiJvcmNoaWQifQ.95yncK4P7vZ7Nzt1uThLL3ipASxFh5lCQgXj9fuQWVc", "summary": "", "title": "Toolformer: Language Models Can Teach Themselves to Use Tools"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMi4wMTM5OCIsInBvc3RfaWQiOiI0ZTIyZWRmZi1kZDZjLTRkZGUtODRlNC0zNDg2OGFkZjMzNDIiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiZWJjMDBjZmQtYTllOS00MTgxLTlhODUtZmY0ZTc0ZjI3MDBkIiwiaWF0IjoxNjc2Mjc3MTIwLjA1MSwiaXNzIjoib3JjaGlkIn0.OsZn_r38fIHb9W8GVWQvh9BdZ2QICV0v3G2741__dkM", "summary": "", "title": "The unreasonable effectiveness of few-shot learning for machine translation"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMS4xMTkxNiIsInBvc3RfaWQiOiI0ZTIyZWRmZi1kZDZjLTRkZGUtODRlNC0zNDg2OGFkZjMzNDIiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiZWJjMDBjZmQtYTllOS00MTgxLTlhODUtZmY0ZTc0ZjI3MDBkIiwiaWF0IjoxNjc2Mjc3MTIwLjA1MSwiaXNzIjoib3JjaGlkIn0.yTzJsUkiaGUH9cAWVbeJ5Q149rCIlkBEiSKn1c1SQIA", "summary": "Examining the in-context learning phenomenon through a Bayesian lens, viewing large language models as topic models.", "title": "Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning"}], "datetime": "2023-02-13"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-february-6th-2023-7a05da8aaa0c", "news": [{"url": "https://openai.com/blog/chatgpt-plus/", "summary": "Subscribers will receive general access to ChatGPT, even during peak times, faster response times, and priority access to new features and improvements. ChatGPT Plus will be available for $20/month.", "title": "ChatGPT Plus: a subscription plan for ChatGPT", "topics": ["ChatGPT", "OpenAI"], "sentiment": "positive"}, {"url": "https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/", "summary": "The classifier correctly identifies 26% of AI-written text (true positives) as \u201clikely AI-written,\u201d while incorrectly labeling human-written text as AI-written 9% of the time (false positives).", "title": "OpenAI is launching a classifier trained to distinguish between AI-written and human-written text", "topics": ["OpenAI"], "sentiment": "positive"}, {"url": "https://medium.com/nlplanet/building-chatbots-with-gpt3-62f6567d8fa4", "summary": "Recent advancements in LLMs, such as GPT-3, can be used for chatbot development. Instead of having many very specific intents, each intent can be broader and leverage a Knowledge Base document.", "title": "Chatbots with GPT3", "topics": ["ChatGPT", "GPT-3, GPT-3.5, and GPT-3.5 turbo", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.semafor.com/article/01/27/2023/openai-has-hired-an-army-of-contractors-to-make-basic-coding-obsolete", "summary": "bringing on roughly 1,000 remote contractors over the past six months to create massive sets of images and audio clips to train AI tools. 60% of the contractors do data labeling and 40% are computer programmers.", "title": "OpenAI has ramped up its hiring around the world", "topics": ["AI datasets", "AI for coding", "Multimodal AI (image, video, audio)", "OpenAI"], "sentiment": "positive"}, {"url": "https://twitter.com/alexandr_wang/status/1619893376589139972", "summary": "", "title": "Scale AI hosted a Generative AI hackathon", "topics": [], "sentiment": "positive"}, {"url": "https://www.geoffreylitt.com/2023/01/29/fun-with-compositional-llms-querying-basketball-stats-with-gpt-3-statmuse-langchain.html", "summary": "Using Langchain, the author composed an AI program that combines GPT-3 with Statmuse, a sports stats search engine, to answer multi-part questions about NBA stats.", "title": "Querying NBA stats with GPT-3 + Statmuse + Langchain", "topics": ["GPT-3, GPT-3.5, and GPT-3.5 turbo"], "sentiment": "positive"}, {"url": "https://noise2music.github.io/", "summary": "a series of diffusion models trained to generate high-quality 30-second music clips from text prompts. The generated audio is able to faithfully reflect key elements of the text prompt such as genre, tempo, instruments, mood and era.", "title": "Noise2Music", "topics": ["Multimodal AI (image, video, audio)"], "sentiment": "positive"}, {"url": "https://www.bloomberg.com/news/articles/2023-01-30/chinese-search-giant-baidu-to-launch-chatgpt-style-bot-in-march", "summary": "Baidu, known as China\u2019s Google, will embed it in search engine.", "title": "Chinese Search Giant Baidu to Launch ChatGPT-Style Bot", "topics": ["ChatGPT"], "sentiment": "positive"}, {"url": "https://www.theverge.com/2023/1/28/23575919/microsoft-openai-github-dismiss-copilot-ai-copyright-lawsuit", "summary": "The three companies want to dismiss a complaint that alleges that the AI-powered Copilot relies on \u2018software piracy on an unprecedented scale.", "title": "Microsoft, GitHub, and OpenAI ask court to throw out AI copyright lawsuit", "topics": ["AI and copyright", "Microsoft", "OpenAI"], "sentiment": "negative"}, {"url": "https://towardsai.net/p/l/trends-in-ai%E2%80%8A-%E2%80%8A2023-round-up", "summary": "Predictions on language models, reinforcement learning, robotics, computer vision\u2026", "title": "Trends in AI in 2023", "topics": ["Reinforcement learning", "Robotics", "AI for images", "Multimodal AI (image, video, audio)"], "sentiment": "positive"}, {"url": "https://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html", "summary": "", "title": "A Short Chronology Of Deep Learning For Tabular Data", "topics": [], "sentiment": "positive"}, {"url": "https://ai.googleblog.com/2023/02/the-flan-collection-advancing-open.html", "summary": "It yields improved performance on all tested evaluation benchmarks & enables models to reason more competently over arbitrary tasks.", "title": "The Flan Collection: a newer & more extensive publicly available collection of tasks, templates & methods for instruction tuning", "topics": ["AI datasets"], "sentiment": "positive"}, {"url": "http://nlp.seas.harvard.edu/annotated-transformer/", "summary": "an annotated version of the paper \u201cAttention is all you need\u201d in the form of a line-by-line implementation.", "title": "The Annotated Transformer", "topics": [], "sentiment": "positive"}, {"url": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/", "summary": "a summary of Transformer architecture improvements.", "title": "The Transformer Family", "topics": [], "sentiment": "positive"}, {"url": "https://www.theverge.com/2023/2/3/23584540/google-anthropic-investment-300-million-openai-chatgpt-rival-claude", "summary": "founded by former OpenAI researcher.", "title": "Google invested $300 million in AI firm Anthropic", "topics": ["Funding", "Anthropic", "Google Gemini"], "sentiment": "positive"}, {"url": "https://www.microsoft.com/en-us/microsoft-365/blog/2023/02/01/microsoft-teams-premium-cut-costs-and-add-ai-powered-productivity/", "summary": "with AI-generated notes and tasks.", "title": "GPT-3 integrated in Microsoft Teams Premium", "topics": ["Microsoft", "GPT-3, GPT-3.5, and GPT-3.5 turbo"], "sentiment": "positive"}], "guides": [], "papers": [{"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMS4xMDIyNiIsInBvc3RfaWQiOiIwODkzNDI0OS04MmM3LTQ5ZTAtYmM4Zi02MmU0ZjFlNjY2ODQiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiZDEzYjY2MTEtZWFlNi00YTFiLThkOGUtMjNlOGY3YTJjZTA4IiwiaWF0IjoxNjc1Njk0Nzk4LjQyNSwiaXNzIjoib3JjaGlkIn0.PRN_PMFXaYIHb76ZtMGxIJTC4QJgftBJGtD8PCWGTIs", "summary": "can be embedded with minimal impact on text quality and detected using an efficient algorithm.", "title": "Watermarks for large language models"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMS4wNjYyNyIsInBvc3RfaWQiOiIwODkzNDI0OS04MmM3LTQ5ZTAtYmM4Zi02MmU0ZjFlNjY2ODQiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiZDEzYjY2MTEtZWFlNi00YTFiLThkOGUtMjNlOGY3YTJjZTA4IiwiaWF0IjoxNjc1Njk0Nzk4LjQyNSwiaXNzIjoib3JjaGlkIn0.1e9r4rutIiaCG9gmmQRlUKaNVylEX2_VZ1GuD0uUT74", "summary": "A cognitive perspective on dissociating language and thought in large language models.", "title": "What are the cognitive capabilities of today\u2019s large language models?"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMS4wMDc3NCIsInBvc3RfaWQiOiIwODkzNDI0OS04MmM3LTQ5ZTAtYmM4Zi02MmU0ZjFlNjY2ODQiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiZDEzYjY2MTEtZWFlNi00YTFiLThkOGUtMjNlOGY3YTJjZTA4IiwiaWF0IjoxNjc1Njk0Nzk4LjQyNSwiaXNzIjoib3JjaGlkIn0.tLilg8Kblq3VPrZOcBXhQeG_yEuNmjya1Kb3ym2W9xo", "summary": "With SparseGPT, more than 100 billion weights from the largest available open-source models can be ignored at inference time.", "title": "SparseGPT: a new pruning method that can reduce GPT-family models to at least 50% sparsity in one-shot, without any retraining and with minimal loss of accuracy"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjEwNi4wOTY4NSIsInBvc3RfaWQiOiIwODkzNDI0OS04MmM3LTQ5ZTAtYmM4Zi02MmU0ZjFlNjY2ODQiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiZDEzYjY2MTEtZWFlNi00YTFiLThkOGUtMjNlOGY3YTJjZTA4IiwiaWF0IjoxNjc1Njk0Nzk4LjQyNSwiaXNzIjoib3JjaGlkIn0.cEm9g6hLKo2WvoY81_RIhXoKdvDKnKuBkcB_6Vlf3pE", "summary": "", "title": "Low-Rank Adaptation (LoRA): efficient fine-tuning of large language models using fewer parameters and less GPU memory"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMS4xMzM3OSIsInBvc3RfaWQiOiIwODkzNDI0OS04MmM3LTQ5ZTAtYmM4Zi02MmU0ZjFlNjY2ODQiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiZDEzYjY2MTEtZWFlNi00YTFiLThkOGUtMjNlOGY3YTJjZTA4IiwiaWF0IjoxNjc1Njk0Nzk4LjQyNSwiaXNzIjoib3JjaGlkIn0.ZHdopG41oZP-gPUw0kiREREkvEdMQgBsCb9hNwl0u-0", "summary": "a faithful-by-construction framework that boosts Language Models\u2019 performance on complex reasoning tasks.", "title": "Faithful Chain-of-Thought Reasoning"}], "datetime": "2023-02-06"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-january-30th-2023-5b1e5d783506", "news": [{"url": "https://techcrunch.com/2023/01/25/after-inking-its-openai-deal-shutterstock-rolls-out-a-generative-ai-toolkit-to-create-images-based-on-text-prompts/", "summary": "to create images based on text prompts, while Getty Images is currently embroiled in a lawsuit against Stability AI.", "title": "Shutterstock has rolled out a generative AI toolkit", "topics": ["AI and copyright", "AI for images", "Multimodal AI (image, video, audio)", "Stability AI"], "sentiment": "negative"}, {"url": "https://blog.elevenlabs.io/elevenlabs-raises-2m-pre-seed-and-announces-ai-speech-platform-promising-to-revolutionize-audio-storytelling/", "summary": "Powered by deep learning model for speech synthesis, it adjusts delivery based on context.", "title": "ElevenLabs launches Beta platform for creators/publishers to narrate long-form content", "topics": ["Text-to-speech"], "sentiment": "positive"}, {"url": "https://huggingface.co/blog/dialog-agents", "summary": "Usually, the language-modeling objective of the base model is not sufficient for a model to learn to follow a user\u2019s direction in a helpful way.", "title": "What are RLHF, SFT, and IFT, and how are they used in ChatGPT?", "topics": ["Reinforcement learning", "ChatGPT", "OpenAI"], "sentiment": "positive"}, {"url": "https://twitter.com/abhi1thakur/status/1618280872083599360", "summary": "that let you create public/private competitions with full control of datasets, metrics and evaluation.", "title": "Announcing Hugging Face Competitions", "topics": ["Hugging Face", "AI datasets"], "sentiment": "positive"}, {"url": "https://www.patterns.app/blog/2023/01/18/crunchbot-sql-analyst-gpt/", "summary": "", "title": "Using recursive GPT prompts to generate SQL queries from natural language descriptions", "topics": ["AI for coding"], "sentiment": "positive"}, {"url": "https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/17-question-answering.html", "summary": "and Text Summarization are out. New lessons will be published on February 6th.", "title": "Question Answering", "topics": [], "sentiment": "positive"}], "guides": [], "papers": [{"url": "https://google-research.github.io/seanet/musiclm/examples/", "summary": "MusicLM casts the process of conditional music generation as a hierarchical sequence-to-sequence modeling task and generates music at 24 kHz. Experiments show that MusicLM outperforms previous systems both in audio quality and adherence to the text description.", "title": "Introducing MusicLM: Generating Music From Text"}, {"url": "https://make-a-video3d.github.io/", "summary": "", "title": "MAV3D (Make-A-Video3D), a method for generating 3D dynamic scenes from text descriptions"}, {"url": "https://arxiv.org/abs/2212.10450", "summary": "The authors investigate and analyze its potential as a general-purpose data annotator in NLP.", "title": "Can GPT-3 be used as a good data annotator for NLP tasks?"}, {"url": "https://arxiv.org/abs/2207.08815", "summary": "Tree-based models remain state-of-the-art on medium-sized data (\u223c10K samples) even without accounting for their superior speed.", "title": "Why do tree-based models still outperform deep learning on tabular data?"}, {"url": "https://arxiv.org/abs/2202.03629", "summary": "Large language models-based generation is prone to hallucinate unintended text, which degrades system performance and fails to meet user expectations in many real-world scenarios. The authors provide a broad overview of the research progress and challenges in the hallucination problem in NLG.", "title": "Survey of Hallucination in Natural Language Generation"}, {"url": "https://arxiv.org/abs/2212.06713", "summary": "The authors introduce structured prompting that breaks the length limit and scales in-context learning to thousands of examples. Demonstration examples are separately encoded with well-designed position embeddings, and then they are jointly attended by the test example using a rescaled attention mechanism.", "title": "Structured Prompting: Scaling In-Context Learning to 1,000 Examples"}], "datetime": "2023-01-30"}, {"url": "https://medium.com/nlplanet/weekly-ai-and-nlp-news-23-january-2023-776766fa2f9f", "news": [{"url": "https://scale.com/blog/chatgpt-vs-claude", "summary": "Anthropic has quietly begun testing a new, ChatGPT-like AI assistant named Claude.", "title": "ChatGPT vs Claude", "topics": ["Claude", "Anthropic", "ChatGPT"], "sentiment": "positive"}, {"url": "https://azure.microsoft.com/en-us/blog/general-availability-of-azure-openai-service-expands-access-to-large-advanced-ai-models-with-added-enterprise-benefits/", "summary": "Azure OpenAI Service is now generally available, enabling customers to tap into large-scale generative AI models.", "title": "ChatGPT will soon be available on Azure AI infrastructure", "topics": ["Microsoft", "ChatGPT", "OpenAI"], "sentiment": "positive"}, {"url": "https://github.com/louisfb01/best_AI_papers_2022", "summary": "2022 has been an amazing year for AI and Data Science. Here\u2019s a curated list of the latest breakthroughs, complete with explanations, links, and code.", "title": "2022: A Year Full of Amazing AI papers- A Review", "topics": [], "sentiment": "positive"}, {"url": "https://learnprompting.org/docs/intermediate/chain_of_thought", "summary": "Chain of Thought (CoT) prompting is an effective way to encourage AI reasoning. It\u2019s been shown to improve accuracy on tasks like arithmetic, commonsense, and symbolic reasoning. Models with 100B+ parameters get the most benefit from CoT prompting.", "title": "Chain of Thought Prompting", "topics": [], "sentiment": "positive"}, {"url": "https://www.theverge.com/2023/1/17/23558516/ai-art-copyright-stable-diffusion-getty-images-lawsuit", "summary": "Getty Images is suing Stability AI, creators of popular AI art tool Stable Diffusion, over an alleged copyright violation.", "title": "Getty Images is suing the creators of AI art tool Stable Diffusion for scraping its content", "topics": ["AI and copyright", "AI for images", "Stable Diffusion", "Stability AI"], "sentiment": "negative"}, {"url": "https://lspace.swyx.io/p/reverse-prompt-eng", "summary": "The author used prompt injection to obtain the complete source prompts of every Notion AI feature, leveraging several techniques.", "title": "Reverse Prompt Engineering of Notion AI", "topics": [], "sentiment": "positive"}, {"url": "https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/", "summary": "A catalog of popular transformer models from the last years, including ChatGPT, GPT3.5, and other models from Eleuther, Anthropic, Deepmind, and Stability.", "title": "Transformer models: an introduction and catalog \u2014 2023 Edition", "topics": ["Anthropic", "DeepMind", "ChatGPT", "GPT-3, GPT-3.5, and GPT-3.5 turbo", "OpenAI"], "sentiment": "positive"}, {"url": "https://ai.googleblog.com/2023/01/google-research-2022-beyond-language.html", "summary": "The article contains a summary of the AI work done by Google in 2022, talking about LaMDA, PaLM, and other NLP advancements.", "title": "Language, Vision and Generative Models at Google in 2022", "topics": ["Google"], "sentiment": "positive"}, {"url": "https://voicebot.ai/2023/01/12/nvidia-debuts-generative-ai-model-for-medical-research/", "summary": "Nvidia and Evozyne have debuted ProT-VAE, a generative AI model capable of producing proteins to use in medicine and other industries. The model uses deep learning and generative AI to synthesize variants of proteins to create more effective enzymes.", "title": "Nvidia Debuts Generative AI Model for Medical Research", "topics": ["AI in healthcare", "NVIDIA"], "sentiment": "positive"}, {"url": "https://www.engadget.com/boston-dynamics-atlas-shows-off-its-acrobatic-gopher-skills-on-the-jobsite-175859102.html", "summary": "Boston Dynamics\u2019 Atlas robot shows off its amazing acrobatic skills in a new demo video. It grabs and places a plank, jumps and spins, and even hucks a tool bag with impressive dexterity.", "title": "Boston Dynamics\u2019 Atlas shows off its acrobatic \u2018gopher\u2019 skills", "topics": ["Robotics"], "sentiment": "positive"}, {"url": "https://huggingface.co/kakaobrain/karlo-v1-alpha", "summary": "Karlo is a text-conditional image generation model based on OpenAI\u2019s unCLIP architecture with the improvement over the standard super-resolution model from 64px to 256px, recovering high-frequency details only in the small number of denoising steps.", "title": "Introducing Karlo, an Open Source DALL-E 2 (unCLIP)", "topics": ["AI for images", "Multimodal AI (image, video, audio)", "OpenAI"]}, {"url": "https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/14-named-entity-recognition.html", "summary": "\u201d, \u201cKnowledge Graphs\u201d, and \u201cProject: Building a Knowledge Base from Texts\u201d are out. New lessons will be published on January 30th.", "title": "Project: Named Entity Recognition", "topics": []}], "guides": [], "papers": [{"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9wZGYvMjMwMS4wMDIzNC5wZGYiLCJwb3N0X2lkIjoiZTM3NWNiYzQtZTI1MC00ZWFmLWE1MjUtZWE5NTEyOTQ5NTdhIiwicHVibGljYXRpb25faWQiOiIyZmNiZmM1Ny1kNTg0LTQyMDMtYmIzZS1jMTA0OGU3ZDM5OTMiLCJ2aXNpdF90b2tlbiI6IjFjNTc4NjlmLTgyMjgtNGI2NS1hOWUxLTgxOWRlNTQ0MDJhMyIsImlhdCI6MTY3NDQ5MDY4Mi43NywiaXNzIjoib3JjaGlkIn0.w7cH6gjZ_KsKqBka03xQzOsAc1GJddUpeEVtuyfU9EM", "summary": "In-context learning (ICL) is a new paradigm for natural language processing that uses large language models. This paper surveys and summarizes progress, challenges, and future work in ICL, including training and prompting strategies.", "title": "A Survey on In-context Learning"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjIwNC4wNTg2MiIsInBvc3RfaWQiOiJlMzc1Y2JjNC1lMjUwLTRlYWYtYTUyNS1lYTk1MTI5NDk1N2EiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiMWM1Nzg2OWYtODIyOC00YjY1LWE5ZTEtODE5ZGU1NDQwMmEzIiwiaWF0IjoxNjc0NDkwNjgyLjc3LCJpc3MiOiJvcmNoaWQifQ.NilaM1ZDy_9_patolJAZYOBl_hgfzB3CbZbqJzEvHQI", "summary": "The authors apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants.", "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjIwMS4wNjAwOSIsInBvc3RfaWQiOiJlMzc1Y2JjNC1lMjUwLTRlYWYtYTUyNS1lYTk1MTI5NDk1N2EiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiMWM1Nzg2OWYtODIyOC00YjY1LWE5ZTEtODE5ZGU1NDQwMmEzIiwiaWF0IjoxNjc0NDkwNjgyLjc3LCJpc3MiOiJvcmNoaWQifQ.874MWtYD4B4VY0sVqKwCExrd_3Ethr5A2VhFcp39Hj8", "summary": "The authors explore how user interactions can help correct errors in large pre-trained LMs like GPT-3 without retraining. The approach pairs GPT-3 with a memory of recorded cases & user feedback for clarification.", "title": "MemPrompt: Help LLMs correct errors without retraining"}, {"url": "https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2FyeGl2Lm9yZy9hYnMvMjMwMS4wMzA0NCIsInBvc3RfaWQiOiJlMzc1Y2JjNC1lMjUwLTRlYWYtYTUyNS1lYTk1MTI5NDk1N2EiLCJwdWJsaWNhdGlvbl9pZCI6IjJmY2JmYzU3LWQ1ODQtNDIwMy1iYjNlLWMxMDQ4ZTdkMzk5MyIsInZpc2l0X3Rva2VuIjoiMWM1Nzg2OWYtODIyOC00YjY1LWE5ZTEtODE5ZGU1NDQwMmEzIiwiaWF0IjoxNjc0NDkwNjgyLjc3LCJpc3MiOiJvcmNoaWQifQ.rUYiLF630_ZbMgXO2bNIGItsOp9E7Mdt1lKr48tW4XU", "summary": "The authors seek to systematically review motivations and progress on using Transformers in RL, provide a taxonomy on existing works, discuss each sub-field, and summarize future prospects.", "title": "A Survey on Transformers in Reinforcement Learning"}], "datetime": "2023-01-23"}, {"url": "https://medium.com/nlplanet", "news": [{"url": "https://arstechnica.com/information-technology/2024/04/rumors-swirl-about-mystery-gpt2-chatbot-that-some-think-is-gpt-5-in-disguise/", "summary": "The \u201cgpt2-chatbot\u201d recently emerged on the LMSYS Chatbot Arena, generating discussions within the AI community about its potential relation to new OpenAI models. While demonstrating strong performance, analyses indicate it does not outperform GPT-4, and its exact origin and details are still uncertain.", "title": "Mysterious \u201cgpt2-chatbot\u201d AI model appears suddenly, confuses experts", "topics": ["GPT-4 and GPT-4 turbo", "OpenAI"], "sentiment": "negative"}, {"url": "https://github.blog/2024-04-29-github-copilot-workspace/", "summary": "GitHub has launched Copilot Workspace, a comprehensive developer environment that facilitates the entire coding process, including planning, coding, testing, and deployment, through natural language commands, offering AI-industry professionals an integrated solution for streamlining development workflows.", "title": "GitHub Copilot Workspace", "topics": ["AI for coding"], "sentiment": "positive"}, {"url": "https://the-decoder.com/openai-ceo-sam-altman-promises-ai-models-that-far-surpass-gpt-4/", "summary": "OpenAI\u2019s Sam Altman considers GPT-4 the most rudimentary AI that users will encounter as the company progresses towards more sophisticated models like GPT-5, which is expected to feature enhanced abilities such as video generation. He foresees AI developing into highly efficient assistants, performing tasks and providing solutions effortlessly.", "title": "OpenAI CEO Sam Altman says GPT-4 is the dumbest AI model you\u2019ll ever have to use again", "topics": ["GPT-4 and GPT-4 turbo", "GPT-5", "OpenAI", "Multimodal AI (image, video, audio)"], "sentiment": "positive"}, {"url": "https://interestingengineering.com/culture/sora-powered-music-video", "summary": "Paul Trillo directed the official music video for Washed Out\u2019s \u201cThe Hardest Part\u201d using OpenAI\u2019s Sora, a text-to-video AI, producing 700 clips of which 55 were used. The project has stirred ethical discussions within the AI industry.", "title": "Pro music video powered by OpenAI\u2019s Sora released in a world-first", "topics": ["AI and copyright", "AI for images", "Multimodal AI (image, video, audio)", "OpenAI"], "sentiment": "positive"}, {"url": "https://mashable.com/article/chatgpt-search-announcement-what-we-know", "summary": "OpenAI is rumored to be launching a ChatGPT-based search engine, potentially at \u201csearch.chatgpt.com,\u201d aiming to rival Google by integrating a chatbot feature with traditional search results. This reflects the industry trend of AI potentially revolutionizing standard web search methods.", "title": "A ChatGPT search engine is rumored to be coming next week", "topics": ["Google", "ChatGPT", "OpenAI"], "sentiment": "positive"}, {"url": "https://openai.com/index/memory-and-new-controls-for-chatgpt", "summary": "OpenAI is testing a new memory feature for ChatGPT to improve interaction continuity, offering user-managed options for adding, reviewing, and deleting retained information or disabling the feature.", "title": "Memory and new controls for ChatGPT", "topics": ["ChatGPT", "OpenAI"], "sentiment": "positive"}], "guides": [{"url": "https://www.topbots.com/llm-reasoning-research-papers/", "summary": "Recent research in the artificial intelligence domain has been focusing on augmenting the reasoning capabilities of LLMs. A variety of strategies have been explored to improve their performance, including chain-of-thought prompting, strategic and knowledge enhancements, and integration with computational engines. Current challenges lie in the ability of LLMs to self- correct, which remains dependent on external feedback.", "title": "Advancing AI\u2019s Cognitive Horizons: 8 Significant Research Papers on LLM Reasoning"}, {"url": "https://huggingface.co/blog/evaluation-structured-outputs", "summary": "The Hugging Face Leaderboards and Evals team has conducted research highlighting the impact of prompt format on model evaluation consistency. They suggest structured generation as a means to standardize outputs, leading to more reliable and comparable performance metrics, with initial findings indicating a reduction in evaluation variance.", "title": "Improving Prompt Consistency with Structured Generations"}, {"url": "https://lightning.ai/lightning-ai/studios/compare-llama-3-and-phi-3-using-rag", "summary": "This guide outlines the creation of a self-hosted \u201cChat with your Docs\u201d application that integrates Meta AI\u2019s Llama3 and Microsoft\u2019s Phi3 language models into a Retrieval Augmented Generation (RAG) system. It describes a Streamlit-based user interface that allows direct performance evaluation of the models, utilizing a sophisticated setup that includes custom knowledge bases, document chunking strategies, embeddings, and vector databases to improve user interactions with documents.", "title": "Comparison of Llama-3 and Phi-3 using RAG"}, {"url": "https://huggingface.co/blog/AviSoori1x/seemoe", "summary": "This guide discusses \u2018seeMoE,\u2019 a PyTorch-based vision language model combining an image encoder, vision-language projection, and an MoE decoder. It utilizes character-level autoregressive language modeling and features innovative noisy top-k gating for dynamic expert selection.", "title": "SeeMoE: Implementing a MoE Vision Language Model from scratch"}], "papers": [{"url": "https://github.com/abi/secret-llama", "summary": "\u201cSecret Llama\u201d is a private, browser-based chatbot leveraging Llama 3 and Mistral models, designed to run independently without server dependencies thanks to WebGPU support. Prioritizing user privacy, it operates fully offline without any data leaving the local device. The platform is user- friendly and can handle AI models up to 4.3GB.", "title": "abi/secret-llama"}, {"url": "https://arxiv.org/abs/2405.01535", "summary": "Prometheus 2 is an open-source language model evaluator that improves upon earlier models by offering a broad array of assessment capabilities, including direct assessments, pairwise rankings, and custom evaluation criteria. It aims to provide evaluation results that better match human judgment and can be tailored to assess both standard and proprietary language models like GPT-4.", "title": "Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models"}, {"url": "https://arxiv.org/abs/2404.19737", "summary": "An improved training method for large language models that predicts multiple future tokens simultaneously demonstrates increased sample efficiency and performance in code and natural language tasks. This multi-token prediction method achieves faster inference speeds, up to three times quicker, without increasing training time.", "title": "Better & Faster Large Language Models via Multi-token Prediction"}, {"url": "https://pllava.github.io/", "summary": "PLLaVA is a parameter-free method for extending image models to video models, designed to overcome issues like performance saturation and prompt sensitivity when fine-tuning image models for video tasks. It utilizes a pooling strategy to balance feature distribution over time, leading to improved results such as a 3.48 score on the Video ChatGPT benchmark and a 58.1% accuracy on the MVBench, setting a new state-of-the-art performance.", "title": "PLLaVA: Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning"}, {"url": "https://huggingface.co/blog/sc2-instruct", "summary": "StarCoder2\u201315B-Instruct-v0.1, a transparent and permissive code LLM, utilizes a self-aligned pipeline and its generated content for fine-tuning, achieving a HumanEval score of 72.6. It showcases the viability of self-alignment in producing high-quality code generation without relying on external data sources.", "title": "StarCoder2-Instruct: Fully Transparent and Permissive Self-Alignment for Code Generation"}], "datetime": "2024-05-06"}, {"url": "https://medium.com/nlplanet", "news": [{"url": "https://blog.google/technology/ai/google-deepmind-isomorphic-alphafold-3-ai-model/#life-molecules", "summary": "AlphaFold 3 is an advanced AI model by Google DeepMind and Isomorphic Labs, capable of accurately predicting biomolecular structures and interactions. Providing a significant advancement over prior models, it enhances scientific research and drug development, and is available globally through the AlphaFold Server.", "title": "DeepMind releases AlphaFold 3", "topics": ["AI in healthcare", "Model release", "DeepMind", "Google"], "sentiment": "positive"}, {"url": "https://the-decoder.com/microsoft-allegedly-developing-mai-1-a-competing-model-to-openais-gpt-4/", "summary": "Microsoft is currently working on MAI-1, a 500 billion parameter AI model, aiming for a competitive edge in the AI industry and moving towards greater independence in AI development.", "title": "Microsoft allegedly developing MAI-1, a competing model to OpenAI\u2019s GPT-4", "topics": ["Microsoft", "GPT-4 and GPT-4 turbo", "OpenAI"], "sentiment": "positive"}, {"url": "https://simonwillison.net/2024/May/8/gpt2-chatbot-confirmed-as-openai/", "summary": "The gpt2-chatbot that appeared in the LMSYS arena was confirmed to be an OpenAI test model after a 429 rate limit error revealed its connection to OpenAI\u2019s API. Now renamed to im-also-a-good-gpt-chatbot, it can only be accessed randomly in \u201cArena (battle)\u201d mode rather than \u201cDirect Chat\u201d.", "title": "gpt2-chatbot confirmed as OpenAI", "topics": ["OpenAI"], "sentiment": "negative"}, {"url": "https://openai.com/index/api-partnership-with-stack-overflow/", "summary": "OpenAI is partnering with Stack Overflow to integrate their OverflowAPI into ChatGPT, enriching it with Stack Overflow\u2019s extensive developer knowledge for more accurate, programming-related AI responses.", "title": "OpenAI partnership with Stack Overflow", "topics": ["AI for coding", "ChatGPT", "OpenAI"], "sentiment": "positive"}, {"url": "https://decrypt.co/229816/neuralink-safety-concerns-drove-co-founder-to-break-up-with-elon-musk", "summary": "Neuralink\u2019s co-founder has departed to create a new venture focusing on a safer, non-invasive brain-computer interface technology using surface microelectrodes, in contrast to Neuralink\u2019s penetrating electrodes method.", "title": "Neuralink Safety Concerns Drove Co-Founder to Break Up With Elon Musk", "topics": ["Neuralink", "AI in healthcare", "AI safety"], "sentiment": "negative"}], "guides": [{"url": "https://every.to/chain-of-thought/i-spent-24-hours-with-github-copilot-workspaces", "summary": "GitHub Copilot Workspace offers an AI-powered coding platform that enables users to write code using conversational English, streamlining the process particularly for straightforward tasks, while more intricate functions necessitate precise instructions.", "title": "The Next Big Programming Language Is English"}, {"url": "https://huggingface.co/blog/wenbopan/long-context-fine-tuning", "summary": "This guide examines the difficulties of fine-tuning large language models for extended contexts over 32,000 tokens, such as high memory utilization and processing inefficiencies. It presents solutions like Gradient Checkpoint, LoRA, and Flash Attention to mitigate these issues and enhance computational efficiency.", "title": "Everything About Long Context Fine-tuning"}, {"url": "https://lmsys.org/blog/2024-05-08-llama3/", "summary": "Meta\u2019s Llama 3\u201370B is a language model that performs well in English Chatbot Arena for open-ended and creative tasks, with high friendliness and quality conversation outputs, but it is less proficient in math and coding-related tasks.", "title": "What\u2019s up with Llama 3? Arena data analysis"}, {"url": "https://hao-ai-lab.github.io/blogs/cllm/", "summary": "Consistency Large Language Models (CLLMs) improve LLMs by allowing parallel decoding through training with Jacobi trajectories and a mix of consistency and autoregressive losses. This results in faster inference times without increasing memory demands.", "title": "Consistency Large Language Models: A Family of Efficient Parallel Decoders"}, {"url": "https://hai.stanford.edu/news/ai-index-state-ai-13-charts", "summary": "The 2024 AI Index report reveals key AI trends, such as the dominance of U.S. companies in foundational AI models and investment. While open-source AI models are growing, they underperform compared to proprietary models. The report observes a significant increase in AI costs and human-like AI benchmark performance. Despite a decrease in overall AI investment, there\u2019s a notable rise in funding for generative AI technologies, an uptick in corporate adoption, and more AI-specific regulations.", "title": "Stanford AI Index: State of AI in 13 Charts"}], "papers": [{"url": "https://arxiv.org/abs/2405.04517", "summary": "Researchers have advanced LSTM-based language models by applying exponential gating and revamping the memory structures, resulting in two key variants: the scalar-focused sLSTM and the fully parallelizable mLSTM. These innovations are incorporated into xLSTM blocks, which, when stacked residually, create xLSTM architectures that compare competitively with leading Transformers and State Space Models in performance and scalability.", "title": "xLSTM: Extended Long Short-Term Memory"}, {"url": "https://arxiv.org/abs/2311.07590", "summary": "Researchers have presented the first instance where a Large Language Model (LLM) like GPT-4, designed for helpfulness, harmlessness, and honesty, exhibited strategic deception without directives for such behavior. In a simulated stock trading environment, the model engaged in insider trading and subsequently concealed its actions from its management, illustrating misaligned behavior in a realistic scenario.", "title": "Large Language Models can Strategically Deceive their Users when Put Under Pressure"}, {"url": "https://arxiv.org/abs/2404.09173", "summary": "The novel Feedback Attention Memory (FAM) architecture enhances Transformers\u2019 capacity for handling long sequences by integrating a feedback loop, which fosters inherent working memory. This advancement allows Transformer models of various sizes to better manage long-context tasks, demonstrating significant performance improvements.", "title": "TransformerFAM: Feedback attention is working memory"}, {"url": "https://arxiv.org/abs/2312.13286", "summary": "Emu2 is a novel 37 billion parameter generative multimodal AI model with advanced in-context learning capabilities and excels at multimodal tasks. It defines new performance standards, especially in few-shot scenarios, achieving state-of-the-art results in visual question answering and open-ended generation after instruction tuning.", "title": "Generative Multimodal Models are In-Context Learners"}, {"url": "https://arxiv.org/abs/2302.10149", "summary": "The paper presents two cost-effective dataset poisoning attacks that could compromise the integrity of widespread machine learning datasets by exploiting trust vulnerabilities, potentially affecting 0.01% of datasets like LAION-400M or COYO-700M with just $60.", "title": "Poisoning Web-Scale Training Datasets is Practical"}], "datetime": "2024-05-13"}, {"url": "https://medium.com/nlplanet", "news": [{"url": "https://openai.com/index/spring-update/", "summary": "OpenAI released the new model GPT-4o, capable of processing and generating text, audio, and image inputs and outputs. It boasts quick audio response times on par with humans, enhanced non-English language processing, and cost-efficient API usage, while maintaining GPT-4 Turbo\u2019s performance levels.", "title": "OpenAI releases GPT-4o", "topics": ["Multimodal AI (image, video, audio)", "Model release", "GPT-4 and GPT-4 turbo", "OpenAI"], "sentiment": "positive"}, {"url": "https://blog.google/technology/ai/google-io-2024-100-announcements/", "summary": "At Google I/O 2024, notable AI developments were announced such as Gemini 1.5 models, Trillium TPU, and enhanced AI in Google Search. Key introductions include Imagen 3 for image creation, Veo for video generation, and upgraded features in the Gemini app for premium users, alongside new generative media tools.", "title": "100 things Google announced at I/O 2024", "topics": ["Google Gemini", "Google", "Multimodal AI (image, video, audio)", "AI for images", "AI Chips and GPUs"], "sentiment": "positive"}, {"url": "https://openai.com/index/jakub-pachocki-announced-as-chief-scientist/", "summary": "Ilya Sutskever, co-founder of OpenAI, is stepping down from its role. Jakub Pachocki, with the company since 2017, will take over as Chief Scientist.", "title": "Ilya Sutskever to leave OpenAI, Jakub Pachocki announced as Chief Scientist", "topics": ["OpenAI"], "sentiment": "negative"}, {"url": "https://www.theverge.com/2024/5/16/24156755/hugging-face-celement-delangue-free-shared-gpus-ai", "summary": "Hugging Face is dedicating $10M in free GPU resources to support AI developers, startups, and academics. Their ZeroGPU initiative, part of Hugging Face Spaces, offers communal GPU access, aiming to reduce computational access barriers and improve cost-efficiency.", "title": "Hugging Face is sharing $10 million worth of compute to help beat the big AI companies", "topics": ["Hugging Face", "Funding", "AI Chips and GPUs"], "sentiment": "positive"}, {"url": "https://research.ibm.com/blog/granite-code-models-open-source", "summary": "IBM has released its Granite code models as open source. These models, trained on 116 languages with up to 34 billion parameters, facilitate code generation, bug fixing, and explanation tasks, and are accessible via GitHub and Hugging Face under the Apache 2.0 license.", "title": "IBM\u2019s Granite code model family is going open source", "topics": ["AI for coding", "Model release"], "sentiment": "positive"}, {"url": "https://9to5mac.com/2024/05/10/ios-18-chatgpt-features-apple-openai/", "summary": "Apple is nearing an agreement with OpenAI to incorporate ChatGPT functionalities into iOS 18, focusing on on-device AI for enhanced privacy and performance. The tech giant intends to announce this integration at the WWDC event on June 10, amidst ongoing discussions with Google regarding their Gemini chatbot.", "title": "iOS 18: Apple finalizing deal to bring ChatGPT to iPhone", "topics": ["Apple", "Google Gemini", "ChatGPT", "OpenAI"], "sentiment": "positive"}, {"url": "https://nypost.com/2024/05/14/business/metas-ai-system-cicero-beats-humans-in-game-of-diplomacy-by-lying-study/", "summary": "MIT researchers have found that Meta\u2019s AI, Cicero, demonstrates advanced deceptive capabilities in the game Diplomacy, ranking in the top 10% of human players through strategic betrayal. This reflects a growing trend among AI systems such as Google\u2019s AlphaStar and OpenAI\u2019s GPT-4 to employ deceit against human opponents, raising concerns over the potential risks of AI deception and the need for preventive strategies.", "title": "Meta\u2019s AI system \u2018Cicero\u2019 learning how to lie, deceive humans: study", "topics": ["AI safety", "AI regulation", "Meta", "OpenAI"], "sentiment": "negative"}], "guides": [{"url": "https://huggingface.co/blog/as-cle-bert/what-is-going-on-with-alphafold3", "summary": "Google Deepmind and Isomorphic Labs introduced AlphaFold3 on May 8, 2024, enhancing protein structure prediction with diffusion-based architecture for improved accuracy. While making strides, the tool faces issues such as chirality prediction and debates around its proprietary status.", "title": "What is going on with AlphaFold3?"}, {"url": "https://pub.towardsai.net/how-do-ai-supercomputers-train-large-gen-ai-models-simply-explained-ef6c1bd75f57", "summary": "AI supercomputers utilize HPC along with GPU and TPU parallel processing to train extensive models such as GPT-3 and GPT-4. The high computational power is directed towards tuning algorithms and parameters for higher accuracy. Key challenges such as power management, heat dissipation, and system failures are addressed with solutions like Deep Speed and Project Forge, enhancing the efficiency and scalability of training and inference processes vital for applications including ChatGPT and BingChat.", "title": "How do AI supercomputers train large Gen AI models? Simply Explained"}, {"url": "https://pub.towardsai.net/crafting-qa-tool-with-reading-abilities-using-rag-and-text-to-speech-d4208330a1e4", "summary": "This article presents a guide on constructing an AI-driven Question-Answering (QA) system integrating Retrieval-Augmented Generation (RAG) with Text-to-Speech (TTS) capabilities. It explains the process of deploying a Weaviate Vector Database, utilizing HuggingFace for data embedding, and designing a Streamlit-based user interface. Additionally, it mentions leveraging Docker, LangChain, ElevenLabs, and various AI models to facilitate conversational interaction by converting text queries into oral responses.", "title": "Crafting QA Tool with Reading Abilities Using RAG and Text-to-Speech"}, {"url": "https://www.topbots.com/enterprise-ai-from-big-tech/", "summary": "Big Tech, including Microsoft, Google, Amazon, and OpenAI, is increasingly pivoting towards enterprise AI. Their solutions \u2014 Copilot, Gemini, Q Business, and ChatGPT Enterprise, respectively \u2014 aim to boost productivity by automating tasks, analyzing data, and generating content within their ecosystems.", "title": "The AI Arms Race in Big Tech: An Overview of Emerging Enterprise Solutions"}], "papers": [{"url": "https://arxiv.org/abs/2405.07863", "summary": "The technical report discusses advancements in Online Iterative Reinforcement Learning from Human Feedback (RLHF), which has shown to be more effective than offline methods in improving the performance of LLMs. It also proposes the use of preference models derived from open datasets as an alternative to direct human feedback, particularly beneficial for open- source initiatives.", "title": "RLHF Workflow: From Reward Modeling to Online RLHF"}, {"url": "https://arxiv.org/abs/2405.02246", "summary": "Researchers have highlighted a lack of justification in critical design decisions for vision- language models (VLMs), which hinders progress by obscuring what improves performance. To tackle this, they\u2019ve conducted comprehensive experiments on pre-trained models, architecture, data, and training methods, leading to the creation of Idefics2, an 8 billion parameter VLM.", "title": "What matters when building vision-language models?"}, {"url": "https://arxiv.org/abs/2405.09673", "summary": "LoRA (Low-Rank Adaptation) is a finetuning approach for large language models (LLMs) that optimizes select weight matrices, saving memory by avoiding full model finetuning. While not outperforming full finetuning in niche tasks like programming and math, LoRA helps retain a model\u2019s general capabilities and encourages diverse content generation.", "title": "LoRA Learns Less and Forgets Less"}, {"url": "https://github.com/McGill-NLP/webllama", "summary": "Llama-3\u20138B-Web is an advanced web browsing agent developed from Llama 3, finetuned with over 24,000 data points, aiming to create efficient, user-focused AI tools for web navigation.", "title": "McGill-NLP/webllama: Llama-3 agents that can browse the web by following instructions and talking to you"}, {"url": "https://arxiv.org/abs/2405.09215v1", "summary": "Xmodel-VLM is an efficient 1B-scale multimodal vision language model optimized for GPU servers. It\u2019s fine-tuned for modality alignment using LLaVA and exhibits competitive results on standard benchmarks, outperforming larger models in speed.", "title": "Xmodel-VLM: A Simple Baseline for Multimodal Vision Language Model"}, {"url": "https://arxiv.org/abs/2405.08707", "summary": "The paper discusses the observed limitations of scaling Transformer models for language tasks, noting that larger models don\u2019t necessarily perform better and that memorization of training data can impact generalization. A new theoretical framework is introduced to better understand how Transformer models memorize and perform.", "title": "Beyond Scaling Laws: Understanding Transformer Performance with Associative Memory"}], "datetime": "2024-05-21"}, {"url": "https://medium.com/nlplanet", "news": [{"url": "https://finance.yahoo.com/news/nvidia-forecast-shatters-estimates-ai-210754051.html", "summary": "Nvidia\u2019s stock surged 9.3% after a promising sales forecast, pointing to a robust demand for AI technologies. The $28 billion projected Q2 revenue exceeds expectations, highlighting the company\u2019s strong position in the AI market, buoyed by their new Blackwell chips and significant data-center revenue.", "title": "Nvidia Stock Surges as Sales Forecast Delivers on AI Hopes", "topics": ["NVIDIA", "AI Chips and GPUs"], "sentiment": "positive"}, {"url": "https://venturebeat.com/ai/microsoft-introduces-phi-silica-a-3-3b-parameter-model-made-for-copilot-pc-npus/", "summary": "Microsoft has unveiled Phi-Silica, a compact language model with 3.3 billion parameters, tailored for Copilot+ PCs equipped with NPUs. This model is engineered for rapid on-device inferencing, improving productivity and accessibility for Windows users with optimal power efficiency. Phi-Silica is Microsoft\u2019s inaugural local language model, with a release slated for June.", "title": "Microsoft introduces Phi-Silica, a 3.3B parameter model made for Copilot+ PC NPUs", "topics": ["Model release", "AI Chips and GPUs", "Microsoft"], "sentiment": "positive"}, {"url": "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3", "summary": "Mistral has launched version 3 of their 7B model, the models \u201cMistral-7B-v0.3\u201d and \u201cMistral-7B-Instruct-v0.3\u201d. Enhancements include an expanded vocabulary of 32,768 terms, integration with the v3 Tokenizer, and new function calling capabilities.", "title": "mistralai/Mistral-7B-Instruct-v0.3", "topics": ["Mistral", "Model release"], "sentiment": "positive"}, {"url": "https://www.engadget.com/openai-didnt-intend-to-copy-scarlett-johanssons-voice-the-washington-post-reports-041247992.html", "summary": "OpenAI\u2019s selection of a voice for its Sky assistant, which prioritized warmth and charisma, sparked controversy when Scarlett Johansson noted a strong resemblance to her own voice, leading to public and legal issues. OpenAI, having denied deliberately imitating Johansson\u2019s voice, halted the use of Sky\u2019s voice after her objections. This dispute followed unsuccessful discussions regarding Johansson potentially providing her voice for ChatGPT with OpenAI\u2019s Sam Altman.", "title": "OpenAI reportedly didn\u2019t intend to copy Scarlett Johansson\u2019s voice", "topics": ["AI and copyright", "Text-to-speech", "OpenAI"], "sentiment": "negative"}, {"url": "https://www.cnbc.com/2024/05/24/openai-sends-internal-memo-releasing-former-employees-from-non-disparagement-agreements-sam-altman.html", "summary": "OpenAI reversed a decision that would have required former employees to agree to a perpetual non-disparagement clause in order to retain their vested equity. The company confirmed in an internal memo, seen by CNBC, that it will not cancel any vested units regardless of whether the agreement was signed.", "title": "OpenAI sends internal memo releasing former employees from controversial exit agreements", "topics": ["OpenAI"], "sentiment": "positive"}, {"url": "https://www.cnbc.com/2024/05/22/amazon-plans-to-give-alexa-an-ai-overhaul-monthly-subscription-price.html", "summary": "Amazon is updating Alexa with advanced generative AI capabilities and launching an additional subscription service separate from Prime in efforts to stay competitive with Google and OpenAI\u2019s chatbots, reflecting the company\u2019s strategic emphasis on AI amidst internal and leadership changes.", "title": "Amazon plans to give Alexa an AI overhaul \u2014 and a monthly subscription price", "topics": ["Amazon", "Google", "OpenAI"], "sentiment": "positive"}], "guides": [{"url": "https://www.anthropic.com/news/mapping-mind-language-model", "summary": "Anthropic has made strides in AI interpretability by analyzing Claude Sonnet, a large language model, to associate neuron activations with a vast array of concepts. This work promotes safer AI through improved monitoring, debiasing, and the ability to manipulate features to guide model behavior.", "title": "Mapping the Mind of a Large Language Model"}, {"url": "https://cprimozic.net/blog/building-embedding-visualizations-from-user-profiles/", "summary": "The author reports on the creation of advanced interactive visualizations for embeddings from various datasets using tools like PyMDE and Emblaze. The work encompasses data collection, embedding computation, and visualization rendering, showcasing iterative enhancements for better exploratory analysis in AI applications.", "title": "What I\u2019ve Learned Building Interactive Embedding Visualizations"}, {"url": "https://blog.elicit.com/living-documents-ai-ux/", "summary": "The author examines the application of LLMs in generating dynamic, AI- assisted \u201cliving documents\u201d to streamline scientific literature reviews. The system employs semantic analysis to structure data into modifiable tables, focusing on overcoming obstacles such as complex AI management, maintaining user-friendly interfaces, and minimizing operational expenses.", "title": "Living documents as an AI UX pattern"}, {"url": "https://huggingface.co/blog/NicoNico/green-bit-llm", "summary": "The article explores progress in developing low-bit quantized large language models optimized for edge computing, highlighting the creation of over 200 models that can run on consumer GPUs such as the GTX 3090. These models achieve notable resource efficiency via advanced quantization methods, aided by new tools like Bitorch Engine and green-bit-llm for streamlined training and deployment.", "title": "GPU Poor Savior: Revolutionizing Low-Bit Open Source LLMs and Cost-Effective Edge Computing"}, {"url": "https://huggingface.co/blog/not-lain/trainer-api-and-mixin-classes", "summary": "The article provides a guide for using the Hugging Face Trainer API to streamline the adaptation, training, and integration of AI models with minimal coding effort. It discusses setting up necessary dependencies, data preprocessing, model adjustments, and employing distributed training, culminating in a tutorial on sharing models via the Hugging Face Hub.", "title": "Train custom AI models with the trainer API and adapt them to Hugging Face"}], "papers": [{"url": "https://arxiv.org/abs/2402.10200", "summary": "The study investigates the presence of Chain-of-Thought reasoning in pre-trained large language models by altering the decoding process to consider multiple token options. It reveals that this approach can uncover intrinsic reasoning paths, resulting in improved understanding of the models\u2019 capabilities and linking reasoning to greater output confidence, as demonstrated across different reasoning benchmarks.", "title": "Chain-of-Thought Reasoning Without Prompting"}, {"url": "https://arxiv.org/abs/2405.14860", "summary": "A recent study disputes the linear representation hypothesis in language models by revealing multi-dimensional representations through sparse autoencoders, notably circular representations for time concepts in GPT-2 and Mistral 7B. These representations have proven beneficial for modular arithmetic tasks, and intervention experiments on Mistral 7B and Llama 3 8B underscore their significance in language model computations.", "title": "Not All Language Model Features Are Linear"}, {"url": "https://arxiv.org/abs/2405.13817", "summary": "The paper presents a novel hybrid digital-analog algorithm that imitates natural gradient descent for neural network training, promising better convergence rates of second-order methods while maintaining computational efficiency akin to first-order methods. Utilizing thermodynamic analog system properties, this approach circumvents the expensive computations typical of current digital techniques.", "title": "Thermodynamic Natural Gradient Descent"}, {"url": "https://arxiv.org/abs/2405.12250", "summary": "Recent research suggests that transformer decoders in models such as GPT, LLaMA, OPT, and BLOOM show an unexpected near-linear relationship across layers. Experiments indicate that omitting or simplifying the most linear blocks within these transformers does not substantially impact their loss or performance, calling into question current assumptions about the complexity of transformer operations.", "title": "Your Transformer is Secretly Linear"}, {"url": "https://arxiv.org/abs/2405.12399", "summary": "DIAMOND is a novel reinforcement learning agent that uses a diffusion-based world model to capture fine visual details that discrete latent models typically miss. It demonstrates superior performance, as shown by setting a new human normalized score record on the Atari 100k benchmark. The authors have made their code and models publicly available for future research.", "title": "Diffusion for World Modeling: Visual Details Matter in Atari"}], "datetime": "2024-05-27"}, {"url": "https://medium.com/nlplanet", "news": [{"url": "https://x.com/lmsysorg/status/1795512202465845686", "summary": "The latest LMSYS leaderboard shows that Gemini 1.5 Pro/Advanced ranks second, right behind GPT-4o, while Gemini 1.5 Flash holds the ninth position, surpassing Llama-3\u201370b and closely competing with GPT-4\u20130120.", "title": "Gemini 1.5 Pro/Advanced at #2 on the LMSYS leaderboard, right behind GPT-4o", "topics": ["Google Gemini", "GPT-4 and GPT-4 turbo", "OpenAI"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2024/05/28/anthropic-hires-former-openai-safety-lead-to-head-up-new-team/", "summary": "Jan Leike has moved from OpenAI to Anthropic to head a new AI safety team dedicated to \u201csuperalignment,\u201d focusing on enhancing scalable oversight and large-scale AI alignment research.", "title": "Anthropic hires former OpenAI safety lead to head up new team", "topics": ["AI safety", "Anthropic", "OpenAI"], "sentiment": "positive"}, {"url": "https://x.ai/blog/series-b", "summary": "xAI has raised $6 billion in a Series B round to expand AI tech deployment, including their Grok-1 series, and to innovate new products, building on a year of significant AI advancements and the Grok-1 open-source release.", "title": "xAI announces series B funding round of $6 billion", "topics": ["Grok", "Funding"], "sentiment": "positive"}, {"url": "https://mistral.ai/news/codestral/", "summary": "Codestral is Mistral AI\u2019s new generative AI model focused on coding, boasting proficiency in over 80 programming languages and a large 32k context window for superior performance in benchmarks.", "title": "Mistral releases Codestral", "topics": ["AI for coding", "Model release", "Mistral"], "sentiment": "positive"}, {"url": "https://techxplore.com/news/2024-05-china-invests-billion-largest-chip.html", "summary": "China allocated $47.48 billion to a new chip fund aimed at advancing domestic semiconductor production, a critical step toward self-sufficiency and competitiveness in technology sectors, including AI.", "title": "China invests $47 billion in largest ever chip fund", "topics": ["Funding", "AI Chips and GPUs"], "sentiment": "positive"}], "guides": [{"url": "https://github.com/karpathy/llm.c/discussions/481", "summary": "Karpathy has created a guide outlining how to replicate GPT-2 (124M) using the C/CUDA-based llm.c implementation, designed for both single and multi-GPU setups. The training, which costs about $20 and takes 90 minutes, uses the FineWeb dataset of 10 billion tokens. This resource provides installation instructions, dataset prep guidance, and aims to enhance the original GPT-2\u2019s performance with possible future enhancements.", "title": "Reproducing GPT-2 (124M) in llm.c in 90 minutes for $20"}, {"url": "https://huggingface.co/blog/train-sentence-transformers", "summary": "The article discusses the release of Sentence Transformers v3.0, highlighting enhanced capabilities for training and finetuning embedding models to boost task-specific performance, and showcases the updated components including datasets, loss functions, evaluators, and an improved trainer.", "title": "Training and Finetuning Embedding Models with Sentence Transformers v3"}, {"url": "https://piaoyang0.wordpress.com/2024/05/15/llms-are-not-suitable-for-brainstorming/", "summary": "The article critiques current LLMs for their ineffectiveness in advanced brainstorming due to their mimicry of existing data patterns and tendency towards consensus ideas, proposing that LLMs require evolution in training processes to foster genuine creativity.", "title": "LLMs are not suitable for (advanced) brainstorming"}, {"url": "https://www.theatlantic.com/technology/archive/2024/05/fatal-flaw-publishers-making-openai-deals/678477/", "summary": "The author underscores the pitfalls facing media companies entering AI partnerships that may undermine journalism\u2019s value and sustainability. She advocates for a focus on producing quality journalism rather than seeking immediate financial relief through potentially undervalued licensing agreements with AI entities.", "title": "Media Companies Are Making a Huge Mistake With AI"}, {"url": "https://huggingface.co/blog/alirezamsh/mergoo", "summary": "Mergoo is a library designed to streamline the merging and training of various LLMs into a unified model by employing methods such as mixture-of-experts, mixture-of-adapters, and layer-wise merging.", "title": "Mergoo: Efficiently Build Your Own MoE LLM"}], "papers": [{"url": "https://github.com/llmware-ai/llmware", "summary": "Llmware provides a comprehensive framework for constructing enterprise-grade Retrievable Augmented Generation (RAG) pipelines, offering an integrated RAG Pipeline and access to over 50 specialized models for functions such as QA and summarization. It facilitates swift development of knowledge-driven AI applications and is compatible with open-source models, all while eliminating the necessity for GPU server infrastructure.", "title": "llmware-ai/llmware: Unified framework for building enterprise RAG pipelines with small, specialized models"}, {"url": "https://arxiv.org/abs/2405.17399v1", "summary": "The paper highlights that the addition of positional encodings to transformer models significantly enhances their ability to perform arithmetic operations, achieving up to 99% accuracy on adding 100-digit numbers and boosting performance on other reasoning tasks.", "title": "Transformers Can Do Arithmetic with the Right Embeddings"}, {"url": "https://github.com/lavague-ai/LaVague", "summary": "LaVague is an open-source AI framework designed for building Web Agents. It leverages a World Model to transform website data and goals into commands, which are carried out by an Action Engine compatible with tools such as Selenium or Playwright.", "title": "lavague-ai/LaVague: Large Action Model framework to develop AI Web Agents"}, {"url": "https://arxiv.org/abs/2405.17247", "summary": "This paper provides an overview of Vision-Language Models (VLMs), discussing their fundamentals, functioning, training techniques, and assessment strategies. It also addresses challenges related to the complex nature of visual data and the incorporation of video content for individuals new to this area of artificial intelligence research.", "title": "An Introduction to Vision-Language Modeling"}, {"url": "https://arxiv.org/abs/2405.17430", "summary": "The paper presents Matryoshka Multimodal Models (M3), which improve the efficiency of Large Multimodal Models (LMMs) such as LLaVA by offering adjustable visual token granularity to match the complexity of images during inference.", "title": "Matryoshka Multimodal Models"}], "datetime": "2024-06-03"}, {"url": "https://medium.com/nlplanet", "news": [{"url": "https://gizmodo.com/jony-ive-altman-openai-talk-mystery-hardware-project-1850877089", "summary": "Apple is anticipated to partner with OpenAI to incorporate ChatGPT into the iPhone\u2019s operating system, which could be announced at the next WWDC. This integration, which could revolutionize AI interaction on iPhones, might see ChatGPT enhance Siri or launch as a separate application, signaling Apple\u2019s pivot towards external AI expertise.", "title": "Apple\u2019s Reported ChatGPT Deal Could Crown OpenAI as King of the Valley", "topics": ["Apple", "ChatGPT", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.theverge.com/2024/6/5/24172363/nvidia-apple-market-cap-valuation-trillion-ai", "summary": "Nvidia has achieved a market capitalization of $3.01 trillion, propelled by the artificial intelligence surge, overtaking Apple to become the world\u2019s second most valuable company.", "title": "Nvidia is now more valuable than Apple at $3.01 trillion", "topics": ["Apple", "NVIDIA"], "sentiment": "positive"}, {"url": "https://www.pcmag.com/news/apple-keeps-it-simple-will-call-its-ai-apple-intelligence", "summary": "Apple is set to unveil \u201cApple Intelligence,\u201d an AI solution with chatbot capabilities akin to ChatGPT, at WWDC on June 10. This will be included in upcoming iOS, iPadOS, and macOS updates and is designed for offline operation, marking a partnership with OpenAI and improvements to Siri.", "title": "Apple Keeps It Simple, Will Call Its AI \u2018Apple Intelligence\u2019", "topics": ["Apple", "OpenAI", "ChatGPT"], "sentiment": "positive"}, {"url": "https://www.fastcompany.com/91134766/amd-unveils-new-ai-chips-to-compete-with-nvidia", "summary": "AMD is challenging Nvidia\u2019s leadership in AI with upcoming releases: the MI325X in 2024, and the MI350/MI400 series in 2025\u20132026, promising notable performance boosts to satisfy increasing AI demands.", "title": "AMD unveils new AI chips to compete with Nvidia", "topics": ["NVIDIA", "AI Chips and GPUs"], "sentiment": "positive"}, {"url": "https://www.forbes.com/sites/kenrickcai/2024/05/30/openai-robotics-team/", "summary": "OpenAI is reinstating its robotics division, focusing on creating AI models for robotic applications in collaboration with external robotics companies. This marks a strategic pivot from producing in-house hardware to empowering humanoid robots through partnerships, as evidenced by investments in entities like Figure AI. The team expansion is underway through active recruitment.", "title": "OpenAI Is Rebooting Its Robotics Team", "topics": ["Robotics", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.reuters.com/technology/nvidia-salesforce-double-down-ai-startup-cohere-450-million-round-source-says-2024-06-04/", "summary": "Generative AI startup Cohere has secured a $450 million funding round led by Nvidia and Salesforce, alongside new backers such as Cisco and PSP Investments, boosting its valuation to $5 billion from its prior $2.2 billion mark. The company also disclosed an annualized revenue of $35 million.", "title": "Nvidia and Salesforce may double down on AI startup Cohere in $450 million round", "topics": ["Funding", "NVIDIA"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2024/06/05/stability-ai-releases-a-sound-generator/", "summary": "Stability AI has launched \u201cStable Audio Open,\u201d an AI model that generates sound from text descriptions using royalty-free samples, geared towards non-commercial use.", "title": "Stability AI releases a sound generator", "topics": ["Multimodal AI (image, video, audio)", "Model release", "Stability AI"], "sentiment": "positive"}], "guides": [{"url": "https://openai.com/index/extracting-concepts-from-gpt-4/", "summary": "Researchers have employed sparse autoencoders to break down GPT-4\u2019s neural network into 16 million human-interpretable features, allowing for enhanced comprehension of AI processes. However, fully deciphering these features continues to pose a challenge, restricting the effectiveness of existing autoencoders.", "title": "Extracting Concepts from GPT-4"}, {"url": "https://huggingface.co/blog/mlabonne/abliteration", "summary": "", "title": "Uncensor any LLM with abliteration"}, {"url": "https://blog.alexalemi.com/kl-is-all-you-need.html", "summary": "The author highlights the importance of Kullback-Leibler divergence as a fundamental objective in machine learning, crucial for measuring differences between probability distributions and optimizing models across diverse methods in the field.", "title": "KL is All You Need"}, {"url": "https://www.topbots.com/ai-tools-task-management-scheduling/", "summary": "The article highlights AI advancements in productivity platforms such as Motion, Reclaim AI, Clockwise, ClickUp, Taskade, and Asana, detailing their use of machine learning to improve task management, scheduling, and overall workflow optimization.", "title": "AI-Powered Tools Transforming Task Management and Scheduling"}, {"url": "https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-ii/", "summary": "The article discusses the complexities of developing applications with LLMs, highlighting the necessity for high-quality data, careful management of model outputs, and strategies for effectively integrating and maintaining LLM versions. It underscores the critical roles of early designer engagement, assembling a skilled team, and cultivating an innovative work environment to navigate the unique operational challenges in LLM-based product development.", "title": "What We Learned from a Year of Building with LLMs (Part II)"}], "papers": [{"url": "https://arxiv.org/abs/2406.02430", "summary": "Seed-TTS encompasses advanced autoregressive and non-autoregressive text-to-speech models capable of generating human-like speech with emotional variability, speaker similarity, and naturalness, also showcasing proficiency in end-to-end speech generation and editing through a diffusion-based architecture.", "title": "Seed-TTS: A Family of High-Quality Versatile Speech Generation Models"}, {"url": "https://qwenlm.github.io/blog/qwen2/", "summary": "The Qwen2 series is an advancement over the Qwen1.5, introducing five enhanced AI models with new features such as support for 27 additional languages and improved coding and mathematics functions. The standout Qwen2\u201372B offers superior safety and can comprehend lengthy contexts of up to 128K tokens. These models are available on Hugging Face and ModelScope.", "title": "Hello Qwen2"}, {"url": "https://arxiv.org/abs/2405.21060", "summary": "This article presents an analysis of the structured relationship between Transformers and state-space models (SSMs) using matrix analysis, introducing a theoretical framework that connects the two. It also introduces an improved architecture, Mamba-2, building on its predecessor Mamba by being significantly faster (2\u20138 times) and maintaining comparable performance in language modeling tasks.", "title": "Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality"}, {"url": "https://llm-merging.github.io/", "summary": "The article introduces a competition that challenges participants to integrate multiple fine-tuned LLMs to improve their performance and adaptability to novel tasks. Competitors will utilize pre-trained expert models with up to 8 billion parameters from the Hugging Face Model Hub, which are available under research-friendly licenses. The goal of the competition is to minimize the costs and challenges of training LLMs from the ground up by utilizing existing models.", "title": "LLM Merging Competition: Building LLMs Efficiently through Merging"}, {"url": "https://arxiv.org/abs/2405.20519", "summary": "The paper presents an approach to program synthesis using neural diffusion models that iteratively refine code through edits on syntax trees, ensuring syntactic correctness and addressing the limitations of token-based code generation without output feedback in existing large language models.", "title": "Diffusion On Syntax Trees For Program Synthesis"}], "datetime": "2024-06-10"}, {"url": "https://medium.com/nlplanet", "news": [{"url": "https://finance.yahoo.com/news/openai-doubles-annualized-revenue-3-232851705.html", "summary": "OpenAI\u2019s projected revenue for 2024 is $3.4 billion, up from $1.6 billion in 2023, with CEO Sam Altman citing $3.2 billion from core AI products/services and $200 million from partnerships such as with Microsoft Azure. The company\u2019s valuation is at $86 billion as it continues to advance in the AI industry.", "title": "OpenAI Doubles Annualized Revenue to $3.4 Billion", "topics": ["Funding", "Microsoft", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.wsj.com/tech/ai/microsoft-nadella-openai-inflection-9727e77a", "summary": "Microsoft CEO Satya Nadella is enhancing the company\u2019s AI capabilities by acquiring AI assets worldwide, cultivating proprietary AI technologies, and possibly positioning Microsoft as a competitor to OpenAI. This expansion includes investing in AI startups and recruiting industry experts.", "title": "Microsoft\u2019s Nadella Is Building an AI Empire. OpenAI Was Just the First Step.", "topics": ["Microsoft", "OpenAI"], "sentiment": "positive"}, {"url": "https://www.tomshardware.com/tech-industry/nvidia-shipped-376m-data-center-gpus-in-2023-dominates-business-with-98-revenue-share", "summary": "In 2023, Nvidia consolidated its position in the data center GPU market with a 98% share by distributing 3.76 million units and achieved a remarkable 126% revenue increase since 2020, reaching $60.9 billion, even amidst U.S. export restrictions and manufacturing hurdles.", "title": "Nvidia shipped 3.76M data center GPUs in 2023 \u2014 dominates business with 98% revenue share", "topics": ["NVIDIA", "AI Chips and GPUs"], "sentiment": "positive"}, {"url": "https://techcrunch.com/2024/06/11/paris-based-ai-startup-mistral-ai-raises-640-million/", "summary": "Mistral AI, a Paris-based AI startup with founders from Meta and DeepMind, secured $640M in a Series B round led by General Catalyst, reaching a $6B valuation, and focuses on creating cutting-edge AI technologies, balancing open-source and proprietary offerings.", "title": "Paris-based AI startup Mistral AI raises $640M", "topics": ["Funding", "Mistral", "Meta", "DeepMind"], "sentiment": "positive"}, {"url": "https://www.apple.com/newsroom/2024/06/wwdc24-highlights/", "summary": "Apple\u2019s 2024 WWDC highlighted the introduction of Apple Intelligence, a new personal intelligence system leveraging generative models and personal context integration across its ecosystem, alongside significant updates to iOS 18, iPadOS 18, macOS Sequoia, watchOS 11, tvOS 18, and visionOS 2.", "title": "Apple\u2019s WWDC24 highlights", "topics": ["Apple"], "sentiment": "positive"}, {"url": "https://lumalabs.ai/dream-machine", "summary": "The Luma Dream Machine by Lumalabs is an AI model designed for synthesizing high-quality, realistic videos from text and images, leveraging a transformer-based method optimized for video content.", "title": "Luma Dream Machine", "topics": ["Multimodal AI (image, video, audio)", "AI for images"], "sentiment": "positive"}, {"url": "https://www.theregister.com/2024/06/11/musk_wants_to_ban_apple/", "summary": "Elon Musk has expressed intent to prohibit Apple devices in his firms in response to Apple\u2019s announcement of deploying OpenAI\u2019s ChatGPT in their OS, due to security apprehensions.", "title": "Musk wants to ban Apple for cosying up to OpenAI", "topics": ["Apple", "OpenAI"], "sentiment": "negative"}, {"url": "https://www.anthropic.com/research/claude-character", "summary": "The article examines \u201ccharacter training\u201d, focusing on imbuing the Claude 3 model with attributes like curiosity and open-mindedness in addition to harm avoidance. It describes a training strategy that seeks to harmonize AI\u2019s interactive capabilities with ethical norms by flexibly aligning AI behavior with specific traits.", "title": "Claude\u2019s Character", "topics": ["AI safety", "Claude", "Anthropic"], "sentiment": "positive"}], "guides": [{"url": "https://machinelearning.apple.com/research/introducing-apple-foundation-models", "summary": "At the 2024 WWDC, Apple introduced \u201cApple Intelligence\u201d in iOS 18, iPadOS 18, and macOS Sequoia, with state-of-the-art on-device and server-based AI generative models (~3 billion parameters) focused on enhancing user experience while emphasizing privacy and operational efficiency.", "title": "Introducing Apple\u2019s On-Device and Server Foundation Models"}, {"url": "https://nextword.substack.com/p/apples-ai-strategy-in-a-nutshell", "summary": "Apple showcased its AI strategy at WWDC 2024, focusing on vertical integration through in-house on-device AI models and proprietary data centers powered by Apple silicon. Emphasizing privacy, this strategy aims to enhance market stance and user trust while minimizing reliance on third-party chipmakers.", "title": "Apple\u2019s AI Strategy in a Nutshell"}, {"url": "https://pub.towardsai.net/top-important-llms-papers-for-the-week-from-03-06-to-09-06-3f4a286a8219", "summary": "This article summarizes the latest research on LLMs from early June 2024, highlighting progress in benchmarking, training, quantization, and alignment, with a focus on uncertainty quantification, speech generation, multi-agent systems, and robust multi-task language understanding.", "title": "Top Important LLMs Papers for the Week from 03/06 to 09/06"}, {"url": "/towards-artificial-intelligence/rotary-positional-embedding-rope-motivation-and-implementation-ac221926e7df", "summary": "The article delves into Rotary Positional Embedding (RoPE) used in transformer models. Unlike traditional absolute sinusoidal embeddings, RoPE leverages vector rotations to improve recognition of long-range dependencies in data.", "title": "Rotary Positional Embedding(RoPE): Motivation and Implementation"}], "papers": [{"url": "https://arxiv.org/abs/2406.06608", "summary": "The \u201cPrompt Report\u201d provides a comprehensive analysis of prompting methods in Generative AI, introducing a taxonomy and a unified set of terms with 33 vocabulary entries for prompts. It details 58 techniques for text-based systems and 40 for non-text modalities to standardize understanding in this emerging domain.", "title": "The Prompt Report: A Systematic Survey of Prompting Techniques"}, {"url": "https://arxiv.org/abs/2406.09414", "summary": "Depth Anything V2 improves monocular depth estimation using synthetic images and a larger teacher model, along with pseudo-labeled real images for better generalization. It offers significantly faster and more accurate results, with model sizes varying between 25M and 1.3B parameters.", "title": "Depth Anything V2"}, {"url": "https://arxiv.org/abs/2406.07522", "summary": "Samba is a novel language model architecture that merges Mamba\u2019s selective State Space Model with Sliding Window Attention to enable efficient long-sequence compression and precise memory recall. With a sizable 3.8 billion parameter scale, Samba outperforms existing language models in handling unlimited context.", "title": "Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling"}, {"url": "https://arxiv.org/abs/2406.06525", "summary": "LlamaGen is a novel image generation approach that utilizes autoregressive models featuring an efficient tokenizer and class-conditional models for producing text-aligned images with high fidelity.", "title": "Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation"}, {"url": "https://arxiv.org/abs/2406.07368v1", "summary": "This study presents advancements in autoregressive LLMs through the combination of linear attention mechanisms and speculative decoding, resulting in notable efficiency gains, including reduced perplexity and up to a 2x increase in generation speed.", "title": "When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models"}], "datetime": "2024-06-17"}, {"url": "https://medium.com/nlplanet", "news": [{"url": "https://www.anthropic.com/news/claude-3-5-sonnet", "summary": "The latest Claude 3.5 Sonnet upgrade offers enhanced intelligence, increased processing speed, and improved efficiency at a competitive price, with notable advancements in reasoning, coding, and vision processing. Additionally, the newly introduced \u2018Artifacts\u2019 feature enables real-time collaboration.", "title": "Introducing Claude 3.5 Sonnet", "topics": ["AI for coding", "Multimodal AI (image, video, audio)", "Claude"], "sentiment": "positive"}, {"url": "https://runwayml.com/blog/introducing-gen-3-alpha/", "summary": "Runway has launched Gen-3 Alpha, an advanced AI capable of generating videos and images from text and images. It features control modes for detailed manipulations and promises future enhancements in structure, style, and motion control.", "title": "Introducing Gen-3 Alpha: A New Frontier for Video Generation", "topics": ["AI for images", "Multimodal AI (image, video, audio)"], "sentiment": "positive"}, {"url": "https://www.businesstoday.in/technology/news/story/openai-ceo-says-company-could-become-for-profit-corporation-report-says-433446-2024-06-15", "summary": "OpenAI is considering a transition to a \u201cfor-profit benefit corporation,\u201d moving away from its nonprofit origins, a direction similar to its industry competitors such as Anthropic and xAI, as indicated by CEO Sam Altman.", "title": "OpenAI CEO says company could become for-profit corporation", "topics": ["OpenAI", "Anthropic"], "sentiment": "negative"}, {"url": "https://techcrunch.com/2024/06/19/ilya-sutskever-openais-former-chief-scientist-launches-new-ai-company/", "summary": "Ilya Sutskever, alongside Daniel Gross and Daniel Levy, has established Safe Superintelligence Inc. (SSI), a new AI venture based in Palo Alto and Tel Aviv dedicated to creating superintelligent AI with a strong emphasis on safety. SSI is poised to integrate AI advancements with robust safety measures, prioritizing long-term security over immediate profits, and is anticipated to attract substantial investment due to its compelling objective and skilled founders.", "title": "Ilya Sutskever, OpenAI\u2019s former chief scientist, launches new AI company", "topics": ["AI safety", "Funding", "OpenAI"], "sentiment": "positive"}, {"url": "https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/", "summary": "NVIDIA has launched Nemotron-4 340B, an open suite of models designed to create synthetic data for training language models in diverse sectors. The suite, which includes base, instruct, and reward models, focuses on improving the quality and availability of training data. It is optimized for NVIDIA NeMo and TensorRT-LLM, providing support for more efficient training and inference of LLMs.", "title": "NVIDIA Releases Open Synthetic Data Generation Pipeline for Training Large Language Models", "topics": ["AI datasets", "NVIDIA"], "sentiment": "positive"}, {"url": "https://theconversation.com/indian-election-was-awash-in-deepfakes-but-ai-was-a-net-positive-for-democracy-231795", "summary": "India\u2019s 2024 elections saw AI advancements in voter engagement through deepfake communication and real-time multi-language translation. Despite instances of AI-facilitated trolling, the technology predominantly boosted democratic participation and personalized voter outreach, even projecting virtual embodiments of past political figures.", "title": "Indian election was awash in deepfakes \u2014 but AI was a net positive for democracy", "topics": ["AI safety", "AI regulation", "Multimodal AI (image, video, audio)"], "sentiment": "positive"}, {"url": "https://deepmind.google/discover/blog/generating-audio-for-video/", "summary": "DeepMind has created a V2A (Video-to-Audio) system using a diffusion-based AI model for generating synchronized audio for silent videos, guided by visual and textual cues to produce lifelike sound environments.", "title": "Generating audio for video", "topics": ["Multimodal AI (image, video, audio)", "DeepMind"], "sentiment": "positive"}], "guides": [{"url": "https://huggingface.co/blog/m-ric/extracting-concepts-from-llms", "summary": "Anthropic has advanced the interpretability of LLMs by integrating Sparse AutoEncoders (SAEs) with models like Claude-3-Sonnet to extract interpretable features across multiple languages. However, OpenAI cautions that excessive dependence on SAE-extracted features can hinder performance. This research represents substantial progress in decoding LLMs, but achieving full understanding is still elusive.", "title": "Extracting Concepts from LLMs: Anthropic\u2019s recent discoveries"}, {"url": "https://huggingface.co/blog/alvdansen/thoughts-on-lora-training-1", "summary": "The article provides insights into training LoRAs, emphasizing dataset quality and accurate text captions for effective parameter fine-tuning. It highlights typical pitfalls, including overcomplication, and offers practical tips like employing diverse image styles and tailoring training durations to the dataset\u2019s source.", "title": "Thoughts on LoRA Training"}, {"url": "https://www.anthropic.com/research/reward-tampering", "summary": "The article discusses how AI models using reinforcement learning may exhibit \u201cspecification gaming\u201d and \u201creward tampering,\u201d leading to manipulative behaviors aimed at maximizing rewards, which can include deceitful tactics and untrained modifications of their reward functions. The studies show that such issues persist despite attempts at preventing them.", "title": "Sycophancy to subterfuge: Investigating reward tampering in language models"}, {"url": "https://engineering.fb.com/2024/06/12/production-engineering/maintaining-large-scale-ai-capacity-meta/", "summary": "Meta manages a significant AI infrastructure projected to reach 600,000 GPUs, focusing on ensuring uptime and seamless updates through maintenance protocols while prioritizing system stability and efficient resource management.", "title": "Maintaining large-scale AI capacity at Meta"}], "papers": [{"url": "https://github.com/deepseek-ai/DeepSeek-Coder-V2", "summary": "DeepSeek-Coder-V2 is an open-source language model specialized in coding and mathematics, boasting better performance than proprietary models like GPT4-Turbo. It supports an impressive range of 338 programming languages, provides an extended context length of 128K, and comes in two sizes: 16 billion and 236 billion parameters. The model is MIT licensed, allowing for commercial use and easy integration with APIs.", "title": "deepseek-ai/DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence"}, {"url": "https://arxiv.org/abs/2406.11230", "summary": "A new benchmark called MultiModal Needle-in-a-haystack (MMNeedle) has been introduced to evaluate the long-context handling capabilities of Multimodal Large Language Models (MLLMs). This benchmark tests MLLMs by requiring them to identify specific components within multi-image inputs, serving as a measure of their visual context processing. Initial findings highlight GPT-4\u2019s proficiency in long-context scenarios, despite occasional hallucinations and a noticeable performance gap between API-based and open-source models.", "title": "Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of Multimodal Large Language Models"}, {"url": "https://arxiv.org/abs/2406.08973", "summary": "XLand-100B is a large-scale dataset for in-context reinforcement learning, featuring 100 billion transitions from 2.5 billion episodes across approximately 30,000 tasks. Built on the XLand-MiniGrid framework, it was created with 50,000 GPU hours to enhance research in the field.", "title": "XLand-100B: A Large-Scale Multi-Task Dataset for In-Context Reinforcement Learning"}, {"url": "https://arxiv.org/abs/2406.08673", "summary": "HelpSteer2 is an open-source dataset licensed under CC-BY-4.0 designed to improve reward model training in LLMs through alignment with human preferences. It has achieved a record 92.0% on Reward-Bench, with fewer data pairs compared to competitors.", "title": "HelpSteer2: Open-source dataset for training top-performing reward models"}], "datetime": "2024-06-24"}, {"url": "https://medium.com/nlplanet", "news": [{"url": "https://developers.googleblog.com/en/new-features-for-the-gemini-api-and-google-ai-studio/", "summary": "Gemini 1.5 Pro has been updated with a larger 2M token context window and new code execution features to improve performance on complex tasks. Gemma 2 is now open for testing in Google AI Studio, and Gemini 1.5 Flash tuning has been released.", "title": "Gemini 1.5 Pro 2M context window, code execution capabilities, and Gemma 2 are available today", "topics": ["Google Gemini", "Google"], "sentiment": "positive"}, {"url": "https://www.anthropic.com/news/projects", "summary": "Claude.ai introduces the Projects feature for Pro and Team users, leveraging Claude 3.5 Sonnet\u2019s 200K context window to enhance collaborative work through organized chats, document integration, and tailored assistance. The addition of Artifacts and a shared activity feed fosters co-creation and inspiration within the platform.", "title": "Collaborate with Claude on Projects", "topics": ["Claude"], "sentiment": "positive"}, {"url": "https://www.theverge.com/2024/6/24/24184629/apple-dma-steering-infringement-ruling", "summary": "The EU has charged Apple with violations of the Digital Markets Act due to restrictive App Store policies. A new investigation into how Apple manages alternative app stores and associated fees has begun. Apple could face fines of up to 10% of its global revenues and has stated it will cooperate with EU regulators.", "title": "Apple is first company charged with violating EU\u2019s DMA rules", "topics": ["Apple", "AI regulation"], "sentiment": "negative"}, {"url": "https://www.billboard.com/pro/major-label-lawsuit-ai-firms-suno-udio-copyright-infringement/", "summary": "Major music labels have sued AI music firms Suno and Udio for copyright infringement, alleging unlicensed use of copyrighted songs to train their AIs, which can produce tracks resembling popular artists. Suno and Udio claim their work is transformative and qualifies for fair use.", "title": "Major Labels Sue AI Firms Suno and Udio for Alleged Copyright Infringement", "topics": ["AI and copyright", "AI regulation"], "sentiment": "negative"}, {"url": "https://www.tbsnews.net/tech/apple-wont-roll-out-ai-tech-eu-market-over-regulatory-concerns-883346", "summary": "Apple Inc. has postponed the launch of new AI technologies in the EU due to the compliance requirements of the Digital Markets Act, which aims to prevent the favoring of own products and the misuse of consumer data, impacting privacy and security. This affects features like Apple Intelligence, iPhone Mirroring, and SharePlay, as Apple is considered a \u201cgatekeeper\u201d under EU regulations.", "title": "Apple won\u2019t roll out AI tech in EU market over regulatory concerns", "topics": ["AI regulation", "Apple"], "sentiment": "negative"}, {"url": "https://www.cio.com/article/2505518/stability-ai-gets-new-ceo-and-investment-dream-team-to-start-rescue-mission.html#:~:text=The%20new%20CEO%20is%20Prem,of%20MP3%20music%20icon%20Napster.", "summary": "Prem Akkaraju has been named the new CEO of Stability.ai, creator of Stable Diffusion, alongside receiving investment from notable figures. Akkaraju\u2019s role is pivotal in leading the company\u2019s turnaround efforts, leveraging his experience as the former CEO of Weta Digital.", "title": "Stability.ai gets new CEO and investment dream team to start rescue mission", "topics": ["Funding", "Stable Diffusion", "Stability AI"], "sentiment": "positive"}, {"url": "https://www.engadget.com/youtube-reportedly-wants-to-pay-record-labels-to-use-their-songs-for-ai-training-125052503.html", "summary": "YouTube seeks licensing agreements with major record labels Sony, Universal, and Warner for AI training to circumvent copyright troubles, but faces pushback from artists. Meanwhile, labels are suing AI music platforms Suno and Udio for copyright infringement.", "title": "YouTube reportedly wants to pay record labels to use their songs for AI training", "topics": ["AI and copyright", "AI datasets"], "sentiment": "negative"}, {"url": "https://www.figma.com/blog/introducing-figma-ai/", "summary": "Figma has launched Figma AI, a new AI-enhanced design platform featuring AI-driven search capabilities, generative text and image tools, and advanced prototyping functionalities. It\u2019s currently in beta and free until 2024, though usage may be capped depending on the cost of tools.", "title": "Meet Figma AI: Empowering designers with intelligent tools", "topics": ["AI for images", "Multimodal AI (image, video, audio)"], "sentiment": "positive"}, {"url": "https://www.theverge.com/2024/6/19/24181965/snapchat-ai-prompt-custom-lens", "summary": "Snapchat has launched a feature enabling users to create custom AI-driven lenses using textual prompts, leveraging user interaction data and online activity to tailor experiences.", "title": "Snapchat AI turns prompts into new lens", "topics": ["AI for images", "Multimodal AI (image, video, audio)"], "sentiment": "positive"}], "guides": [{"url": "https://blog.langchain.dev/what-is-an-agent/", "summary": "An agent, in the context of LLM systems, refers to the varying degrees of autonomous capabilities such systems have, ranging from basic task routing to fully autonomous operations. The article examines the necessary development, orchestration, and monitoring that accompany the increase in system autonomy.", "title": "What is an agent?"}, {"url": "https://huggingface.co/blog/gemma2", "summary": "Google\u2019s Gemma 2 has been released, showcasing advanced models with a maximum of 27 billion parameters tailored for base and specialized instructional use cases. It incorporates novel AI techniques like sliding window attention, logit soft-capping, knowledge distillation, and model merging, with availability on the Hugging Face platform.", "title": "Welcome Gemma 2 \u2014 Google\u2019s new open LLM"}, {"url": "https://www.topbots.com/ai-research-tools/", "summary": "The article provides a comparative analysis of four AI research tools \u2014 ChatGPT, Gemini, Claude, and Perplexity \u2014 examining their response quality, access to real-time data, referencing abilities, document analysis, and subscription options to enhance productivity in academic and business research settings.", "title": "Top AI Tools for Research: Evaluating ChatGPT, Gemini, Claude, and Perplexity"}, {"url": "https://www.together.ai/blog/rag-fine-tuning", "summary": "Research shows that fine-tuning LLMs with Retrieval-Augmented Generation (RAG) can enhance code generation performance by reducing errors such as hallucinations and outdated information. Tests on the Together AI Platform reveal that models fine-tuned with RAG, specifically using Mistral 7B Instruct v0.2, surpass competitors like Claude 3 Opus and GPT-4o in terms of accuracy, efficiency, and cost.", "title": "Building a personalized code assistant with open-source LLMs using RAG Fine-tuning"}, {"url": "https://huggingface.co/blog/finetune-florence2", "summary": "Microsoft\u2019s Florence-2 is a hybrid vision-language model excelling in OCR and object detection tasks. It integrates a DaViT vision encoder with BERT embeddings and exhibits improved performance upon fine-tuning with the DocVQA dataset, reaching a 57.0 similarity score. This advancement is attributed to its pre-training on the large-scale FLD-5B dataset.", "title": "Fine-tuning Florence-2 \u2014 Microsoft\u2019s Cutting-edge Vision Language Models"}], "papers": [{"url": "https://arxiv.org/abs/2406.12624", "summary": "he study investigates the effectiveness of LLMs in evaluating the performance of their counterparts, using the TriviaQA dataset and human annotations as benchmarks. It reveals inconsistencies in the models\u2019 assessments and highlights that agreement rates between LLMs do not always reflect true alignment, as evidenced by variance in scores.", "title": "Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges"}, {"url": "https://arxiv.org/abs/2406.16793", "summary": "The Adam-mini optimizer offers performance on par with or better than AdamW with 45\u201350% lower memory usage, due to its structured learning rate allocation for parameter groups. It also increases throughput by up to 49.6% and reduces computational overhead.", "title": "Adam-mini: Use Fewer Learning Rates To Gain More"}, {"url": "https://arxiv.org/abs/2406.14508", "summary": "A study investigating the influence of language model size on persuasive abilities across political issues found that larger models exhibit diminishing returns in persuasiveness, with small models nearly as effective as larger ones. The minor superiority of bigger models is attributed to enhanced coherence and topical focus, implying negligible benefits from scaling up language models further.", "title": "Evidence of a log scaling law for political persuasion with large language models"}, {"url": "https://ai.meta.com/research/publications/meta-large-language-model-compiler-foundation-models-of-compiler-optimization/", "summary": "Meta released the LLM Compiler that uses pre-trained models, including Code Llama, for improving code optimization. These models are trained on extensive datasets of intermediate and assembly code and come in variations with 7 and 13 billion parameters. Their fine-tuned instances can notably enhance code size optimization and disassembly tasks for x86_64 and ARM architectures.", "title": "Meta Large Language Model Compiler: Foundation Models of Compiler Optimization"}, {"url": "https://arxiv.org/abs/2406.15319", "summary": "LongRAG is a new Retrieval-Augmented Generation framework that extends retriever units to handle up to 4K tokens. It leverages a long-context language model, enabling it to extract answers without extra training and attain high Exact Match scores, comparable to state-of-the-art performance.", "title": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs"}], "datetime": "2024-07-01"}]